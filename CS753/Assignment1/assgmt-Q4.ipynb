{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3S0Etz_Nokz"
      },
      "source": [
        "# PART I: Running a SpeechBrain ASR Recipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-xktzpeEBjf"
      },
      "source": [
        "## You have to fill in appropriate code in 4 locations in the notebook.\n",
        "\n",
        "* The four locations start with \"####TASK\". Read the task specifications mentioned there.\n",
        "* The required function names are already available in this notebook somewhere. You're only required to find and use the appropriate ones.\n",
        "* Check the SpeechBrain documentation to find out how to use those functions. Refer to the starting material for more resources.\n",
        "* By the end of this part of the assignment, you should be comfortable running a Speechbrain recipe.\n",
        "* **P.S.** Note that none of the four tasks require you to write more than 1 line of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKXIYTJSOnt_"
      },
      "source": [
        "### Setting up the codebase."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vjFtAh6rTQS",
        "outputId": "c16e0bdc-92ad-4fcf-b5ce-b56605f5db50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5H2lpHX7npZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Clone SpeechBrain repository\n",
        "!git clone https://github.com/Darshan7575/speechbrain.git\n",
        "%cd /content/speechbrain/\n",
        "\n",
        "# Install required dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Install SpeechBrain in editable mode\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61C_gxJo5aRH"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Required imports\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import logging\n",
        "import sys\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "from speechbrain.utils.data_utils import get_all_files, download_file\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "from speechbrain.utils.distributed import run_on_main, if_main_process\n",
        "\n",
        "# Required variables and loggers\n",
        "logger = logging.getLogger(__name__)\n",
        "logger = logging.getLogger(__name__)\n",
        "MINILIBRI_TRAIN_URL = \"http://www.openslr.org/resources/31/train-clean-5.tar.gz\"\n",
        "MINILIBRI_VALID_URL = \"http://www.openslr.org/resources/31/dev-clean-2.tar.gz\"\n",
        "MINILIBRI_TEST_URL = \"https://www.openslr.org/resources/12/test-clean.tar.gz\"\n",
        "SAMPLERATE = 16000\n",
        "\n",
        "device=\"cuda\"\n",
        "run_opts = {'device':device}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import speechbrain as sb\n",
        "\n",
        "# Print the SpeechBrain version\n",
        "print(\"SpeechBrain version:\", sb.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1l_KPxZyq35",
        "outputId": "f283d0d4-2668-467d-ecda-46d54e9bea3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpeechBrain version: 0.5.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtdr1VnyCQTJ"
      },
      "source": [
        "### Tokenizer Training\n",
        "In this section, we will train a BPE tokenizer with **150 tokens** using `Sentencepiece`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujToJHWC4T5y"
      },
      "outputs": [],
      "source": [
        "# ############################################################################\n",
        "# Dataset creation helper functions\n",
        "# ############################################################################\n",
        "\n",
        "def prepare_mini_librispeech(\n",
        "    data_folder, save_json_train, save_json_valid, save_json_test\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares the json files for the Mini Librispeech dataset.\n",
        "    Downloads the dataset if its not found in the `data_folder`.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if this phase is already done (if so, skip it)\n",
        "    if skip(save_json_train, save_json_valid, save_json_test):\n",
        "        logger.info(\"Preparation completed in previous run, skipping.\")\n",
        "        return\n",
        "\n",
        "    # If the dataset doesn't exist yet, download it\n",
        "    train_folder = os.path.join(data_folder, \"LibriSpeech\", \"train-clean-5\")\n",
        "    valid_folder = os.path.join(data_folder, \"LibriSpeech\", \"dev-clean-2\")\n",
        "    test_folder = os.path.join(data_folder, \"LibriSpeech\", \"test-clean\")\n",
        "    if not check_folders(train_folder, valid_folder, test_folder):\n",
        "        download_mini_librispeech(data_folder)\n",
        "\n",
        "    # List files and create manifest from list\n",
        "    logger.info(\n",
        "        f\"Creating {save_json_train}, {save_json_valid}, and {save_json_test}\"\n",
        "    )\n",
        "    extension = [\".flac\"]\n",
        "\n",
        "    # List of flac audio files\n",
        "    wav_list_train = get_all_files(train_folder, match_and=extension)\n",
        "    wav_list_valid = get_all_files(valid_folder, match_and=extension)\n",
        "    wav_list_test = get_all_files(test_folder, match_and=extension)\n",
        "\n",
        "    # List of transcription file\n",
        "    extension = [\".trans.txt\"]\n",
        "    trans_list = get_all_files(data_folder, match_and=extension)\n",
        "    trans_dict = get_transcription(trans_list)\n",
        "\n",
        "    # Create the json files\n",
        "    create_json(wav_list_train, trans_dict, save_json_train)\n",
        "    create_json(wav_list_valid, trans_dict, save_json_valid)\n",
        "    create_json(wav_list_test, trans_dict, save_json_test)\n",
        "\n",
        "\n",
        "def get_transcription(trans_list):\n",
        "    \"\"\"\n",
        "    Returns a dictionary with the transcription of each sentence in the dataset.\n",
        "    \"\"\"\n",
        "    # Processing all the transcription files in the list\n",
        "    trans_dict = {}\n",
        "    for trans_file in trans_list:\n",
        "        # Reading the text file\n",
        "        with open(trans_file) as f:\n",
        "            for line in f:\n",
        "                uttid = line.split(\" \")[0]\n",
        "                text = line.rstrip().split(\" \")[1:]\n",
        "                text = \" \".join(text)\n",
        "                trans_dict[uttid] = text\n",
        "\n",
        "    logger.info(\"Transcription files read!\")\n",
        "    return trans_dict\n",
        "\n",
        "\n",
        "def create_json(wav_list, trans_dict, json_file):\n",
        "    \"\"\"\n",
        "    Creates the json file given a list of wav files and their transcriptions.\n",
        "    \"\"\"\n",
        "    # Processing all the wav files in the list\n",
        "    json_dict = {}\n",
        "    for wav_file in wav_list:\n",
        "\n",
        "        # Reading the signal (to retrieve duration in seconds)\n",
        "        signal = read_audio(wav_file)\n",
        "        duration = signal.shape[0] / SAMPLERATE\n",
        "\n",
        "        # Manipulate path to get relative path and uttid\n",
        "        path_parts = wav_file.split(os.path.sep)\n",
        "        uttid, _ = os.path.splitext(path_parts[-1])\n",
        "        relative_path = os.path.join(\"{data_root}\", *path_parts[-5:])\n",
        "\n",
        "        # Create entry for this utterance\n",
        "        json_dict[uttid] = {\n",
        "            \"wav\": relative_path,\n",
        "            \"length\": duration,\n",
        "            \"words\": trans_dict[uttid],\n",
        "        }\n",
        "\n",
        "    # Writing the dictionary to the json file\n",
        "    with open(json_file, mode=\"w\") as json_f:\n",
        "        json.dump(json_dict, json_f, indent=2)\n",
        "\n",
        "    logger.info(f\"{json_file} successfully created!\")\n",
        "\n",
        "\n",
        "def skip(*filenames):\n",
        "    \"\"\"\n",
        "    Detects if the data preparation has been already done.\n",
        "    If the preparation has been done, we can skip it.\n",
        "    \"\"\"\n",
        "    for filename in filenames:\n",
        "        if not os.path.isfile(filename):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def check_folders(*folders):\n",
        "    \"\"\"Returns False if any passed folder does not exist.\"\"\"\n",
        "    for folder in folders:\n",
        "        if not os.path.exists(folder):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def download_mini_librispeech(destination):\n",
        "    \"\"\"Download dataset and unpack it.\n",
        "    \"\"\"\n",
        "    train_archive = os.path.join(destination, \"train-clean-5.tar.gz\")\n",
        "    valid_archive = os.path.join(destination, \"dev-clean-2.tar.gz\")\n",
        "    test_archive = os.path.join(destination, \"test-clean.tar.gz\")\n",
        "    download_file(MINILIBRI_TRAIN_URL, train_archive)\n",
        "    download_file(MINILIBRI_VALID_URL, valid_archive)\n",
        "    download_file(MINILIBRI_TEST_URL, test_archive)\n",
        "    shutil.unpack_archive(train_archive, destination)\n",
        "    shutil.unpack_archive(valid_archive, destination)\n",
        "    shutil.unpack_archive(test_archive, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz9pyHan4V0S"
      },
      "outputs": [],
      "source": [
        "tokenizer_hyperparams = \"\"\"\n",
        "# ############################################################################\n",
        "# Tokenizer: subword BPE with unigram 150\n",
        "# ############################################################################\n",
        "\n",
        "output_folder: !ref results/tokenizer/\n",
        "\n",
        "# Data files\n",
        "data_folder: data\n",
        "train_annotation: !ref <data_folder>/train.json\n",
        "valid_annotation: !ref <data_folder>/valid.json\n",
        "test_annotation: !ref <data_folder>/test.json\n",
        "\n",
        "# Tokenizer training parameters\n",
        "token_type: unigram  # [\"unigram\", \"bpe\", \"char\"]\n",
        "token_output: 150  # index(blank/eos/bos/unk) = 0\n",
        "character_coverage: 1.0\n",
        "json_read: words\n",
        "\n",
        "tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece\n",
        "   model_dir: !ref <output_folder>\n",
        "   vocab_size: !ref <token_output>\n",
        "   annotation_train: !ref <train_annotation>\n",
        "   annotation_read: !ref <json_read>\n",
        "   annotation_format: json\n",
        "   model_type: !ref <token_type> # [\"unigram\", \"bpe\", \"char\"]\n",
        "   character_coverage: !ref <character_coverage>\n",
        "   annotation_list_to_check: [!ref <train_annotation>, !ref <valid_annotation>]\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "nA8xD6Do4Xl7",
        "outputId": "a94a03c7-6f02-4e11-a216-8b2acc769aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/tokenizer/\n",
            "__main__ - Preparation completed in previous run, skipping.\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer is already trained.\n",
            "speechbrain.tokenizers.SentencePiece - ==== Loading Tokenizer ===\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer path: results/tokenizer/150_unigram.model\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer vocab_size: 150\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer type: unigram\n",
            "speechbrain.tokenizers.SentencePiece - ==== Accuracy checking for recovering text from tokenizer ===\n",
            "speechbrain.tokenizers.SentencePiece - recover words from: data/train.json\n",
            "speechbrain.tokenizers.SentencePiece - Wrong recover words: 0\n",
            "speechbrain.tokenizers.SentencePiece - accuracy recovering words: 1.0\n",
            "speechbrain.tokenizers.SentencePiece - ==== Accuracy checking for recovering text from tokenizer ===\n",
            "speechbrain.tokenizers.SentencePiece - recover words from: data/valid.json\n",
            "speechbrain.tokenizers.SentencePiece - Wrong recover words: 0\n",
            "speechbrain.tokenizers.SentencePiece - accuracy recovering words: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'results/tokenizer//tokenizer.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# load required params from the hyperpyyaml file\n",
        "hparams = load_hyperpyyaml(tokenizer_hyperparams)\n",
        "\n",
        "# 1. Dataset creation\n",
        "\n",
        "## Create experiment directory\n",
        "sb.create_experiment_directory(\n",
        "    experiment_directory=hparams[\"output_folder\"],\n",
        "    overrides=None,\n",
        ")\n",
        "\n",
        "## Create dataset\n",
        "run_on_main(\n",
        "    prepare_mini_librispeech,\n",
        "    kwargs={\n",
        "        \"data_folder\": hparams[\"data_folder\"],\n",
        "        \"save_json_train\": hparams[\"train_annotation\"],\n",
        "        \"save_json_valid\": hparams[\"valid_annotation\"],\n",
        "        \"save_json_test\": hparams[\"test_annotation\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "# 2. Tokenizer training\n",
        "hparams[\"tokenizer\"]()\n",
        "\n",
        "# 3. Saving tokenizer in .ckpt extension\n",
        "output_path = hparams[\"output_folder\"]\n",
        "token_output = hparams[\"token_output\"]\n",
        "token_type = hparams[\"token_type\"]\n",
        "bpe_model = f\"{output_path}/{token_output}_{token_type}.model\"\n",
        "tokenizer_ckpt = f\"{output_path}/tokenizer.ckpt\"\n",
        "shutil.copyfile(bpe_model, tokenizer_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwqFx3QcOdd3"
      },
      "source": [
        "### Model Training\n",
        "In this section, we will train a **6 layer Conformer** encoder only architecture with the `CTC objective`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WPcwXueCRm7"
      },
      "outputs": [],
      "source": [
        "global_hyperparams = \"\"\"\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 2024\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Data files\n",
        "data_folder: data\n",
        "\n",
        "####TASK ADD APPROPRIATE REFERENCES TO LOAD THE FILES ##############\n",
        "\n",
        "train_annotation: !ref <data_folder>/train.json\n",
        "valid_annotation: !ref <data_folder>/valid.json\n",
        "test_annotation: !ref <data_folder>/test.json\n",
        "#####################################################################\n",
        "\n",
        "# Language model (LM) pretraining\n",
        "pretrained_lm_tokenizer_path: ./results/tokenizer\n",
        "\n",
        "# Training parameters\n",
        "number_of_epochs: 35\n",
        "batch_size: 8\n",
        "lr_adam: 0.001\n",
        "max_grad_norm: 5.0\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "loss_reduction: 'batchmean'\n",
        "\n",
        "# Dataloader options\n",
        "train_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "valid_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "test_dataloader_opts:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "# Feature parameters\n",
        "sample_rate: 16000\n",
        "n_fft: 400\n",
        "n_mels: 80\n",
        "\n",
        "####################### Model parameters ###########################\n",
        "# Transformer\n",
        "d_model: 64\n",
        "nhead: 4\n",
        "num_encoder_layers: 6\n",
        "d_ffn: 256\n",
        "transformer_dropout: 0.1\n",
        "activation: !name:torch.nn.GELU\n",
        "output_neurons: 150\n",
        "label_smoothing: 0.0\n",
        "attention_type: RelPosMHAXL\n",
        "\n",
        "# Outputs\n",
        "blank_index: 0\n",
        "pad_index: 0\n",
        "bos_index: 1\n",
        "eos_index: 2\n",
        "\n",
        "# Decoding parameters\n",
        "min_decode_ratio: 0.0\n",
        "max_decode_ratio: 1.0\n",
        "test_beam_size: 1\n",
        "ctc_weight_decode: 1.0\n",
        "\n",
        "############################## models ################################\n",
        "\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    n_fft: !ref <n_fft>\n",
        "    n_mels: !ref <n_mels>\n",
        "\n",
        "CNN: !new:speechbrain.lobes.models.convolution.ConvolutionFrontEnd\n",
        "    input_shape: (8, 10, 80)\n",
        "    num_blocks: 2\n",
        "    num_layers_per_block: 1\n",
        "    out_channels: (64, 32)\n",
        "    kernel_sizes: (3, 3)\n",
        "    strides: (2, 2)\n",
        "    residuals: (False, False)\n",
        "\n",
        "# standard parameters for the BASE model\n",
        "Transformer: !new:speechbrain.lobes.models.transformer.TransformerASR.TransformerASR\n",
        "    input_size: 640\n",
        "    tgt_vocab: !ref <output_neurons>\n",
        "    d_model: !ref <d_model>\n",
        "    nhead: !ref <nhead>\n",
        "    num_encoder_layers: !ref <num_encoder_layers>\n",
        "    num_decoder_layers: 0\n",
        "    d_ffn: !ref <d_ffn>\n",
        "    dropout: !ref <transformer_dropout>\n",
        "    activation: !ref <activation>\n",
        "    encoder_module: conformer\n",
        "    attention_type: !ref <attention_type>\n",
        "    normalize_before: True\n",
        "\n",
        "tokenizer: !new:sentencepiece.SentencePieceProcessor\n",
        "\n",
        "ctc_lin: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <d_model>\n",
        "    n_neurons: !ref <output_neurons>\n",
        "\n",
        "normalize: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: global\n",
        "    update_until_epoch: 4\n",
        "\n",
        "modules:\n",
        "    CNN: !ref <CNN>\n",
        "    Transformer: !ref <Transformer>\n",
        "    ctc_lin: !ref <ctc_lin>\n",
        "    normalize: !ref <normalize>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <CNN>, !ref <Transformer>, !ref <ctc_lin>]\n",
        "\n",
        "# define two optimizers here for two-stage training\n",
        "Adam: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_adam>\n",
        "    betas: (0.9, 0.98)\n",
        "    eps: 0.000000001\n",
        "\n",
        "log_softmax: !new:torch.nn.LogSoftmax\n",
        "    dim: -1\n",
        "\n",
        "ctc_cost: !name:speechbrain.nnet.losses.ctc_loss\n",
        "    blank_index: !ref <blank_index>\n",
        "    reduction: !ref <loss_reduction>\n",
        "\n",
        "noam_annealing: !new:speechbrain.nnet.schedulers.NoamScheduler\n",
        "    lr_initial: !ref <lr_adam>\n",
        "    n_warmup_steps: 1500\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
        "\n",
        "cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
        "   split_tokens: True\n",
        "\n",
        "# The pretrainer allows a mapping between pretrained files and instances that\n",
        "# are declared in the yaml. E.g here, we will download the file tokenizer.ckpt\n",
        "# and it will be loaded into \"tokenizer\" which is pointing to the <pretrained_lm_tokenizer_path> defined\n",
        "# before.\n",
        "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
        "    loadables:\n",
        "        tokenizer: !ref <tokenizer>\n",
        "    paths:\n",
        "        tokenizer: !ref <pretrained_lm_tokenizer_path>/tokenizer.ckpt\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euJMqDLSWv7Y"
      },
      "outputs": [],
      "source": [
        "def dataio_prepare(hparams):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined functions.\n",
        "    \"\"\"\n",
        "    # Define audio pipeline. In this case, we simply read the path contained\n",
        "    # in the variable wav with the audio reader.\n",
        "    @sb.utils.data_pipeline.takes(\"wav\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav):\n",
        "        \"\"\"Load the audio signal. This is done on the CPU in the `collate_fn`.\"\"\"\n",
        "        sig = sb.dataio.dataio.read_audio(wav)\n",
        "        return sig\n",
        "\n",
        "    tokenizer = hparams[\"tokenizer\"]\n",
        "    # Define text processing pipeline. We start from the raw text and then\n",
        "    # encode it using the tokenizer. The tokens with BOS are used for feeding\n",
        "    # decoder during training, the tokens with EOS for computing the cost function.\n",
        "    # The tokens without BOS or EOS is for computing CTC loss.\n",
        "    @sb.utils.data_pipeline.takes(\"words\")\n",
        "    @sb.utils.data_pipeline.provides(\n",
        "        \"wrd\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\n",
        "    )\n",
        "    def text_pipeline(wrd):\n",
        "        \"\"\"Processes the transcriptions to generate proper labels\"\"\"\n",
        "        yield wrd\n",
        "        tokens_list = tokenizer.encode_as_ids(wrd)\n",
        "        yield tokens_list\n",
        "        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\n",
        "        yield tokens_bos\n",
        "        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\n",
        "        yield tokens_eos\n",
        "        tokens = torch.LongTensor(tokens_list)\n",
        "        yield tokens\n",
        "\n",
        "    # Define datasets from json data manifest file\n",
        "    # Define datasets sorted by ascending lengths for efficiency\n",
        "    datasets = {}\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "    for dataset in [\"train\", \"valid\", \"test\"]:\n",
        "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
        "            json_path=hparams[f\"{dataset}_annotation\"],\n",
        "            replacements={\"data_root\": data_folder},\n",
        "            dynamic_items=[audio_pipeline, text_pipeline],\n",
        "            output_keys=[\n",
        "                \"id\",\n",
        "                \"sig\",\n",
        "                \"wrd\",\n",
        "                \"tokens_bos\",\n",
        "                \"tokens_eos\",\n",
        "                \"tokens\",\n",
        "            ],\n",
        "        )\n",
        "        hparams[f\"{dataset}_dataloader_opts\"][\"shuffle\"] = False\n",
        "\n",
        "    datasets[\"train\"] = datasets[\"train\"].filtered_sorted(sort_key=\"length\")\n",
        "    hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
        "\n",
        "    return (\n",
        "        datasets[\"train\"],\n",
        "        datasets[\"valid\"],\n",
        "        datasets[\"test\"],\n",
        "        tokenizer\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUEBBUOQZ4V-"
      },
      "outputs": [],
      "source": [
        "# Define training procedure\n",
        "class BaseASR(sb.Brain):\n",
        "    def __init__(\n",
        "        self,\n",
        "        modules=None,\n",
        "        opt_class=None,\n",
        "        hparams=None,\n",
        "        run_opts=None,\n",
        "        checkpointer=None,\n",
        "        profiler=None,\n",
        "        tokenizer=None,\n",
        "    ):\n",
        "        super(BaseASR, self).__init__(\n",
        "            modules=modules,\n",
        "            opt_class=opt_class,\n",
        "            hparams=hparams,\n",
        "            run_opts=run_opts,\n",
        "            checkpointer=checkpointer,\n",
        "            profiler=profiler\n",
        "        )\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Performs a forward pass through the encoder\"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, wav_lens = batch.sig\n",
        "        tokens_bos, _ = batch.tokens_bos\n",
        "\n",
        "        # compute features\n",
        "        ####TASK MAKE APPROPRIATE FUNCTION CALLS TO COMPUTE THE FEATURES BELOW\n",
        "        feats = self.hparams.compute_features(wavs)\n",
        "        current_epoch = self.hparams.epoch_counter.current\n",
        "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
        "\n",
        "        # forward modules\n",
        "        src = self.modules.CNN(feats)\n",
        "\n",
        "        enc_out, _ = self.modules.Transformer(\n",
        "            src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index,\n",
        "        )\n",
        "\n",
        "        # output layer for ctc log-probabilities\n",
        "        logits = self.modules.ctc_lin(enc_out)\n",
        "\n",
        "        ####TASK CALCULATE THE PROBABILITIES OF THESE LOGITS\n",
        "        #### USING SPEECHBRAIN\n",
        "        p_ctc = self.hparams.log_softmax(logits)\n",
        "\n",
        "        # Compute outputs\n",
        "        hyps = None\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            hyps = None\n",
        "        else:\n",
        "            hyps = sb.decoders.ctc_greedy_decode(\n",
        "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
        "            )\n",
        "\n",
        "        return p_ctc, wav_lens, hyps\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the CTC loss given predictions and targets.\"\"\"\n",
        "\n",
        "        (p_ctc, wav_lens, hyps,) = predictions\n",
        "\n",
        "        ids = batch.id\n",
        "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
        "        tokens, tokens_lens = batch.tokens\n",
        "\n",
        "        # Calculate CTC loss\n",
        "        ####TASK Make required function call to compute CTC LOSS\n",
        "        #### You have to aggregate the loss in the end so make appropriate\n",
        "        #### modifications to the value returned.\n",
        "        loss = self.hparams.ctc_cost(\n",
        "            p_ctc, tokens, wav_lens, tokens_lens\n",
        "        ).sum()\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            # Decode token terms to words\n",
        "            predicted_words = [\n",
        "                self.tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in hyps\n",
        "            ]\n",
        "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
        "            self.wer_metric.append(ids, predicted_words, target_words)\n",
        "            self.cer_metric.append(ids, predicted_words, target_words)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_evaluate_start(self, max_key=None, min_key=None):\n",
        "        \"\"\"Performs checkpoint averge if needed\"\"\"\n",
        "        super().on_evaluate_start()\n",
        "\n",
        "        ckpts = self.checkpointer.find_checkpoints(\n",
        "            max_key=max_key, min_key=min_key\n",
        "        )\n",
        "        ckpt = sb.utils.checkpoints.average_checkpoints(\n",
        "            ckpts, recoverable_name=\"model\", device=self.device\n",
        "        )\n",
        "\n",
        "        self.hparams.model.load_state_dict(ckpt, strict=True)\n",
        "        self.hparams.model.eval()\n",
        "        print(\"Loaded the average\")\n",
        "\n",
        "    def evaluate_batch(self, batch, stage):\n",
        "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
        "        with torch.no_grad():\n",
        "            predictions = self.compute_forward(batch, stage=stage)\n",
        "            loss = self.compute_objectives(predictions, batch, stage=stage)\n",
        "        return loss.detach()\n",
        "\n",
        "    def on_stage_start(self, stage, epoch):\n",
        "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.cer_metric = self.hparams.cer_computer()\n",
        "            self.wer_metric = self.hparams.error_rate_computer()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch):\n",
        "        \"\"\"Gets called at the end of a epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
        "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
        "\n",
        "        # log stats and save checkpoint at end-of-epoch\n",
        "        if stage == sb.Stage.VALID and sb.utils.distributed.if_main_process():\n",
        "\n",
        "            lr = self.hparams.noam_annealing.current_lr\n",
        "            steps = self.optimizer_step\n",
        "            optimizer = self.optimizer.__class__.__name__\n",
        "\n",
        "            epoch_stats = {\n",
        "                \"epoch\": epoch,\n",
        "                \"lr\": lr,\n",
        "                \"steps\": steps,\n",
        "                \"optimizer\": optimizer,\n",
        "            }\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta=epoch_stats,\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            # Save only last 10 checkpoints\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"loss\": stage_loss, \"epoch\": epoch},\n",
        "                max_keys=[\"epoch\"],\n",
        "                num_to_keep=10,\n",
        "            )\n",
        "\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "            # Write the WER metric for test dataset\n",
        "            if if_main_process():\n",
        "                with open(self.hparams.test_wer_file, \"w\") as w:\n",
        "                    self.wer_metric.write_stats(w)\n",
        "\n",
        "    def fit_batch(self, batch):\n",
        "        \"\"\"Performs a forward + backward pass on 1 batch\n",
        "        \"\"\"\n",
        "\n",
        "        should_step = self.step % self.grad_accumulation_factor == 0\n",
        "\n",
        "        outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
        "        loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
        "        loss.backward()\n",
        "        if self.check_gradients(loss):\n",
        "            self.optimizer.step()\n",
        "        self.zero_grad()\n",
        "        self.optimizer_step += 1\n",
        "        self.hparams.noam_annealing(self.optimizer)\n",
        "\n",
        "        self.on_fit_batch_end(batch, outputs, loss, should_step)\n",
        "        return loss.detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDQEQ8M2MNAi"
      },
      "outputs": [],
      "source": [
        "task_hyperparameters = \"\"\"\n",
        "# Setup the directory to host experiment results\n",
        "output_folder: !ref results/transformer/Task_1\n",
        "wer_file: !ref <output_folder>/wer.txt\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        noam_scheduler: !ref <noam_annealing>\n",
        "        normalizer: !ref <normalize>\n",
        "        counter: !ref <epoch_counter>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGonNC7u8hlJ",
        "outputId": "4621dbbc-9d30-47b5-815a-89313b717ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/transformer/Task_1\n",
            "speechbrain.pretrained.fetching - Fetch tokenizer.ckpt: Using existing file/symlink in model_checkpoints/tokenizer.ckpt.\n",
            "speechbrain.utils.parameter_transfer - Set local path in self.paths[tokenizer] = model_checkpoints/tokenizer.ckpt\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: tokenizer\n",
            "speechbrain.utils.parameter_transfer - Redirecting (loading from local path): model_checkpoints/tokenizer.ckpt -> model_checkpoints/tokenizer.ckpt\n",
            "speechbrain.core - Info: max_grad_norm arg from hparam file is used\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - 698.9k trainable parameters in BaseASR\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/transformer/Task_1/save/CKPT+2024-02-18+16-40-00+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/transformer/Task_1/save/CKPT+2024-02-18+16-40-00+00\n",
            "Loaded the average\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 328/328 [00:26<00:00, 12.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - Epoch loaded: 1 - test loss: 2.65e+02, test CER: 1.00e+02, test WER: 1.00e+02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "265.0717240310293"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "hyperparams = global_hyperparams + task_hyperparameters\n",
        "hparams = load_hyperpyyaml(hyperparams)\n",
        "\n",
        "# Create experiment directory\n",
        "sb.create_experiment_directory(\n",
        "    experiment_directory=hparams[\"output_folder\"],\n",
        "    overrides=None,\n",
        ")\n",
        "\n",
        "# Here we create the datasets objects as well as tokenization and encoding\n",
        "(\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    test_data,\n",
        "    tokenizer\n",
        ") = dataio_prepare(hparams)\n",
        "\n",
        "# We download the pretrained LM from HuggingFace (or elsewhere depending on\n",
        "# the path given in the YAML file). The tokenizer is loaded at the same time.\n",
        "run_on_main(hparams[\"pretrainer\"].collect_files)\n",
        "hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
        "\n",
        "# Trainer initialization\n",
        "asr_brain = BaseASR(\n",
        "    modules=hparams[\"modules\"],\n",
        "    opt_class=hparams[\"Adam\"],\n",
        "    hparams=hparams,\n",
        "    checkpointer=hparams[\"checkpointer\"],\n",
        "    run_opts=run_opts,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# adding objects to trainer:\n",
        "train_dataloader_opts = hparams[\"train_dataloader_opts\"]\n",
        "valid_dataloader_opts = hparams[\"valid_dataloader_opts\"]\n",
        "\n",
        "# Training\n",
        "asr_brain.fit(\n",
        "    asr_brain.hparams.epoch_counter,\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    train_loader_kwargs=train_dataloader_opts,\n",
        "    valid_loader_kwargs=valid_dataloader_opts\n",
        ")\n",
        "\n",
        "# Testing\n",
        "asr_brain.hparams.test_wer_file = asr_brain.hparams.wer_file\n",
        "asr_brain.evaluate(\n",
        "    test_data,\n",
        "    max_key=\"epoch\",\n",
        "    test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHIRKDrPPRGW"
      },
      "source": [
        "# Part II(A): CTC is all you need\n",
        "In this section, you will train a **6-layer Conformer** encoder with both  `CTC` and `inter-CTC` losses.\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdoAx39-pSAw"
      },
      "outputs": [],
      "source": [
        "class ASR_2A(BaseASR):\n",
        "    def __init__(\n",
        "        self, *args, **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.inter_ctc_weight = self.hparams.interctc_weight\n",
        "        self.intermediate_layers = [int(layer) for layer in self.hparams.intermediate_layers.split(',')]\n",
        "\n",
        "        # Variable to hold intermediate logits for interCTC loss calculation\n",
        "        self.inter_logits = []\n",
        "\n",
        "        # TODO: Define a helper function get_intermediate_output for the forward hook\n",
        "        def get_intermediate_output(module, input, output):\n",
        "            # print(\"The tuple length is \", output[0].size(), output[1].size())\n",
        "            self.inter_logits.append(output.detach())\n",
        "\n",
        "        # TODO: Register hooks for all the intermediate encoder layers of interest.\n",
        "        # TODO: Refer to register_forward_hook (https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html)\n",
        "        # TODO: Save all the hooks in a list self.hooks that you can remove later from the module\n",
        "        self.hooks = []\n",
        "        for layer in self.intermediate_layers:\n",
        "          self.hooks.append(self.hparams.model[1].encoder.layers[layer-1].norm2.register_forward_hook(get_intermediate_output))\n",
        "\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Performs a forward pass through the encoder\"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, wav_lens = batch.sig\n",
        "        tokens_bos, _ = batch.tokens_bos\n",
        "\n",
        "        # compute features\n",
        "        feats = self.hparams.compute_features(wavs)\n",
        "        current_epoch = self.hparams.epoch_counter.current\n",
        "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
        "\n",
        "        # forward modules\n",
        "        src = self.modules.CNN(feats)\n",
        "\n",
        "        assert len(self.inter_logits) == 0, \"self.inter_logits should be empty as we haven't done a forward pass yet\"\n",
        "        enc_out, _ = self.modules.Transformer(\n",
        "            src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index,\n",
        "        )\n",
        "\n",
        "        # Compute final layer logit\n",
        "        logits = self.modules.ctc_lin(enc_out)\n",
        "        p_ctc = self.hparams.log_softmax(logits)\n",
        "\n",
        "        # TODO: Append all the intermediate layer logits to the following list: inter_p_ctc\n",
        "        # TODO: Go through all the layers in intermediate_layers. Note that the comma-separated list in intermediate_layers is 1-indexed.\n",
        "        # TODO: Complete code below to populate inter_p_ctc\n",
        "\n",
        "        inter_p_ctc = [self.hparams.log_softmax(self.modules.ctc_lin(i_logits)) for i_logits in self.inter_logits]        # Flush the logits saved during last forward pass.\n",
        "        self.inter_logits = []\n",
        "\n",
        "        # Compute outputs\n",
        "        hyps = None\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            assert len(inter_p_ctc) != 0 , \"inter_p_ctc should NOT be empty as forward pass is already done\"\n",
        "            hyps = None\n",
        "        else:\n",
        "            hyps = sb.decoders.ctc_greedy_decode(\n",
        "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
        "            )\n",
        "\n",
        "        return p_ctc, inter_p_ctc, wav_lens, hyps\n",
        "\n",
        "    def on_evaluate_start(self, max_key=None, min_key=None):\n",
        "        \"\"\"Performs sanity operations before inferencing on the test set.\"\"\"\n",
        "        if self.checkpointer is not None:\n",
        "            self.checkpointer.recover_if_possible(\n",
        "                max_key=max_key,\n",
        "                min_key=min_key,\n",
        "                device=torch.device(self.device),\n",
        "            )\n",
        "\n",
        "        # Deregister hooks here as they are not needed during evaluation\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the CTC + inter-CTC loss given predictions and targets.\"\"\"\n",
        "\n",
        "        (p_ctc, inter_p_ctc, wav_lens, hyps,) = predictions\n",
        "\n",
        "        ids = batch.id\n",
        "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
        "        tokens, tokens_lens = batch.tokens\n",
        "\n",
        "        # TODO: Compute inter-CTC loss\n",
        "        #loss_inter_ctc = sum(inter_p_ctc)\n",
        "\n",
        "        loss_inter_ctc = sum([self.hparams.ctc_cost(\n",
        "            inter_ctc_i, tokens, wav_lens, tokens_lens\n",
        "        ).sum() for inter_ctc_i in inter_p_ctc])\n",
        "\n",
        "        # TODO: Write code to appropriately accumulate the inter-CTC loss in loss_inter_ctc\n",
        "        # TODO: using the softmax probabilities saved for each intermediate layer in inter_p_ctc\n",
        "\n",
        "        # Compute final layer CTC loss\n",
        "        loss_ctc = self.hparams.ctc_cost(\n",
        "            p_ctc, tokens, wav_lens, tokens_lens\n",
        "        ).sum()\n",
        "\n",
        "        # Compute final loss as a weighted combination of inter-CTC and CTC\n",
        "        loss = self.inter_ctc_weight * loss_inter_ctc + (1 - self.inter_ctc_weight) * loss_ctc\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            # Decode token terms to words\n",
        "            predicted_words = [\n",
        "                    self.tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in hyps\n",
        "                ]\n",
        "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
        "            self.wer_metric.append(ids, predicted_words, target_words)\n",
        "            self.cer_metric.append(ids, predicted_words, target_words)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoIb-4TdnyC4"
      },
      "outputs": [],
      "source": [
        "task_hyperparameters = \"\"\"\n",
        "\n",
        "# Setup the directory to host experiment results\n",
        "output_folder: !ref results/transformer/Part_2A\n",
        "wer_file: !ref <output_folder>/wer.txt\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "interctc_weight: 0.3\n",
        "intermediate_layers: '2,4'\n",
        "\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        noam_scheduler: !ref <noam_annealing>\n",
        "        normalizer: !ref <normalize>\n",
        "        counter: !ref <epoch_counter>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX-kl1dKMx-Q",
        "outputId": "dea1e3aa-7516-4293-d45b-c10364999336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/transformer/Part_2A\n",
            "speechbrain.pretrained.fetching - Fetch tokenizer.ckpt: Using existing file/symlink in model_checkpoints/tokenizer.ckpt.\n",
            "speechbrain.utils.parameter_transfer - Set local path in self.paths[tokenizer] = model_checkpoints/tokenizer.ckpt\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: tokenizer\n",
            "speechbrain.utils.parameter_transfer - Redirecting (loading from local path): model_checkpoints/tokenizer.ckpt -> model_checkpoints/tokenizer.ckpt\n",
            "speechbrain.core - Info: max_grad_norm arg from hparam file is used\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - 698.9k trainable parameters in ASR_2A\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/transformer/Part_2A/save/CKPT+2024-02-18+16-41-26+00\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 328/328 [00:28<00:00, 11.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - Epoch loaded: 1 - test loss: 1.91e+02, test CER: 1.00e+02, test WER: 1.00e+02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191.32170388756708"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "hyperparams = global_hyperparams + task_hyperparameters\n",
        "hparams = load_hyperpyyaml(hyperparams)\n",
        "\n",
        "# Create experiment directory\n",
        "sb.create_experiment_directory(\n",
        "    experiment_directory=hparams[\"output_folder\"],\n",
        "    overrides=None,\n",
        ")\n",
        "\n",
        "# We download the pretrained LM from HuggingFace (or elsewhere depending on\n",
        "# the path given in the YAML file). The tokenizer is loaded at the same time.\n",
        "run_on_main(hparams[\"pretrainer\"].collect_files)\n",
        "hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
        "\n",
        "# Trainer initialization\n",
        "asr_brain = ASR_2A(\n",
        "    modules=hparams[\"modules\"],\n",
        "    opt_class=hparams[\"Adam\"],\n",
        "    hparams=hparams,\n",
        "    checkpointer=hparams[\"checkpointer\"],\n",
        "    run_opts=run_opts,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# adding objects to trainer:\n",
        "train_dataloader_opts = hparams[\"train_dataloader_opts\"]\n",
        "valid_dataloader_opts = hparams[\"valid_dataloader_opts\"]\n",
        "\n",
        "# Training\n",
        "asr_brain.fit(\n",
        "    asr_brain.hparams.epoch_counter,\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    train_loader_kwargs=train_dataloader_opts,\n",
        "    valid_loader_kwargs=valid_dataloader_opts\n",
        ")\n",
        "\n",
        "# Testing\n",
        "\n",
        "asr_brain.hparams.test_wer_file = asr_brain.hparams.wer_file\n",
        "asr_brain.evaluate(\n",
        "    test_data,\n",
        "    max_key=\"ACC\",\n",
        "    test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06q84-WOPblt"
      },
      "source": [
        "# Task 2.2: The PowerConv Module\n",
        "In this section, we will update the Conformer encoder by replacing Convolution with **PowerConv**. Rest of the architecture remains the same. Note this will be added on top of inter-CTC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwQRTzBE3Qga"
      },
      "outputs": [],
      "source": [
        "task_hyperparameters = \"\"\"\n",
        "\n",
        "# Setup the directory to host experiment results\n",
        "output_folder: !ref results/transformer/Task_2B\n",
        "wer_file: !ref <output_folder>/wer.txt\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "interctc_weight: 0.3\n",
        "intermediate_layers: '2,4'\n",
        "\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        noam_scheduler: !ref <noam_annealing>\n",
        "        normalizer: !ref <normalize>\n",
        "        counter: !ref <epoch_counter>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gneZWFH7TmC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import speechbrain as sb\n",
        "\n",
        "from speechbrain.nnet.attention import (\n",
        "    RelPosMHAXL,\n",
        "    MultiheadAttention,\n",
        "    PositionalwiseFeedForward,\n",
        ")\n",
        "from speechbrain.nnet.normalization import LayerNorm\n",
        "from speechbrain.nnet.activations import Swish\n",
        "from speechbrain.nnet.CNN import Conv1d\n",
        "\n",
        "class PowerConv(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        kernel_size=31,\n",
        "        dropout=0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # We upsample our input by a factor of 2 to\n",
        "        input_size = input_size*2\n",
        "        self.input_size = input_size\n",
        "\n",
        "        n_channels = input_size // 2  # split input channels\n",
        "\n",
        "        # TODO: First projection feedforward layer to upsample the input\n",
        "        self.channel_proj1 = torch.nn.Linear(n_channels, input_size) ### TODO: Fill in\n",
        "        self.norm = LayerNorm(n_channels) ### TODO: Layer normalization\n",
        "        self.conv = Conv1d(in_channels = n_channels, out_channels = n_channels, groups = n_channels, kernel_size = kernel_size, stride = 1) ### TODO: Initialize depthwise 1D Convolution\n",
        "        ### TODO: Use the groups parameter in the Conv1D class and set it to n_channels\n",
        "        ### TODO: Note that this conv operator does not change the feature dimensionality.\n",
        "        ### TODO: Use the appropriate value for the padding parameter in the Conv1D class to keep the feature dimensionality unaltered.\n",
        "\n",
        "        # TODO: Second projection feedforward layer\n",
        "        self.channel_proj2 = torch.nn.Linear(n_channels, n_channels) ### TODO: Fill in\n",
        "        self.channel_proj3 = torch.nn.Linear(n_channels, n_channels) ### TODO: Fill in\n",
        "        self.channel_proj0 = torch.nn.Linear(n_channels, n_channels)\n",
        "        self.dropout = torch.nn.Dropout(0.2) ### TODO: Dropout layer\n",
        "\n",
        "        # Initialize convolution with ones.\n",
        "        torch.nn.init.ones_(self.conv.conv.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            Shape of input x: (B, T, D)\n",
        "            Return output of shape: (B, T, D)\n",
        "        \"\"\"\n",
        "        # TODO: Implement the PowerConv module as described in the assignment pdf\n",
        "        x = self.norm(x)\n",
        "        self.channel_proj0(x)\n",
        "        residual = x\n",
        "        x = self.channel_proj1(x)\n",
        "        v1 = x[:,:,:self.input_size//2]\n",
        "        v2 = x[:,:,self.input_size//2:]\n",
        "        v2 = self.norm(v2)\n",
        "        v2 = self.conv(v2)\n",
        "        z = v1 * v2\n",
        "        z = self.dropout(z)\n",
        "        z = self.channel_proj2(z)\n",
        "        z += residual\n",
        "        z = self.dropout(z)\n",
        "        z = self.channel_proj3(z)\n",
        "        return z\n",
        "\n",
        "class CustomConformerEncoderLayer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model,\n",
        "        d_ffn,\n",
        "        nhead,\n",
        "        kernel_size=31,\n",
        "        kdim=None,\n",
        "        vdim=None,\n",
        "        activation=Swish,\n",
        "        bias=True,\n",
        "        dropout=0.0,\n",
        "        causal=False,\n",
        "        attention_type=\"RelPosMHAXL\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self attention block\n",
        "        if attention_type == \"regularMHA\":\n",
        "            self.mha_layer = MultiheadAttention(\n",
        "                nhead=nhead,\n",
        "                d_model=d_model,\n",
        "                dropout=dropout,\n",
        "                kdim=kdim,\n",
        "                vdim=vdim,\n",
        "            )\n",
        "        elif attention_type == \"RelPosMHAXL\":\n",
        "            # transformerXL style positional encoding\n",
        "            self.mha_layer = RelPosMHAXL(\n",
        "                num_heads=nhead,\n",
        "                embed_dim=d_model,\n",
        "                dropout=dropout,\n",
        "                mask_pos_future=causal,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Unknown attention type\")\n",
        "\n",
        "        # Create instance of our custom convolution block\n",
        "        self.convolution_module = PowerConv(\n",
        "            d_model, kernel_size, dropout\n",
        "        )\n",
        "        self.convolution_module2 = PowerConv(\n",
        "            d_model, kernel_size, dropout\n",
        "        )\n",
        "\n",
        "        # Feed forward macaron block\n",
        "        self.ffn_module1 = torch.nn.Sequential(\n",
        "            torch.nn.LayerNorm(d_model),\n",
        "            PositionalwiseFeedForward(\n",
        "                d_ffn=d_ffn,\n",
        "                input_size=d_model,\n",
        "                dropout=dropout,\n",
        "                activation=activation,\n",
        "            ),\n",
        "            torch.nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Feed forward block\n",
        "        self.ffn_module2 = torch.nn.Sequential(\n",
        "            torch.nn.LayerNorm(d_model),\n",
        "            PositionalwiseFeedForward(\n",
        "                d_ffn=d_ffn,\n",
        "                input_size=d_model,\n",
        "                dropout=dropout,\n",
        "                activation=activation,\n",
        "            ),\n",
        "            torch.nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        self.drop = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        src_mask = None,\n",
        "        src_key_padding_mask = None,\n",
        "        pos_embs = None,\n",
        "    ):\n",
        "        conv_mask = None\n",
        "        if src_key_padding_mask is not None:\n",
        "            conv_mask = src_key_padding_mask.unsqueeze(-1)\n",
        "\n",
        "        # ffn module\n",
        "        x = x + 0.5 * self.ffn_module1(x)\n",
        "\n",
        "        # muti-head attention module\n",
        "        skip = x\n",
        "        x = self.norm1(x)\n",
        "        x, self_attn = self.mha_layer(\n",
        "            x,\n",
        "            x,\n",
        "            x,\n",
        "            attn_mask=src_mask,\n",
        "            key_padding_mask=src_key_padding_mask,\n",
        "            pos_embs=pos_embs,\n",
        "        )\n",
        "        x = x + skip\n",
        "\n",
        "        # convolution module\n",
        "        x = x + self.convolution_module(x)\n",
        "        x = x + self.convolution_module2(x)\n",
        "\n",
        "        # ffn module\n",
        "        x = self.norm2(x + 0.5 * self.ffn_module2(x))\n",
        "\n",
        "        return x, self_attn\n",
        "\n",
        "\n",
        "class CustomConformerEncoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers,\n",
        "        d_model,\n",
        "        d_ffn,\n",
        "        nhead,\n",
        "        kernel_size=31,\n",
        "        kdim=None,\n",
        "        vdim=None,\n",
        "        activation=Swish,\n",
        "        bias=True,\n",
        "        dropout=0.0,\n",
        "        causal=False,\n",
        "        attention_type=\"RelPosMHAXL\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create layers using our custom encoder layer that utilizes PowerConv\n",
        "        self.layers = torch.nn.ModuleList(\n",
        "            [\n",
        "                CustomConformerEncoderLayer(\n",
        "                    d_ffn=d_ffn,\n",
        "                    nhead=nhead,\n",
        "                    d_model=d_model,\n",
        "                    kdim=kdim,\n",
        "                    vdim=vdim,\n",
        "                    dropout=dropout,\n",
        "                    activation=activation,\n",
        "                    kernel_size=kernel_size,\n",
        "                    bias=bias,\n",
        "                    causal=causal,\n",
        "                    attention_type=attention_type,\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.norm = LayerNorm(d_model, eps=1e-6)\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        src_mask = None,\n",
        "        src_key_padding_mask = None,\n",
        "        pos_embs = None,\n",
        "    ):\n",
        "\n",
        "        if self.attention_type == \"RelPosMHAXL\":\n",
        "            if pos_embs is None:\n",
        "                raise ValueError(\n",
        "                    \"The chosen attention type for the Conformer is RelPosMHAXL. For this attention type, the positional embeddings are mandatory\"\n",
        "                )\n",
        "\n",
        "        output = src\n",
        "        attention_lst = []\n",
        "        # Loop through the encoder layers\n",
        "        for enc_layer in self.layers:\n",
        "            output, attention = enc_layer(\n",
        "                output,\n",
        "                src_mask=src_mask,\n",
        "                src_key_padding_mask=src_key_padding_mask,\n",
        "                pos_embs=pos_embs,\n",
        "            )\n",
        "            attention_lst.append(attention)\n",
        "        output = self.norm(output)\n",
        "\n",
        "        return output, attention_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wgsf3f27VuR"
      },
      "outputs": [],
      "source": [
        "class ASR_2B(ASR_2A):\n",
        "    def __init__(\n",
        "        self, device=\"cpu\", *args, **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Remove the old hooks as they are not useful\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        # Instantiate our custom encoder that uses PowerConv\n",
        "        encoder = CustomConformerEncoder(\n",
        "            nhead=self.hparams.nhead,\n",
        "            num_layers=self.hparams.num_encoder_layers,\n",
        "            d_ffn=self.hparams.d_ffn,\n",
        "            d_model=self.hparams.d_model,\n",
        "            dropout=self.hparams.transformer_dropout,\n",
        "            activation=self.hparams.activation,\n",
        "            attention_type=self.hparams.attention_type,\n",
        "        ).to(device)\n",
        "\n",
        "        # Replace the standard encoder with our encoder\n",
        "        self.modules.Transformer.encoder = encoder\n",
        "        #print(self.modules.Transformer.encoder)\n",
        "\n",
        "        self.inter_logits = []\n",
        "        def get_intermediate_output(module, input, output):\n",
        "            self.inter_logits.append(output.detach())\n",
        "\n",
        "        self.hooks = []\n",
        "        # TODO: Copy this code from your implemention in Part II(A) within the __init__ function of ASR_2A that populates self.hooks\n",
        "        for layer in self.intermediate_layers:\n",
        "          #print(self.hparams.model[1].encoder.layers)\n",
        "          self.hooks.append(self.hparams.model[1].encoder.layers[layer-1].norm2.register_forward_hook(get_intermediate_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "s1pcomwa-zwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e5e18d-8497-4f98-a2b9-564a1d4092a9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/transformer/Task_2B\n",
            "speechbrain.pretrained.fetching - Fetch tokenizer.ckpt: Using existing file/symlink in model_checkpoints/tokenizer.ckpt.\n",
            "speechbrain.utils.parameter_transfer - Set local path in self.paths[tokenizer] = model_checkpoints/tokenizer.ckpt\n",
            "speechbrain.utils.parameter_transfer - Loading pretrained files for: tokenizer\n",
            "speechbrain.utils.parameter_transfer - Redirecting (loading from local path): model_checkpoints/tokenizer.ckpt -> model_checkpoints/tokenizer.ckpt\n",
            "speechbrain.core - Info: max_grad_norm arg from hparam file is used\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - 698.9k trainable parameters in ASR_2B\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.31it/s, train_loss=1.02e+3]\n",
            "100%|██████████| 137/137 [00:09<00:00, 14.42it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 1, lr: 1.26e-04, steps: 190, optimizer: Adam - train loss: 1.02e+03 - valid loss: 4.39e+02, valid CER: 1.00e+02, valid WER: 1.00e+02\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+16-57-23+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.30it/s, train_loss=694]\n",
            "100%|██████████| 137/137 [00:09<00:00, 14.08it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 2, lr: 2.53e-04, steps: 380, optimizer: Adam - train loss: 6.94e+02 - valid loss: 3.54e+02, valid CER: 1.00e+02, valid WER: 1.00e+02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+16-58-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 190/190 [00:29<00:00,  6.44it/s, train_loss=590]\n",
            "100%|██████████| 137/137 [00:09<00:00, 14.21it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 3, lr: 3.79e-04, steps: 570, optimizer: Adam - train loss: 5.90e+02 - valid loss: 3.38e+02, valid CER: 99.98, valid WER: 1.00e+02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+16-58-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 190/190 [00:29<00:00,  6.36it/s, train_loss=672]\n",
            "100%|██████████| 137/137 [00:09<00:00, 13.95it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 4, lr: 5.06e-04, steps: 760, optimizer: Adam - train loss: 6.72e+02 - valid loss: 3.63e+02, valid CER: 100.00, valid WER: 1.00e+02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+16-59-22+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 190/190 [00:30<00:00,  6.25it/s, train_loss=633]\n",
            "100%|██████████| 137/137 [00:09<00:00, 14.57it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 5, lr: 6.33e-04, steps: 950, optimizer: Adam - train loss: 6.33e+02 - valid loss: 3.26e+02, valid CER: 99.90, valid WER: 1.00e+02\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-00-02+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.36it/s, train_loss=562]\n",
            "100%|██████████| 137/137 [00:10<00:00, 13.04it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 6, lr: 7.59e-04, steps: 1140, optimizer: Adam - train loss: 5.62e+02 - valid loss: 2.91e+02, valid CER: 95.47, valid WER: 99.57\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-00-43+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.42it/s, train_loss=509]\n",
            "100%|██████████| 137/137 [00:12<00:00, 11.29it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 7, lr: 8.86e-04, steps: 1330, optimizer: Adam - train loss: 5.09e+02 - valid loss: 2.69e+02, valid CER: 77.85, valid WER: 95.65\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-01-24+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.26it/s, train_loss=474]\n",
            "100%|██████████| 137/137 [00:13<00:00, 10.25it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 8, lr: 9.94e-04, steps: 1520, optimizer: Adam - train loss: 4.74e+02 - valid loss: 2.48e+02, valid CER: 68.98, valid WER: 92.87\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-02-08+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 9\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.41it/s, train_loss=444]\n",
            "100%|██████████| 137/137 [00:15<00:00,  9.10it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 9, lr: 9.37e-04, steps: 1710, optimizer: Adam - train loss: 4.44e+02 - valid loss: 2.32e+02, valid CER: 62.06, valid WER: 90.52\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-02-53+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.37it/s, train_loss=421]\n",
            "100%|██████████| 137/137 [00:14<00:00,  9.47it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 10, lr: 8.89e-04, steps: 1900, optimizer: Adam - train loss: 4.21e+02 - valid loss: 2.24e+02, valid CER: 59.53, valid WER: 89.36\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-03-38+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.epoch_loop - Going into epoch 11\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.27it/s, train_loss=403]\n",
            "100%|██████████| 137/137 [00:14<00:00,  9.18it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 11, lr: 8.47e-04, steps: 2090, optimizer: Adam - train loss: 4.03e+02 - valid loss: 2.15e+02, valid CER: 55.47, valid WER: 88.05\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-04-24+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+16-57-23+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.34it/s, train_loss=389]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.76it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 12, lr: 8.11e-04, steps: 2280, optimizer: Adam - train loss: 3.89e+02 - valid loss: 2.09e+02, valid CER: 54.17, valid WER: 87.35\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-05-10+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+16-58-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.33it/s, train_loss=377]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.22it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 13, lr: 7.79e-04, steps: 2470, optimizer: Adam - train loss: 3.77e+02 - valid loss: 2.03e+02, valid CER: 50.66, valid WER: 86.14\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-05-57+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+16-58-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:29<00:00,  6.39it/s, train_loss=367]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.68it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 14, lr: 7.51e-04, steps: 2660, optimizer: Adam - train loss: 3.67e+02 - valid loss: 1.98e+02, valid CER: 49.52, valid WER: 84.92\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-06-43+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+16-59-22+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.19it/s, train_loss=358]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.66it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.train_logger - epoch: 15, lr: 7.26e-04, steps: 2850, optimizer: Adam - train loss: 3.58e+02 - valid loss: 1.96e+02, valid CER: 47.96, valid WER: 84.23\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-07-29+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-00-02+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.30it/s, train_loss=350]\n",
            "100%|██████████| 137/137 [00:15<00:00,  8.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 16, lr: 7.03e-04, steps: 3040, optimizer: Adam - train loss: 3.50e+02 - valid loss: 1.93e+02, valid CER: 46.27, valid WER: 83.47\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-08-16+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-00-43+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:31<00:00,  6.13it/s, train_loss=343]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 17, lr: 6.82e-04, steps: 3230, optimizer: Adam - train loss: 3.43e+02 - valid loss: 1.89e+02, valid CER: 44.57, valid WER: 82.77\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-09-04+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-01-24+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.25it/s, train_loss=337]\n",
            "100%|██████████| 137/137 [00:16<00:00,  8.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 18, lr: 6.62e-04, steps: 3420, optimizer: Adam - train loss: 3.37e+02 - valid loss: 1.88e+02, valid CER: 43.85, valid WER: 81.83\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-09-51+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-02-08+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.31it/s, train_loss=331]\n",
            "100%|██████████| 137/137 [00:17<00:00,  8.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 19, lr: 6.45e-04, steps: 3610, optimizer: Adam - train loss: 3.31e+02 - valid loss: 1.85e+02, valid CER: 42.45, valid WER: 80.91\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-10-39+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-02-53+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.26it/s, train_loss=325]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 20, lr: 6.28e-04, steps: 3800, optimizer: Adam - train loss: 3.25e+02 - valid loss: 1.84e+02, valid CER: 42.15, valid WER: 80.72\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-11-27+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-03-38+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.31it/s, train_loss=320]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 21, lr: 6.13e-04, steps: 3990, optimizer: Adam - train loss: 3.20e+02 - valid loss: 1.81e+02, valid CER: 41.17, valid WER: 80.23\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-12-15+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-04-24+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.28it/s, train_loss=315]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 22, lr: 5.99e-04, steps: 4180, optimizer: Adam - train loss: 3.15e+02 - valid loss: 1.81e+02, valid CER: 40.55, valid WER: 79.19\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-13-03+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-05-10+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.29it/s, train_loss=312]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 23, lr: 5.86e-04, steps: 4370, optimizer: Adam - train loss: 3.12e+02 - valid loss: 1.80e+02, valid CER: 40.44, valid WER: 78.65\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-13-51+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-05-57+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.22it/s, train_loss=307]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 24, lr: 5.74e-04, steps: 4560, optimizer: Adam - train loss: 3.07e+02 - valid loss: 1.78e+02, valid CER: 39.61, valid WER: 78.68\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-14-40+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-06-43+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.18it/s, train_loss=304]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 25, lr: 5.62e-04, steps: 4750, optimizer: Adam - train loss: 3.04e+02 - valid loss: 1.77e+02, valid CER: 39.09, valid WER: 78.26\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-15-29+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-07-29+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:31<00:00,  6.11it/s, train_loss=301]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 26, lr: 5.51e-04, steps: 4940, optimizer: Adam - train loss: 3.01e+02 - valid loss: 1.75e+02, valid CER: 38.32, valid WER: 77.24\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-16-17+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-08-16+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:31<00:00,  6.10it/s, train_loss=297]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 27, lr: 5.41e-04, steps: 5130, optimizer: Adam - train loss: 2.97e+02 - valid loss: 1.75e+02, valid CER: 37.88, valid WER: 77.64\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-17-06+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-09-04+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:31<00:00,  6.10it/s, train_loss=294]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 28, lr: 5.31e-04, steps: 5320, optimizer: Adam - train loss: 2.94e+02 - valid loss: 1.74e+02, valid CER: 37.84, valid WER: 77.03\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-17-55+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-09-51+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.14it/s, train_loss=291]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 29, lr: 5.22e-04, steps: 5510, optimizer: Adam - train loss: 2.91e+02 - valid loss: 1.73e+02, valid CER: 37.24, valid WER: 76.70\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-18-44+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-10-39+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:31<00:00,  6.10it/s, train_loss=288]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 30, lr: 5.13e-04, steps: 5700, optimizer: Adam - train loss: 2.88e+02 - valid loss: 1.72e+02, valid CER: 37.03, valid WER: 76.29\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-19-33+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-11-27+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:31<00:00,  6.11it/s, train_loss=285]\n",
            "100%|██████████| 137/137 [00:17<00:00,  8.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 31, lr: 5.05e-04, steps: 5890, optimizer: Adam - train loss: 2.85e+02 - valid loss: 1.72e+02, valid CER: 36.99, valid WER: 76.09\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-20-22+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-12-15+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:31<00:00,  6.06it/s, train_loss=283]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 32, lr: 4.97e-04, steps: 6080, optimizer: Adam - train loss: 2.83e+02 - valid loss: 1.72e+02, valid CER: 36.51, valid WER: 75.29\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-21-11+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-13-03+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:31<00:00,  6.10it/s, train_loss=281]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 33, lr: 4.89e-04, steps: 6270, optimizer: Adam - train loss: 2.81e+02 - valid loss: 1.70e+02, valid CER: 36.07, valid WER: 75.26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-22-01+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-13-51+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.18it/s, train_loss=279]\n",
            "100%|██████████| 137/137 [00:17<00:00,  7.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 34, lr: 4.82e-04, steps: 6460, optimizer: Adam - train loss: 2.79e+02 - valid loss: 1.71e+02, valid CER: 36.08, valid WER: 75.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-22-50+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-14-40+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 190/190 [00:30<00:00,  6.20it/s, train_loss=276]\n",
            "100%|██████████| 137/137 [00:18<00:00,  7.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - epoch: 35, lr: 4.75e-04, steps: 6650, optimizer: Adam - train loss: 2.76e+02 - valid loss: 1.69e+02, valid CER: 35.62, valid WER: 74.54\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-23-40+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+17-15-29+00\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 328/328 [00:49<00:00,  6.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speechbrain.utils.train_logger - Epoch loaded: 35 - test loss: 80.17, test CER: 36.19, test WER: 75.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80.17316601916059"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "hyperparams = global_hyperparams + task_hyperparameters\n",
        "hparams = load_hyperpyyaml(hyperparams)\n",
        "\n",
        "# Create experiment directory\n",
        "sb.create_experiment_directory(\n",
        "    experiment_directory=hparams[\"output_folder\"],\n",
        "    overrides=None,\n",
        ")\n",
        "\n",
        "# We download the pretrained LM from HuggingFace (or elsewhere depending on\n",
        "# the path given in the YAML file). The tokenizer is loaded at the same time.\n",
        "run_on_main(hparams[\"pretrainer\"].collect_files)\n",
        "hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
        "\n",
        "# Trainer initialization\n",
        "asr_brain = ASR_2B(\n",
        "    modules=hparams[\"modules\"],\n",
        "    opt_class=hparams[\"Adam\"],\n",
        "    hparams=hparams,\n",
        "    checkpointer=hparams[\"checkpointer\"],\n",
        "    run_opts=run_opts,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# adding objects to trainer:\n",
        "train_dataloader_opts = hparams[\"train_dataloader_opts\"]\n",
        "valid_dataloader_opts = hparams[\"valid_dataloader_opts\"]\n",
        "\n",
        "# Training\n",
        "asr_brain.fit(\n",
        "    asr_brain.hparams.epoch_counter,\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    train_loader_kwargs=train_dataloader_opts,\n",
        "    valid_loader_kwargs=valid_dataloader_opts\n",
        ")\n",
        "\n",
        "# Testing\n",
        "\n",
        "asr_brain.hparams.test_wer_file = asr_brain.hparams.wer_file\n",
        "asr_brain.evaluate(\n",
        "    test_data,\n",
        "    max_key=\"ACC\",\n",
        "    test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'folder_path' with the path of the folder you want to download\n",
        "folder_path = '/content/speechbrain/results/transformer/Task_2B/save/CKPT+2024-02-18+17-23-40+00'\n",
        "shutil.make_archive('/content/task2B_model_Checkpoints', 'zip', folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-RpF2mbRQE6-",
        "outputId": "72c552c1-7215-40a7-fea0-347581f90ea2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/task2B_model_Checkpoints.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/task2B_model_Checkpoints.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dkV5dCO9QJif",
        "outputId": "1f86d349-8773-4882-b5a8-242df518188c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f62add46-1d53-4d2e-9c26-893a5351bc37\", \"task2B_model_Checkpoints.zip\", 9999796)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSCJpvuUQ8Vl",
        "outputId": "8ee66e63-7b7a-472c-debe-945c4cf0396e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blind_path = '/content/drive/MyDrive/blindtest.zip'\n",
        "!unzip '/content/drive/MyDrive/blindtest.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj7_X0riRM7t",
        "outputId": "35587f6a-fbd9-4b82-ef56-68094c0c060e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/blindtest.zip\n",
            "   creating: blindtest/test/\n",
            "  inflating: blindtest/test/116-288045-0000.flac  \n",
            "  inflating: blindtest/test/116-288045-0001.flac  \n",
            "  inflating: blindtest/test/116-288045-0002.flac  \n",
            "  inflating: blindtest/test/116-288045-0003.flac  \n",
            "  inflating: blindtest/test/116-288045-0004.flac  \n",
            "  inflating: blindtest/test/116-288045-0005.flac  \n",
            "  inflating: blindtest/test/116-288045-0006.flac  \n",
            "  inflating: blindtest/test/116-288045-0007.flac  \n",
            "  inflating: blindtest/test/116-288045-0008.flac  \n",
            "  inflating: blindtest/test/116-288045-0009.flac  \n",
            "  inflating: blindtest/test/116-288045-0010.flac  \n",
            "  inflating: blindtest/test/116-288045-0011.flac  \n",
            "  inflating: blindtest/test/116-288045-0012.flac  \n",
            "  inflating: blindtest/test/116-288045-0013.flac  \n",
            "  inflating: blindtest/test/116-288045-0014.flac  \n",
            "  inflating: blindtest/test/116-288045-0015.flac  \n",
            "  inflating: blindtest/test/116-288045-0016.flac  \n",
            "  inflating: blindtest/test/116-288045-0017.flac  \n",
            "  inflating: blindtest/test/116-288045-0018.flac  \n",
            "  inflating: blindtest/test/116-288045-0019.flac  \n",
            "  inflating: blindtest/test/116-288045-0020.flac  \n",
            "  inflating: blindtest/test/116-288045-0021.flac  \n",
            "  inflating: blindtest/test/116-288045-0022.flac  \n",
            "  inflating: blindtest/test/116-288045-0023.flac  \n",
            "  inflating: blindtest/test/116-288045-0024.flac  \n",
            "  inflating: blindtest/test/116-288045-0025.flac  \n",
            "  inflating: blindtest/test/116-288045-0026.flac  \n",
            "  inflating: blindtest/test/116-288045-0027.flac  \n",
            "  inflating: blindtest/test/116-288045-0028.flac  \n",
            "  inflating: blindtest/test/116-288045-0029.flac  \n",
            "  inflating: blindtest/test/116-288045-0030.flac  \n",
            "  inflating: blindtest/test/116-288045-0031.flac  \n",
            "  inflating: blindtest/test/116-288045-0032.flac  \n",
            "  inflating: blindtest/test/116-288046-0000.flac  \n",
            "  inflating: blindtest/test/116-288046-0001.flac  \n",
            "  inflating: blindtest/test/116-288046-0002.flac  \n",
            "  inflating: blindtest/test/116-288046-0003.flac  \n",
            "  inflating: blindtest/test/116-288046-0004.flac  \n",
            "  inflating: blindtest/test/116-288046-0005.flac  \n",
            "  inflating: blindtest/test/116-288046-0006.flac  \n",
            "  inflating: blindtest/test/116-288046-0007.flac  \n",
            "  inflating: blindtest/test/116-288046-0008.flac  \n",
            "  inflating: blindtest/test/116-288046-0009.flac  \n",
            "  inflating: blindtest/test/116-288046-0010.flac  \n",
            "  inflating: blindtest/test/116-288046-0011.flac  \n",
            "  inflating: blindtest/test/116-288047-0000.flac  \n",
            "  inflating: blindtest/test/116-288047-0001.flac  \n",
            "  inflating: blindtest/test/116-288047-0002.flac  \n",
            "  inflating: blindtest/test/116-288047-0003.flac  \n",
            "  inflating: blindtest/test/116-288047-0004.flac  \n",
            "  inflating: blindtest/test/116-288047-0005.flac  \n",
            "  inflating: blindtest/test/116-288047-0006.flac  \n",
            "  inflating: blindtest/test/116-288047-0007.flac  \n",
            "  inflating: blindtest/test/116-288047-0008.flac  \n",
            "  inflating: blindtest/test/116-288047-0009.flac  \n",
            "  inflating: blindtest/test/116-288047-0010.flac  \n",
            "  inflating: blindtest/test/116-288047-0011.flac  \n",
            "  inflating: blindtest/test/116-288047-0012.flac  \n",
            "  inflating: blindtest/test/116-288047-0013.flac  \n",
            "  inflating: blindtest/test/116-288047-0014.flac  \n",
            "  inflating: blindtest/test/116-288047-0015.flac  \n",
            "  inflating: blindtest/test/116-288047-0016.flac  \n",
            "  inflating: blindtest/test/116-288047-0017.flac  \n",
            "  inflating: blindtest/test/116-288047-0018.flac  \n",
            "  inflating: blindtest/test/116-288047-0019.flac  \n",
            "  inflating: blindtest/test/116-288047-0020.flac  \n",
            "  inflating: blindtest/test/116-288047-0021.flac  \n",
            "  inflating: blindtest/test/116-288047-0022.flac  \n",
            "  inflating: blindtest/test/116-288048-0000.flac  \n",
            "  inflating: blindtest/test/116-288048-0001.flac  \n",
            "  inflating: blindtest/test/116-288048-0002.flac  \n",
            "  inflating: blindtest/test/116-288048-0003.flac  \n",
            "  inflating: blindtest/test/116-288048-0004.flac  \n",
            "  inflating: blindtest/test/116-288048-0005.flac  \n",
            "  inflating: blindtest/test/116-288048-0006.flac  \n",
            "  inflating: blindtest/test/116-288048-0007.flac  \n",
            "  inflating: blindtest/test/116-288048-0008.flac  \n",
            "  inflating: blindtest/test/116-288048-0009.flac  \n",
            "  inflating: blindtest/test/116-288048-0010.flac  \n",
            "  inflating: blindtest/test/116-288048-0011.flac  \n",
            "  inflating: blindtest/test/116-288048-0012.flac  \n",
            "  inflating: blindtest/test/116-288048-0013.flac  \n",
            "  inflating: blindtest/test/116-288048-0014.flac  \n",
            "  inflating: blindtest/test/116-288048-0015.flac  \n",
            "  inflating: blindtest/test/116-288048-0016.flac  \n",
            "  inflating: blindtest/test/116-288048-0017.flac  \n",
            "  inflating: blindtest/test/116-288048-0018.flac  \n",
            "  inflating: blindtest/test/116-288048-0019.flac  \n",
            "  inflating: blindtest/test/116-288048-0020.flac  \n",
            "  inflating: blindtest/test/116-288048-0021.flac  \n",
            "  inflating: blindtest/test/116-288048-0022.flac  \n",
            "  inflating: blindtest/test/116-288048-0023.flac  \n",
            "  inflating: blindtest/test/116-288048-0024.flac  \n",
            "  inflating: blindtest/test/116-288048-0025.flac  \n",
            "  inflating: blindtest/test/116-288048-0026.flac  \n",
            "  inflating: blindtest/test/116-288048-0027.flac  \n",
            "  inflating: blindtest/test/116-288048-0028.flac  \n",
            "  inflating: blindtest/test/116-288048-0029.flac  \n",
            "  inflating: blindtest/test/116-288048-0030.flac  \n",
            "  inflating: blindtest/test/116-288048-0031.flac  \n",
            "  inflating: blindtest/test/116-288048-0032.flac  \n",
            "  inflating: blindtest/test/1272-128104-0000.flac  \n",
            "  inflating: blindtest/test/1272-128104-0001.flac  \n",
            "  inflating: blindtest/test/1272-128104-0002.flac  \n",
            "  inflating: blindtest/test/1272-128104-0003.flac  \n",
            "  inflating: blindtest/test/1272-128104-0004.flac  \n",
            "  inflating: blindtest/test/1272-128104-0005.flac  \n",
            "  inflating: blindtest/test/1272-128104-0006.flac  \n",
            "  inflating: blindtest/test/1272-128104-0007.flac  \n",
            "  inflating: blindtest/test/1272-128104-0008.flac  \n",
            "  inflating: blindtest/test/1272-128104-0009.flac  \n",
            "  inflating: blindtest/test/1272-128104-0010.flac  \n",
            "  inflating: blindtest/test/1272-128104-0011.flac  \n",
            "  inflating: blindtest/test/1272-128104-0012.flac  \n",
            "  inflating: blindtest/test/1272-128104-0013.flac  \n",
            "  inflating: blindtest/test/1272-128104-0014.flac  \n",
            "  inflating: blindtest/test/1272-135031-0000.flac  \n",
            "  inflating: blindtest/test/1272-135031-0001.flac  \n",
            "  inflating: blindtest/test/1272-135031-0002.flac  \n",
            "  inflating: blindtest/test/1272-135031-0003.flac  \n",
            "  inflating: blindtest/test/1272-135031-0004.flac  \n",
            "  inflating: blindtest/test/1272-135031-0005.flac  \n",
            "  inflating: blindtest/test/1272-135031-0006.flac  \n",
            "  inflating: blindtest/test/1272-135031-0007.flac  \n",
            "  inflating: blindtest/test/1272-135031-0008.flac  \n",
            "  inflating: blindtest/test/1272-135031-0009.flac  \n",
            "  inflating: blindtest/test/1272-135031-0010.flac  \n",
            "  inflating: blindtest/test/1272-135031-0011.flac  \n",
            "  inflating: blindtest/test/1272-135031-0012.flac  \n",
            "  inflating: blindtest/test/1272-135031-0013.flac  \n",
            "  inflating: blindtest/test/1272-135031-0014.flac  \n",
            "  inflating: blindtest/test/1272-135031-0015.flac  \n",
            "  inflating: blindtest/test/1272-135031-0016.flac  \n",
            "  inflating: blindtest/test/1272-135031-0017.flac  \n",
            "  inflating: blindtest/test/1272-135031-0018.flac  \n",
            "  inflating: blindtest/test/1272-135031-0019.flac  \n",
            "  inflating: blindtest/test/1272-135031-0020.flac  \n",
            "  inflating: blindtest/test/1272-135031-0021.flac  \n",
            "  inflating: blindtest/test/1272-135031-0022.flac  \n",
            "  inflating: blindtest/test/1272-135031-0023.flac  \n",
            "  inflating: blindtest/test/1272-135031-0024.flac  \n",
            "  inflating: blindtest/test/1272-141231-0000.flac  \n",
            "  inflating: blindtest/test/1272-141231-0001.flac  \n",
            "  inflating: blindtest/test/1272-141231-0002.flac  \n",
            "  inflating: blindtest/test/1272-141231-0003.flac  \n",
            "  inflating: blindtest/test/1272-141231-0004.flac  \n",
            "  inflating: blindtest/test/1272-141231-0005.flac  \n",
            "  inflating: blindtest/test/1272-141231-0006.flac  \n",
            "  inflating: blindtest/test/1272-141231-0007.flac  \n",
            "  inflating: blindtest/test/1272-141231-0008.flac  \n",
            "  inflating: blindtest/test/1272-141231-0009.flac  \n",
            "  inflating: blindtest/test/1272-141231-0010.flac  \n",
            "  inflating: blindtest/test/1272-141231-0011.flac  \n",
            "  inflating: blindtest/test/1272-141231-0012.flac  \n",
            "  inflating: blindtest/test/1272-141231-0013.flac  \n",
            "  inflating: blindtest/test/1272-141231-0014.flac  \n",
            "  inflating: blindtest/test/1272-141231-0015.flac  \n",
            "  inflating: blindtest/test/1272-141231-0016.flac  \n",
            "  inflating: blindtest/test/1272-141231-0017.flac  \n",
            "  inflating: blindtest/test/1272-141231-0018.flac  \n",
            "  inflating: blindtest/test/1272-141231-0019.flac  \n",
            "  inflating: blindtest/test/1272-141231-0020.flac  \n",
            "  inflating: blindtest/test/1272-141231-0021.flac  \n",
            "  inflating: blindtest/test/1272-141231-0022.flac  \n",
            "  inflating: blindtest/test/1272-141231-0023.flac  \n",
            "  inflating: blindtest/test/1272-141231-0024.flac  \n",
            "  inflating: blindtest/test/1272-141231-0025.flac  \n",
            "  inflating: blindtest/test/1272-141231-0026.flac  \n",
            "  inflating: blindtest/test/1272-141231-0027.flac  \n",
            "  inflating: blindtest/test/1272-141231-0028.flac  \n",
            "  inflating: blindtest/test/1272-141231-0029.flac  \n",
            "  inflating: blindtest/test/1272-141231-0030.flac  \n",
            "  inflating: blindtest/test/1272-141231-0031.flac  \n",
            "  inflating: blindtest/test/1272-141231-0032.flac  \n",
            "  inflating: blindtest/test/1462-170138-0000.flac  \n",
            "  inflating: blindtest/test/1462-170138-0001.flac  \n",
            "  inflating: blindtest/test/1462-170138-0002.flac  \n",
            "  inflating: blindtest/test/1462-170138-0003.flac  \n",
            "  inflating: blindtest/test/1462-170138-0004.flac  \n",
            "  inflating: blindtest/test/1462-170138-0005.flac  \n",
            "  inflating: blindtest/test/1462-170138-0006.flac  \n",
            "  inflating: blindtest/test/1462-170138-0007.flac  \n",
            "  inflating: blindtest/test/1462-170138-0008.flac  \n",
            "  inflating: blindtest/test/1462-170138-0009.flac  \n",
            "  inflating: blindtest/test/1462-170138-0010.flac  \n",
            "  inflating: blindtest/test/1462-170138-0011.flac  \n",
            "  inflating: blindtest/test/1462-170138-0012.flac  \n",
            "  inflating: blindtest/test/1462-170138-0013.flac  \n",
            "  inflating: blindtest/test/1462-170138-0014.flac  \n",
            "  inflating: blindtest/test/1462-170138-0015.flac  \n",
            "  inflating: blindtest/test/1462-170138-0016.flac  \n",
            "  inflating: blindtest/test/1462-170138-0017.flac  \n",
            "  inflating: blindtest/test/1462-170138-0018.flac  \n",
            "  inflating: blindtest/test/1462-170138-0019.flac  \n",
            "  inflating: blindtest/test/1462-170138-0020.flac  \n",
            "  inflating: blindtest/test/1462-170138-0021.flac  \n",
            "  inflating: blindtest/test/1462-170138-0022.flac  \n",
            "  inflating: blindtest/test/1462-170138-0023.flac  \n",
            "  inflating: blindtest/test/1462-170138-0024.flac  \n",
            "  inflating: blindtest/test/1462-170138-0025.flac  \n",
            "  inflating: blindtest/test/1462-170138-0026.flac  \n",
            "  inflating: blindtest/test/1462-170138-0027.flac  \n",
            "  inflating: blindtest/test/1462-170142-0000.flac  \n",
            "  inflating: blindtest/test/1462-170142-0001.flac  \n",
            "  inflating: blindtest/test/1462-170142-0002.flac  \n",
            "  inflating: blindtest/test/1462-170142-0003.flac  \n",
            "  inflating: blindtest/test/1462-170142-0004.flac  \n",
            "  inflating: blindtest/test/1462-170142-0005.flac  \n",
            "  inflating: blindtest/test/1462-170142-0006.flac  \n",
            "  inflating: blindtest/test/1462-170142-0007.flac  \n",
            "  inflating: blindtest/test/1462-170142-0008.flac  \n",
            "  inflating: blindtest/test/1462-170142-0009.flac  \n",
            "  inflating: blindtest/test/1462-170142-0010.flac  \n",
            "  inflating: blindtest/test/1462-170142-0011.flac  \n",
            "  inflating: blindtest/test/1462-170142-0012.flac  \n",
            "  inflating: blindtest/test/1462-170142-0013.flac  \n",
            "  inflating: blindtest/test/1462-170142-0014.flac  \n",
            "  inflating: blindtest/test/1462-170142-0015.flac  \n",
            "  inflating: blindtest/test/1462-170142-0016.flac  \n",
            "  inflating: blindtest/test/1462-170142-0017.flac  \n",
            "  inflating: blindtest/test/1462-170142-0018.flac  \n",
            "  inflating: blindtest/test/1462-170142-0019.flac  \n",
            "  inflating: blindtest/test/1462-170142-0020.flac  \n",
            "  inflating: blindtest/test/1462-170142-0021.flac  \n",
            "  inflating: blindtest/test/1462-170142-0022.flac  \n",
            "  inflating: blindtest/test/1462-170142-0023.flac  \n",
            "  inflating: blindtest/test/1462-170142-0024.flac  \n",
            "  inflating: blindtest/test/1462-170142-0025.flac  \n",
            "  inflating: blindtest/test/1462-170142-0026.flac  \n",
            "  inflating: blindtest/test/1462-170142-0027.flac  \n",
            "  inflating: blindtest/test/1462-170142-0028.flac  \n",
            "  inflating: blindtest/test/1462-170142-0029.flac  \n",
            "  inflating: blindtest/test/1462-170142-0030.flac  \n",
            "  inflating: blindtest/test/1462-170142-0031.flac  \n",
            "  inflating: blindtest/test/1462-170142-0032.flac  \n",
            "  inflating: blindtest/test/1462-170142-0033.flac  \n",
            "  inflating: blindtest/test/1462-170142-0034.flac  \n",
            "  inflating: blindtest/test/1462-170142-0035.flac  \n",
            "  inflating: blindtest/test/1462-170142-0036.flac  \n",
            "  inflating: blindtest/test/1462-170142-0037.flac  \n",
            "  inflating: blindtest/test/1462-170142-0038.flac  \n",
            "  inflating: blindtest/test/1462-170142-0039.flac  \n",
            "  inflating: blindtest/test/1462-170142-0040.flac  \n",
            "  inflating: blindtest/test/1462-170142-0041.flac  \n",
            "  inflating: blindtest/test/1462-170142-0042.flac  \n",
            "  inflating: blindtest/test/1462-170145-0000.flac  \n",
            "  inflating: blindtest/test/1462-170145-0001.flac  \n",
            "  inflating: blindtest/test/1462-170145-0002.flac  \n",
            "  inflating: blindtest/test/1462-170145-0003.flac  \n",
            "  inflating: blindtest/test/1462-170145-0004.flac  \n",
            "  inflating: blindtest/test/1462-170145-0005.flac  \n",
            "  inflating: blindtest/test/1462-170145-0006.flac  \n",
            "  inflating: blindtest/test/1462-170145-0007.flac  \n",
            "  inflating: blindtest/test/1462-170145-0008.flac  \n",
            "  inflating: blindtest/test/1462-170145-0009.flac  \n",
            "  inflating: blindtest/test/1462-170145-0010.flac  \n",
            "  inflating: blindtest/test/1462-170145-0011.flac  \n",
            "  inflating: blindtest/test/1462-170145-0012.flac  \n",
            "  inflating: blindtest/test/1462-170145-0013.flac  \n",
            "  inflating: blindtest/test/1462-170145-0014.flac  \n",
            "  inflating: blindtest/test/1462-170145-0015.flac  \n",
            "  inflating: blindtest/test/1462-170145-0016.flac  \n",
            "  inflating: blindtest/test/1462-170145-0017.flac  \n",
            "  inflating: blindtest/test/1462-170145-0018.flac  \n",
            "  inflating: blindtest/test/1462-170145-0019.flac  \n",
            "  inflating: blindtest/test/1462-170145-0020.flac  \n",
            "  inflating: blindtest/test/1462-170145-0021.flac  \n",
            "  inflating: blindtest/test/1462-170145-0022.flac  \n",
            "  inflating: blindtest/test/1688-142285-0000.flac  \n",
            "  inflating: blindtest/test/1688-142285-0001.flac  \n",
            "  inflating: blindtest/test/1688-142285-0002.flac  \n",
            "  inflating: blindtest/test/1688-142285-0003.flac  \n",
            "  inflating: blindtest/test/1688-142285-0004.flac  \n",
            "  inflating: blindtest/test/1688-142285-0005.flac  \n",
            "  inflating: blindtest/test/1688-142285-0006.flac  \n",
            "  inflating: blindtest/test/1688-142285-0007.flac  \n",
            "  inflating: blindtest/test/1688-142285-0008.flac  \n",
            "  inflating: blindtest/test/1688-142285-0009.flac  \n",
            "  inflating: blindtest/test/1688-142285-0010.flac  \n",
            "  inflating: blindtest/test/1688-142285-0011.flac  \n",
            "  inflating: blindtest/test/1688-142285-0012.flac  \n",
            "  inflating: blindtest/test/1688-142285-0013.flac  \n",
            "  inflating: blindtest/test/1688-142285-0014.flac  \n",
            "  inflating: blindtest/test/1688-142285-0015.flac  \n",
            "  inflating: blindtest/test/1688-142285-0016.flac  \n",
            "  inflating: blindtest/test/1688-142285-0017.flac  \n",
            "  inflating: blindtest/test/1688-142285-0018.flac  \n",
            "  inflating: blindtest/test/1688-142285-0019.flac  \n",
            "  inflating: blindtest/test/1688-142285-0020.flac  \n",
            "  inflating: blindtest/test/1688-142285-0021.flac  \n",
            "  inflating: blindtest/test/1688-142285-0022.flac  \n",
            "  inflating: blindtest/test/1688-142285-0023.flac  \n",
            "  inflating: blindtest/test/1688-142285-0024.flac  \n",
            "  inflating: blindtest/test/1688-142285-0025.flac  \n",
            "  inflating: blindtest/test/1688-142285-0026.flac  \n",
            "  inflating: blindtest/test/1688-142285-0027.flac  \n",
            "  inflating: blindtest/test/1688-142285-0028.flac  \n",
            "  inflating: blindtest/test/1688-142285-0029.flac  \n",
            "  inflating: blindtest/test/1688-142285-0030.flac  \n",
            "  inflating: blindtest/test/1688-142285-0031.flac  \n",
            "  inflating: blindtest/test/1688-142285-0032.flac  \n",
            "  inflating: blindtest/test/1688-142285-0033.flac  \n",
            "  inflating: blindtest/test/1688-142285-0034.flac  \n",
            "  inflating: blindtest/test/1688-142285-0035.flac  \n",
            "  inflating: blindtest/test/1688-142285-0036.flac  \n",
            "  inflating: blindtest/test/1688-142285-0037.flac  \n",
            "  inflating: blindtest/test/1688-142285-0038.flac  \n",
            "  inflating: blindtest/test/1688-142285-0039.flac  \n",
            "  inflating: blindtest/test/1688-142285-0040.flac  \n",
            "  inflating: blindtest/test/1688-142285-0041.flac  \n",
            "  inflating: blindtest/test/1688-142285-0042.flac  \n",
            "  inflating: blindtest/test/1688-142285-0043.flac  \n",
            "  inflating: blindtest/test/1688-142285-0044.flac  \n",
            "  inflating: blindtest/test/1688-142285-0045.flac  \n",
            "  inflating: blindtest/test/1688-142285-0046.flac  \n",
            "  inflating: blindtest/test/1688-142285-0047.flac  \n",
            "  inflating: blindtest/test/1688-142285-0048.flac  \n",
            "  inflating: blindtest/test/1688-142285-0049.flac  \n",
            "  inflating: blindtest/test/1688-142285-0050.flac  \n",
            "  inflating: blindtest/test/1688-142285-0051.flac  \n",
            "  inflating: blindtest/test/1688-142285-0052.flac  \n",
            "  inflating: blindtest/test/1688-142285-0053.flac  \n",
            "  inflating: blindtest/test/1688-142285-0054.flac  \n",
            "  inflating: blindtest/test/1688-142285-0055.flac  \n",
            "  inflating: blindtest/test/1688-142285-0056.flac  \n",
            "  inflating: blindtest/test/1688-142285-0057.flac  \n",
            "  inflating: blindtest/test/1688-142285-0058.flac  \n",
            "  inflating: blindtest/test/1688-142285-0059.flac  \n",
            "  inflating: blindtest/test/1688-142285-0060.flac  \n",
            "  inflating: blindtest/test/1688-142285-0061.flac  \n",
            "  inflating: blindtest/test/1688-142285-0062.flac  \n",
            "  inflating: blindtest/test/1688-142285-0063.flac  \n",
            "  inflating: blindtest/test/1688-142285-0064.flac  \n",
            "  inflating: blindtest/test/1688-142285-0065.flac  \n",
            "  inflating: blindtest/test/1688-142285-0066.flac  \n",
            "  inflating: blindtest/test/1688-142285-0067.flac  \n",
            "  inflating: blindtest/test/1688-142285-0068.flac  \n",
            "  inflating: blindtest/test/1688-142285-0069.flac  \n",
            "  inflating: blindtest/test/1688-142285-0070.flac  \n",
            "  inflating: blindtest/test/1688-142285-0071.flac  \n",
            "  inflating: blindtest/test/1688-142285-0072.flac  \n",
            "  inflating: blindtest/test/1688-142285-0073.flac  \n",
            "  inflating: blindtest/test/1688-142285-0074.flac  \n",
            "  inflating: blindtest/test/1688-142285-0075.flac  \n",
            "  inflating: blindtest/test/1688-142285-0076.flac  \n",
            "  inflating: blindtest/test/1688-142285-0077.flac  \n",
            "  inflating: blindtest/test/1688-142285-0078.flac  \n",
            "  inflating: blindtest/test/1688-142285-0079.flac  \n",
            "  inflating: blindtest/test/1688-142285-0080.flac  \n",
            "  inflating: blindtest/test/1688-142285-0081.flac  \n",
            "  inflating: blindtest/test/1688-142285-0082.flac  \n",
            "  inflating: blindtest/test/1688-142285-0083.flac  \n",
            "  inflating: blindtest/test/1688-142285-0084.flac  \n",
            "  inflating: blindtest/test/1688-142285-0085.flac  \n",
            "  inflating: blindtest/test/1688-142285-0086.flac  \n",
            "  inflating: blindtest/test/1688-142285-0087.flac  \n",
            "  inflating: blindtest/test/1688-142285-0088.flac  \n",
            "  inflating: blindtest/test/1688-142285-0089.flac  \n",
            "  inflating: blindtest/test/1688-142285-0090.flac  \n",
            "  inflating: blindtest/test/1688-142285-0091.flac  \n",
            "  inflating: blindtest/test/1688-142285-0092.flac  \n",
            "  inflating: blindtest/test/1688-142285-0093.flac  \n",
            "  inflating: blindtest/test/1688-142285-0094.flac  \n",
            "  inflating: blindtest/test/1688-142285-0095.flac  \n",
            "  inflating: blindtest/test/1988-147956-0000.flac  \n",
            "  inflating: blindtest/test/1988-147956-0001.flac  \n",
            "  inflating: blindtest/test/1988-147956-0002.flac  \n",
            "  inflating: blindtest/test/1988-147956-0003.flac  \n",
            "  inflating: blindtest/test/1988-147956-0004.flac  \n",
            "  inflating: blindtest/test/1988-147956-0005.flac  \n",
            "  inflating: blindtest/test/1988-147956-0006.flac  \n",
            "  inflating: blindtest/test/1988-147956-0007.flac  \n",
            "  inflating: blindtest/test/1988-147956-0008.flac  \n",
            "  inflating: blindtest/test/1988-147956-0009.flac  \n",
            "  inflating: blindtest/test/1988-147956-0010.flac  \n",
            "  inflating: blindtest/test/1988-147956-0011.flac  \n",
            "  inflating: blindtest/test/1988-147956-0012.flac  \n",
            "  inflating: blindtest/test/1988-147956-0013.flac  \n",
            "  inflating: blindtest/test/1988-147956-0014.flac  \n",
            "  inflating: blindtest/test/1988-147956-0015.flac  \n",
            "  inflating: blindtest/test/1988-147956-0016.flac  \n",
            "  inflating: blindtest/test/1988-147956-0017.flac  \n",
            "  inflating: blindtest/test/1988-147956-0018.flac  \n",
            "  inflating: blindtest/test/1988-147956-0019.flac  \n",
            "  inflating: blindtest/test/1988-147956-0020.flac  \n",
            "  inflating: blindtest/test/1988-147956-0021.flac  \n",
            "  inflating: blindtest/test/1988-147956-0022.flac  \n",
            "  inflating: blindtest/test/1988-147956-0023.flac  \n",
            "  inflating: blindtest/test/1988-147956-0024.flac  \n",
            "  inflating: blindtest/test/1988-147956-0025.flac  \n",
            "  inflating: blindtest/test/1988-147956-0026.flac  \n",
            "  inflating: blindtest/test/1988-147956-0027.flac  \n",
            "  inflating: blindtest/test/1988-147956-0028.flac  \n",
            "  inflating: blindtest/test/1988-147956-0029.flac  \n",
            "  inflating: blindtest/test/1988-148538-0000.flac  \n",
            "  inflating: blindtest/test/1988-148538-0001.flac  \n",
            "  inflating: blindtest/test/1988-148538-0002.flac  \n",
            "  inflating: blindtest/test/1988-148538-0003.flac  \n",
            "  inflating: blindtest/test/1988-148538-0004.flac  \n",
            "  inflating: blindtest/test/1988-148538-0005.flac  \n",
            "  inflating: blindtest/test/1988-148538-0006.flac  \n",
            "  inflating: blindtest/test/1988-148538-0007.flac  \n",
            "  inflating: blindtest/test/1988-148538-0008.flac  \n",
            "  inflating: blindtest/test/1988-148538-0009.flac  \n",
            "  inflating: blindtest/test/1988-148538-0010.flac  \n",
            "  inflating: blindtest/test/1988-148538-0011.flac  \n",
            "  inflating: blindtest/test/1988-148538-0012.flac  \n",
            "  inflating: blindtest/test/1988-148538-0013.flac  \n",
            "  inflating: blindtest/test/1988-148538-0014.flac  \n",
            "  inflating: blindtest/test/1988-148538-0015.flac  \n",
            "  inflating: blindtest/test/1988-24833-0000.flac  \n",
            "  inflating: blindtest/test/1988-24833-0001.flac  \n",
            "  inflating: blindtest/test/1988-24833-0002.flac  \n",
            "  inflating: blindtest/test/1988-24833-0003.flac  \n",
            "  inflating: blindtest/test/1988-24833-0004.flac  \n",
            "  inflating: blindtest/test/1988-24833-0005.flac  \n",
            "  inflating: blindtest/test/1988-24833-0006.flac  \n",
            "  inflating: blindtest/test/1988-24833-0007.flac  \n",
            "  inflating: blindtest/test/1988-24833-0008.flac  \n",
            "  inflating: blindtest/test/1988-24833-0009.flac  \n",
            "  inflating: blindtest/test/1988-24833-0010.flac  \n",
            "  inflating: blindtest/test/1988-24833-0011.flac  \n",
            "  inflating: blindtest/test/1988-24833-0012.flac  \n",
            "  inflating: blindtest/test/1988-24833-0013.flac  \n",
            "  inflating: blindtest/test/1988-24833-0014.flac  \n",
            "  inflating: blindtest/test/1988-24833-0015.flac  \n",
            "  inflating: blindtest/test/1988-24833-0016.flac  \n",
            "  inflating: blindtest/test/1988-24833-0017.flac  \n",
            "  inflating: blindtest/test/1988-24833-0018.flac  \n",
            "  inflating: blindtest/test/1988-24833-0019.flac  \n",
            "  inflating: blindtest/test/1988-24833-0020.flac  \n",
            "  inflating: blindtest/test/1988-24833-0021.flac  \n",
            "  inflating: blindtest/test/1988-24833-0022.flac  \n",
            "  inflating: blindtest/test/1988-24833-0023.flac  \n",
            "  inflating: blindtest/test/1988-24833-0024.flac  \n",
            "  inflating: blindtest/test/1988-24833-0025.flac  \n",
            "  inflating: blindtest/test/1988-24833-0026.flac  \n",
            "  inflating: blindtest/test/1988-24833-0027.flac  \n",
            "  inflating: blindtest/test/1988-24833-0028.flac  \n",
            "  inflating: blindtest/test/1993-147149-0000.flac  \n",
            "  inflating: blindtest/test/1993-147149-0001.flac  \n",
            "  inflating: blindtest/test/1993-147149-0002.flac  \n",
            "  inflating: blindtest/test/1993-147149-0003.flac  \n",
            "  inflating: blindtest/test/1993-147149-0004.flac  \n",
            "  inflating: blindtest/test/1993-147149-0005.flac  \n",
            "  inflating: blindtest/test/1993-147149-0006.flac  \n",
            "  inflating: blindtest/test/1993-147149-0007.flac  \n",
            "  inflating: blindtest/test/1993-147149-0008.flac  \n",
            "  inflating: blindtest/test/1993-147149-0009.flac  \n",
            "  inflating: blindtest/test/1993-147149-0010.flac  \n",
            "  inflating: blindtest/test/1993-147149-0011.flac  \n",
            "  inflating: blindtest/test/1993-147149-0012.flac  \n",
            "  inflating: blindtest/test/1993-147149-0013.flac  \n",
            "  inflating: blindtest/test/1993-147149-0014.flac  \n",
            "  inflating: blindtest/test/1993-147149-0015.flac  \n",
            "  inflating: blindtest/test/1993-147149-0016.flac  \n",
            "  inflating: blindtest/test/1993-147149-0017.flac  \n",
            "  inflating: blindtest/test/1993-147149-0018.flac  \n",
            "  inflating: blindtest/test/1993-147149-0019.flac  \n",
            "  inflating: blindtest/test/1993-147149-0020.flac  \n",
            "  inflating: blindtest/test/1993-147149-0021.flac  \n",
            "  inflating: blindtest/test/1993-147149-0022.flac  \n",
            "  inflating: blindtest/test/1993-147149-0023.flac  \n",
            "  inflating: blindtest/test/1993-147149-0024.flac  \n",
            "  inflating: blindtest/test/1993-147149-0025.flac  \n",
            "  inflating: blindtest/test/1993-147149-0026.flac  \n",
            "  inflating: blindtest/test/1993-147149-0027.flac  \n",
            "  inflating: blindtest/test/1993-147149-0028.flac  \n",
            "  inflating: blindtest/test/1993-147149-0029.flac  \n",
            "  inflating: blindtest/test/1993-147149-0030.flac  \n",
            "  inflating: blindtest/test/1993-147964-0000.flac  \n",
            "  inflating: blindtest/test/1993-147964-0001.flac  \n",
            "  inflating: blindtest/test/1993-147964-0002.flac  \n",
            "  inflating: blindtest/test/1993-147964-0003.flac  \n",
            "  inflating: blindtest/test/1993-147964-0004.flac  \n",
            "  inflating: blindtest/test/1993-147964-0005.flac  \n",
            "  inflating: blindtest/test/1993-147964-0006.flac  \n",
            "  inflating: blindtest/test/1993-147964-0007.flac  \n",
            "  inflating: blindtest/test/1993-147964-0008.flac  \n",
            "  inflating: blindtest/test/1993-147964-0009.flac  \n",
            "  inflating: blindtest/test/1993-147964-0010.flac  \n",
            "  inflating: blindtest/test/1993-147965-0000.flac  \n",
            "  inflating: blindtest/test/1993-147965-0001.flac  \n",
            "  inflating: blindtest/test/1993-147965-0002.flac  \n",
            "  inflating: blindtest/test/1993-147965-0003.flac  \n",
            "  inflating: blindtest/test/1993-147965-0004.flac  \n",
            "  inflating: blindtest/test/1993-147965-0005.flac  \n",
            "  inflating: blindtest/test/1993-147965-0006.flac  \n",
            "  inflating: blindtest/test/1993-147965-0007.flac  \n",
            "  inflating: blindtest/test/1993-147965-0008.flac  \n",
            "  inflating: blindtest/test/1993-147966-0000.flac  \n",
            "  inflating: blindtest/test/1993-147966-0001.flac  \n",
            "  inflating: blindtest/test/1993-147966-0002.flac  \n",
            "  inflating: blindtest/test/1993-147966-0003.flac  \n",
            "  inflating: blindtest/test/1993-147966-0004.flac  \n",
            "  inflating: blindtest/test/1993-147966-0005.flac  \n",
            "  inflating: blindtest/test/1993-147966-0006.flac  \n",
            "  inflating: blindtest/test/1998-15444-0000.flac  \n",
            "  inflating: blindtest/test/1998-15444-0001.flac  \n",
            "  inflating: blindtest/test/1998-15444-0002.flac  \n",
            "  inflating: blindtest/test/1998-15444-0003.flac  \n",
            "  inflating: blindtest/test/1998-15444-0004.flac  \n",
            "  inflating: blindtest/test/1998-15444-0005.flac  \n",
            "  inflating: blindtest/test/1998-15444-0006.flac  \n",
            "  inflating: blindtest/test/1998-15444-0007.flac  \n",
            "  inflating: blindtest/test/1998-15444-0008.flac  \n",
            "  inflating: blindtest/test/1998-15444-0009.flac  \n",
            "  inflating: blindtest/test/1998-15444-0010.flac  \n",
            "  inflating: blindtest/test/1998-15444-0011.flac  \n",
            "  inflating: blindtest/test/1998-15444-0012.flac  \n",
            "  inflating: blindtest/test/1998-15444-0013.flac  \n",
            "  inflating: blindtest/test/1998-15444-0014.flac  \n",
            "  inflating: blindtest/test/1998-15444-0015.flac  \n",
            "  inflating: blindtest/test/1998-15444-0016.flac  \n",
            "  inflating: blindtest/test/1998-15444-0017.flac  \n",
            "  inflating: blindtest/test/1998-15444-0018.flac  \n",
            "  inflating: blindtest/test/1998-15444-0019.flac  \n",
            "  inflating: blindtest/test/1998-15444-0020.flac  \n",
            "  inflating: blindtest/test/1998-15444-0021.flac  \n",
            "  inflating: blindtest/test/1998-15444-0022.flac  \n",
            "  inflating: blindtest/test/1998-15444-0023.flac  \n",
            "  inflating: blindtest/test/1998-15444-0024.flac  \n",
            "  inflating: blindtest/test/1998-15444-0025.flac  \n",
            "  inflating: blindtest/test/1998-15444-0026.flac  \n",
            "  inflating: blindtest/test/1998-15444-0027.flac  \n",
            "  inflating: blindtest/test/1998-29454-0000.flac  \n",
            "  inflating: blindtest/test/1998-29454-0001.flac  \n",
            "  inflating: blindtest/test/1998-29454-0002.flac  \n",
            "  inflating: blindtest/test/1998-29454-0003.flac  \n",
            "  inflating: blindtest/test/1998-29454-0004.flac  \n",
            "  inflating: blindtest/test/1998-29454-0005.flac  \n",
            "  inflating: blindtest/test/1998-29454-0006.flac  \n",
            "  inflating: blindtest/test/1998-29454-0007.flac  \n",
            "  inflating: blindtest/test/1998-29454-0008.flac  \n",
            "  inflating: blindtest/test/1998-29454-0009.flac  \n",
            "  inflating: blindtest/test/1998-29454-0010.flac  \n",
            "  inflating: blindtest/test/1998-29454-0011.flac  \n",
            "  inflating: blindtest/test/1998-29454-0012.flac  \n",
            "  inflating: blindtest/test/1998-29454-0013.flac  \n",
            "  inflating: blindtest/test/1998-29454-0014.flac  \n",
            "  inflating: blindtest/test/1998-29454-0015.flac  \n",
            "  inflating: blindtest/test/1998-29454-0016.flac  \n",
            "  inflating: blindtest/test/1998-29454-0017.flac  \n",
            "  inflating: blindtest/test/1998-29454-0018.flac  \n",
            "  inflating: blindtest/test/1998-29454-0019.flac  \n",
            "  inflating: blindtest/test/1998-29454-0020.flac  \n",
            "  inflating: blindtest/test/1998-29454-0021.flac  \n",
            "  inflating: blindtest/test/1998-29454-0022.flac  \n",
            "  inflating: blindtest/test/1998-29454-0023.flac  \n",
            "  inflating: blindtest/test/1998-29454-0024.flac  \n",
            "  inflating: blindtest/test/1998-29454-0025.flac  \n",
            "  inflating: blindtest/test/1998-29454-0026.flac  \n",
            "  inflating: blindtest/test/1998-29454-0027.flac  \n",
            "  inflating: blindtest/test/1998-29454-0028.flac  \n",
            "  inflating: blindtest/test/1998-29454-0029.flac  \n",
            "  inflating: blindtest/test/1998-29454-0030.flac  \n",
            "  inflating: blindtest/test/1998-29454-0031.flac  \n",
            "  inflating: blindtest/test/1998-29454-0032.flac  \n",
            "  inflating: blindtest/test/1998-29454-0033.flac  \n",
            "  inflating: blindtest/test/1998-29454-0034.flac  \n",
            "  inflating: blindtest/test/1998-29454-0035.flac  \n",
            "  inflating: blindtest/test/1998-29454-0036.flac  \n",
            "  inflating: blindtest/test/1998-29454-0037.flac  \n",
            "  inflating: blindtest/test/1998-29454-0038.flac  \n",
            "  inflating: blindtest/test/1998-29454-0039.flac  \n",
            "  inflating: blindtest/test/1998-29454-0040.flac  \n",
            "  inflating: blindtest/test/1998-29454-0041.flac  \n",
            "  inflating: blindtest/test/1998-29454-0042.flac  \n",
            "  inflating: blindtest/test/1998-29454-0043.flac  \n",
            "  inflating: blindtest/test/1998-29454-0044.flac  \n",
            "  inflating: blindtest/test/1998-29454-0045.flac  \n",
            "  inflating: blindtest/test/1998-29454-0046.flac  \n",
            "  inflating: blindtest/test/1998-29455-0000.flac  \n",
            "  inflating: blindtest/test/1998-29455-0001.flac  \n",
            "  inflating: blindtest/test/1998-29455-0002.flac  \n",
            "  inflating: blindtest/test/1998-29455-0003.flac  \n",
            "  inflating: blindtest/test/1998-29455-0004.flac  \n",
            "  inflating: blindtest/test/1998-29455-0005.flac  \n",
            "  inflating: blindtest/test/1998-29455-0006.flac  \n",
            "  inflating: blindtest/test/1998-29455-0007.flac  \n",
            "  inflating: blindtest/test/1998-29455-0008.flac  \n",
            "  inflating: blindtest/test/1998-29455-0009.flac  \n",
            "  inflating: blindtest/test/1998-29455-0010.flac  \n",
            "  inflating: blindtest/test/1998-29455-0011.flac  \n",
            "  inflating: blindtest/test/1998-29455-0012.flac  \n",
            "  inflating: blindtest/test/1998-29455-0013.flac  \n",
            "  inflating: blindtest/test/1998-29455-0014.flac  \n",
            "  inflating: blindtest/test/1998-29455-0015.flac  \n",
            "  inflating: blindtest/test/1998-29455-0016.flac  \n",
            "  inflating: blindtest/test/1998-29455-0017.flac  \n",
            "  inflating: blindtest/test/1998-29455-0018.flac  \n",
            "  inflating: blindtest/test/1998-29455-0019.flac  \n",
            "  inflating: blindtest/test/1998-29455-0020.flac  \n",
            "  inflating: blindtest/test/1998-29455-0021.flac  \n",
            "  inflating: blindtest/test/1998-29455-0022.flac  \n",
            "  inflating: blindtest/test/1998-29455-0023.flac  \n",
            "  inflating: blindtest/test/1998-29455-0024.flac  \n",
            "  inflating: blindtest/test/1998-29455-0025.flac  \n",
            "  inflating: blindtest/test/1998-29455-0026.flac  \n",
            "  inflating: blindtest/test/1998-29455-0027.flac  \n",
            "  inflating: blindtest/test/1998-29455-0028.flac  \n",
            "  inflating: blindtest/test/1998-29455-0029.flac  \n",
            "  inflating: blindtest/test/1998-29455-0030.flac  \n",
            "  inflating: blindtest/test/1998-29455-0031.flac  \n",
            "  inflating: blindtest/test/1998-29455-0032.flac  \n",
            "  inflating: blindtest/test/1998-29455-0033.flac  \n",
            "  inflating: blindtest/test/1998-29455-0034.flac  \n",
            "  inflating: blindtest/test/1998-29455-0035.flac  \n",
            "  inflating: blindtest/test/1998-29455-0036.flac  \n",
            "  inflating: blindtest/test/1998-29455-0037.flac  \n",
            "  inflating: blindtest/test/1998-29455-0038.flac  \n",
            "  inflating: blindtest/test/1998-29455-0039.flac  \n",
            "  inflating: blindtest/test/2078-142845-0000.flac  \n",
            "  inflating: blindtest/test/2078-142845-0001.flac  \n",
            "  inflating: blindtest/test/2078-142845-0002.flac  \n",
            "  inflating: blindtest/test/2078-142845-0003.flac  \n",
            "  inflating: blindtest/test/2078-142845-0004.flac  \n",
            "  inflating: blindtest/test/2078-142845-0005.flac  \n",
            "  inflating: blindtest/test/2078-142845-0006.flac  \n",
            "  inflating: blindtest/test/2078-142845-0007.flac  \n",
            "  inflating: blindtest/test/2078-142845-0008.flac  \n",
            "  inflating: blindtest/test/2078-142845-0009.flac  \n",
            "  inflating: blindtest/test/2078-142845-0010.flac  \n",
            "  inflating: blindtest/test/2078-142845-0011.flac  \n",
            "  inflating: blindtest/test/2078-142845-0012.flac  \n",
            "  inflating: blindtest/test/2078-142845-0013.flac  \n",
            "  inflating: blindtest/test/2078-142845-0014.flac  \n",
            "  inflating: blindtest/test/2078-142845-0015.flac  \n",
            "  inflating: blindtest/test/2078-142845-0016.flac  \n",
            "  inflating: blindtest/test/2078-142845-0017.flac  \n",
            "  inflating: blindtest/test/2078-142845-0018.flac  \n",
            "  inflating: blindtest/test/2078-142845-0019.flac  \n",
            "  inflating: blindtest/test/2078-142845-0020.flac  \n",
            "  inflating: blindtest/test/2078-142845-0021.flac  \n",
            "  inflating: blindtest/test/2078-142845-0022.flac  \n",
            "  inflating: blindtest/test/2078-142845-0023.flac  \n",
            "  inflating: blindtest/test/2078-142845-0024.flac  \n",
            "  inflating: blindtest/test/2078-142845-0025.flac  \n",
            "  inflating: blindtest/test/2078-142845-0026.flac  \n",
            "  inflating: blindtest/test/2078-142845-0027.flac  \n",
            "  inflating: blindtest/test/2078-142845-0028.flac  \n",
            "  inflating: blindtest/test/2078-142845-0029.flac  \n",
            "  inflating: blindtest/test/2078-142845-0030.flac  \n",
            "  inflating: blindtest/test/2078-142845-0031.flac  \n",
            "  inflating: blindtest/test/2078-142845-0032.flac  \n",
            "  inflating: blindtest/test/2078-142845-0033.flac  \n",
            "  inflating: blindtest/test/2078-142845-0034.flac  \n",
            "  inflating: blindtest/test/2078-142845-0035.flac  \n",
            "  inflating: blindtest/test/2078-142845-0036.flac  \n",
            "  inflating: blindtest/test/2078-142845-0037.flac  \n",
            "  inflating: blindtest/test/2078-142845-0038.flac  \n",
            "  inflating: blindtest/test/2078-142845-0039.flac  \n",
            "  inflating: blindtest/test/2078-142845-0040.flac  \n",
            "  inflating: blindtest/test/2078-142845-0041.flac  \n",
            "  inflating: blindtest/test/2078-142845-0042.flac  \n",
            "  inflating: blindtest/test/2078-142845-0043.flac  \n",
            "  inflating: blindtest/test/2078-142845-0044.flac  \n",
            "  inflating: blindtest/test/2078-142845-0045.flac  \n",
            "  inflating: blindtest/test/2078-142845-0046.flac  \n",
            "  inflating: blindtest/test/2078-142845-0047.flac  \n",
            "  inflating: blindtest/test/2078-142845-0048.flac  \n",
            "  inflating: blindtest/test/2078-142845-0049.flac  \n",
            "  inflating: blindtest/test/2078-142845-0050.flac  \n",
            "  inflating: blindtest/test/2078-142845-0051.flac  \n",
            "  inflating: blindtest/test/2277-149874-0000.flac  \n",
            "  inflating: blindtest/test/2277-149874-0001.flac  \n",
            "  inflating: blindtest/test/2277-149874-0002.flac  \n",
            "  inflating: blindtest/test/2277-149874-0003.flac  \n",
            "  inflating: blindtest/test/2277-149874-0004.flac  \n",
            "  inflating: blindtest/test/2277-149874-0005.flac  \n",
            "  inflating: blindtest/test/2277-149874-0006.flac  \n",
            "  inflating: blindtest/test/2277-149874-0007.flac  \n",
            "  inflating: blindtest/test/2277-149874-0008.flac  \n",
            "  inflating: blindtest/test/2277-149874-0009.flac  \n",
            "  inflating: blindtest/test/2277-149874-0010.flac  \n",
            "  inflating: blindtest/test/2277-149874-0011.flac  \n",
            "  inflating: blindtest/test/2277-149874-0012.flac  \n",
            "  inflating: blindtest/test/2277-149874-0013.flac  \n",
            "  inflating: blindtest/test/2277-149874-0014.flac  \n",
            "  inflating: blindtest/test/2277-149874-0015.flac  \n",
            "  inflating: blindtest/test/2277-149874-0016.flac  \n",
            "  inflating: blindtest/test/2277-149874-0017.flac  \n",
            "  inflating: blindtest/test/2277-149874-0018.flac  \n",
            "  inflating: blindtest/test/2277-149874-0019.flac  \n",
            "  inflating: blindtest/test/2277-149874-0020.flac  \n",
            "  inflating: blindtest/test/2277-149874-0021.flac  \n",
            "  inflating: blindtest/test/2277-149896-0000.flac  \n",
            "  inflating: blindtest/test/2277-149896-0001.flac  \n",
            "  inflating: blindtest/test/2277-149896-0002.flac  \n",
            "  inflating: blindtest/test/2277-149896-0003.flac  \n",
            "  inflating: blindtest/test/2277-149896-0004.flac  \n",
            "  inflating: blindtest/test/2277-149896-0005.flac  \n",
            "  inflating: blindtest/test/2277-149896-0006.flac  \n",
            "  inflating: blindtest/test/2277-149896-0007.flac  \n",
            "  inflating: blindtest/test/2277-149896-0008.flac  \n",
            "  inflating: blindtest/test/2277-149896-0009.flac  \n",
            "  inflating: blindtest/test/2277-149896-0010.flac  \n",
            "  inflating: blindtest/test/2277-149896-0011.flac  \n",
            "  inflating: blindtest/test/2277-149896-0012.flac  \n",
            "  inflating: blindtest/test/2277-149896-0013.flac  \n",
            "  inflating: blindtest/test/2277-149896-0014.flac  \n",
            "  inflating: blindtest/test/2277-149896-0015.flac  \n",
            "  inflating: blindtest/test/2277-149896-0016.flac  \n",
            "  inflating: blindtest/test/2277-149896-0017.flac  \n",
            "  inflating: blindtest/test/2277-149896-0018.flac  \n",
            "  inflating: blindtest/test/2277-149896-0019.flac  \n",
            "  inflating: blindtest/test/2277-149896-0020.flac  \n",
            "  inflating: blindtest/test/2277-149896-0021.flac  \n",
            "  inflating: blindtest/test/2277-149896-0022.flac  \n",
            "  inflating: blindtest/test/2277-149896-0023.flac  \n",
            "  inflating: blindtest/test/2277-149896-0024.flac  \n",
            "  inflating: blindtest/test/2277-149896-0025.flac  \n",
            "  inflating: blindtest/test/2277-149896-0026.flac  \n",
            "  inflating: blindtest/test/2277-149896-0027.flac  \n",
            "  inflating: blindtest/test/2277-149896-0028.flac  \n",
            "  inflating: blindtest/test/2277-149896-0029.flac  \n",
            "  inflating: blindtest/test/2277-149896-0030.flac  \n",
            "  inflating: blindtest/test/2277-149896-0031.flac  \n",
            "  inflating: blindtest/test/2277-149896-0032.flac  \n",
            "  inflating: blindtest/test/2277-149896-0033.flac  \n",
            "  inflating: blindtest/test/2277-149896-0034.flac  \n",
            "  inflating: blindtest/test/2277-149897-0000.flac  \n",
            "  inflating: blindtest/test/2277-149897-0001.flac  \n",
            "  inflating: blindtest/test/2277-149897-0002.flac  \n",
            "  inflating: blindtest/test/2277-149897-0003.flac  \n",
            "  inflating: blindtest/test/2277-149897-0004.flac  \n",
            "  inflating: blindtest/test/2277-149897-0005.flac  \n",
            "  inflating: blindtest/test/2277-149897-0006.flac  \n",
            "  inflating: blindtest/test/2277-149897-0007.flac  \n",
            "  inflating: blindtest/test/2277-149897-0008.flac  \n",
            "  inflating: blindtest/test/2277-149897-0009.flac  \n",
            "  inflating: blindtest/test/2277-149897-0010.flac  \n",
            "  inflating: blindtest/test/2277-149897-0011.flac  \n",
            "  inflating: blindtest/test/2277-149897-0012.flac  \n",
            "  inflating: blindtest/test/2277-149897-0013.flac  \n",
            "  inflating: blindtest/test/2277-149897-0014.flac  \n",
            "  inflating: blindtest/test/2277-149897-0015.flac  \n",
            "  inflating: blindtest/test/2277-149897-0016.flac  \n",
            "  inflating: blindtest/test/2277-149897-0017.flac  \n",
            "  inflating: blindtest/test/2277-149897-0018.flac  \n",
            "  inflating: blindtest/test/2277-149897-0019.flac  \n",
            "  inflating: blindtest/test/2277-149897-0020.flac  \n",
            "  inflating: blindtest/test/2277-149897-0021.flac  \n",
            "  inflating: blindtest/test/2277-149897-0022.flac  \n",
            "  inflating: blindtest/test/2277-149897-0023.flac  \n",
            "  inflating: blindtest/test/2277-149897-0024.flac  \n",
            "  inflating: blindtest/test/2277-149897-0025.flac  \n",
            "  inflating: blindtest/test/2277-149897-0026.flac  \n",
            "  inflating: blindtest/test/2277-149897-0027.flac  \n",
            "  inflating: blindtest/test/2277-149897-0028.flac  \n",
            "  inflating: blindtest/test/2277-149897-0029.flac  \n",
            "  inflating: blindtest/test/2277-149897-0030.flac  \n",
            "  inflating: blindtest/test/2277-149897-0031.flac  \n",
            "  inflating: blindtest/test/2277-149897-0032.flac  \n",
            "  inflating: blindtest/test/2277-149897-0033.flac  \n",
            "  inflating: blindtest/test/2277-149897-0034.flac  \n",
            "  inflating: blindtest/test/2277-149897-0035.flac  \n",
            "  inflating: blindtest/test/2277-149897-0036.flac  \n",
            "  inflating: blindtest/test/2277-149897-0037.flac  \n",
            "  inflating: blindtest/test/2412-153947-0000.flac  \n",
            "  inflating: blindtest/test/2412-153947-0001.flac  \n",
            "  inflating: blindtest/test/2412-153947-0002.flac  \n",
            "  inflating: blindtest/test/2412-153947-0003.flac  \n",
            "  inflating: blindtest/test/2412-153947-0004.flac  \n",
            "  inflating: blindtest/test/2412-153947-0005.flac  \n",
            "  inflating: blindtest/test/2412-153947-0006.flac  \n",
            "  inflating: blindtest/test/2412-153947-0007.flac  \n",
            "  inflating: blindtest/test/2412-153947-0008.flac  \n",
            "  inflating: blindtest/test/2412-153947-0009.flac  \n",
            "  inflating: blindtest/test/2412-153947-0010.flac  \n",
            "  inflating: blindtest/test/2412-153947-0011.flac  \n",
            "  inflating: blindtest/test/2412-153947-0012.flac  \n",
            "  inflating: blindtest/test/2412-153947-0013.flac  \n",
            "  inflating: blindtest/test/2412-153947-0014.flac  \n",
            "  inflating: blindtest/test/2412-153947-0015.flac  \n",
            "  inflating: blindtest/test/2412-153947-0016.flac  \n",
            "  inflating: blindtest/test/2412-153948-0000.flac  \n",
            "  inflating: blindtest/test/2412-153948-0001.flac  \n",
            "  inflating: blindtest/test/2412-153948-0002.flac  \n",
            "  inflating: blindtest/test/2412-153948-0003.flac  \n",
            "  inflating: blindtest/test/2412-153948-0004.flac  \n",
            "  inflating: blindtest/test/2412-153948-0005.flac  \n",
            "  inflating: blindtest/test/2412-153948-0006.flac  \n",
            "  inflating: blindtest/test/2412-153948-0007.flac  \n",
            "  inflating: blindtest/test/2412-153948-0008.flac  \n",
            "  inflating: blindtest/test/2412-153948-0009.flac  \n",
            "  inflating: blindtest/test/2412-153948-0010.flac  \n",
            "  inflating: blindtest/test/2412-153948-0011.flac  \n",
            "  inflating: blindtest/test/2412-153948-0012.flac  \n",
            "  inflating: blindtest/test/2412-153948-0013.flac  \n",
            "  inflating: blindtest/test/2412-153948-0014.flac  \n",
            "  inflating: blindtest/test/2412-153948-0015.flac  \n",
            "  inflating: blindtest/test/2412-153954-0000.flac  \n",
            "  inflating: blindtest/test/2412-153954-0001.flac  \n",
            "  inflating: blindtest/test/2412-153954-0002.flac  \n",
            "  inflating: blindtest/test/2412-153954-0003.flac  \n",
            "  inflating: blindtest/test/2412-153954-0004.flac  \n",
            "  inflating: blindtest/test/2412-153954-0005.flac  \n",
            "  inflating: blindtest/test/2412-153954-0006.flac  \n",
            "  inflating: blindtest/test/2412-153954-0007.flac  \n",
            "  inflating: blindtest/test/2412-153954-0008.flac  \n",
            "  inflating: blindtest/test/2412-153954-0009.flac  \n",
            "  inflating: blindtest/test/2412-153954-0010.flac  \n",
            "  inflating: blindtest/test/2412-153954-0011.flac  \n",
            "  inflating: blindtest/test/2412-153954-0012.flac  \n",
            "  inflating: blindtest/test/2412-153954-0013.flac  \n",
            "  inflating: blindtest/test/2412-153954-0014.flac  \n",
            "  inflating: blindtest/test/2412-153954-0015.flac  \n",
            "  inflating: blindtest/test/2412-153954-0016.flac  \n",
            "  inflating: blindtest/test/2412-153954-0017.flac  \n",
            "  inflating: blindtest/test/2412-153954-0018.flac  \n",
            "  inflating: blindtest/test/2412-153954-0019.flac  \n",
            "  inflating: blindtest/test/2412-153954-0020.flac  \n",
            "  inflating: blindtest/test/2412-153954-0021.flac  \n",
            "  inflating: blindtest/test/2412-153954-0022.flac  \n",
            "  inflating: blindtest/test/2412-153954-0023.flac  \n",
            "  inflating: blindtest/test/2412-153954-0024.flac  \n",
            "  inflating: blindtest/test/2428-83699-0000.flac  \n",
            "  inflating: blindtest/test/2428-83699-0001.flac  \n",
            "  inflating: blindtest/test/2428-83699-0002.flac  \n",
            "  inflating: blindtest/test/2428-83699-0003.flac  \n",
            "  inflating: blindtest/test/2428-83699-0004.flac  \n",
            "  inflating: blindtest/test/2428-83699-0005.flac  \n",
            "  inflating: blindtest/test/2428-83699-0006.flac  \n",
            "  inflating: blindtest/test/2428-83699-0007.flac  \n",
            "  inflating: blindtest/test/2428-83699-0008.flac  \n",
            "  inflating: blindtest/test/2428-83699-0009.flac  \n",
            "  inflating: blindtest/test/2428-83699-0010.flac  \n",
            "  inflating: blindtest/test/2428-83699-0011.flac  \n",
            "  inflating: blindtest/test/2428-83699-0012.flac  \n",
            "  inflating: blindtest/test/2428-83699-0013.flac  \n",
            "  inflating: blindtest/test/2428-83699-0014.flac  \n",
            "  inflating: blindtest/test/2428-83699-0015.flac  \n",
            "  inflating: blindtest/test/2428-83699-0016.flac  \n",
            "  inflating: blindtest/test/2428-83699-0017.flac  \n",
            "  inflating: blindtest/test/2428-83699-0018.flac  \n",
            "  inflating: blindtest/test/2428-83699-0019.flac  \n",
            "  inflating: blindtest/test/2428-83699-0020.flac  \n",
            "  inflating: blindtest/test/2428-83699-0021.flac  \n",
            "  inflating: blindtest/test/2428-83699-0022.flac  \n",
            "  inflating: blindtest/test/2428-83699-0023.flac  \n",
            "  inflating: blindtest/test/2428-83699-0024.flac  \n",
            "  inflating: blindtest/test/2428-83699-0025.flac  \n",
            "  inflating: blindtest/test/2428-83699-0026.flac  \n",
            "  inflating: blindtest/test/2428-83699-0027.flac  \n",
            "  inflating: blindtest/test/2428-83699-0028.flac  \n",
            "  inflating: blindtest/test/2428-83699-0029.flac  \n",
            "  inflating: blindtest/test/2428-83699-0030.flac  \n",
            "  inflating: blindtest/test/2428-83699-0031.flac  \n",
            "  inflating: blindtest/test/2428-83699-0032.flac  \n",
            "  inflating: blindtest/test/2428-83699-0033.flac  \n",
            "  inflating: blindtest/test/2428-83699-0034.flac  \n",
            "  inflating: blindtest/test/2428-83699-0035.flac  \n",
            "  inflating: blindtest/test/2428-83699-0036.flac  \n",
            "  inflating: blindtest/test/2428-83699-0037.flac  \n",
            "  inflating: blindtest/test/2428-83699-0038.flac  \n",
            "  inflating: blindtest/test/2428-83699-0039.flac  \n",
            "  inflating: blindtest/test/2428-83699-0040.flac  \n",
            "  inflating: blindtest/test/2428-83699-0041.flac  \n",
            "  inflating: blindtest/test/2428-83699-0042.flac  \n",
            "  inflating: blindtest/test/2428-83705-0000.flac  \n",
            "  inflating: blindtest/test/2428-83705-0001.flac  \n",
            "  inflating: blindtest/test/2428-83705-0002.flac  \n",
            "  inflating: blindtest/test/2428-83705-0003.flac  \n",
            "  inflating: blindtest/test/2428-83705-0004.flac  \n",
            "  inflating: blindtest/test/2428-83705-0005.flac  \n",
            "  inflating: blindtest/test/2428-83705-0006.flac  \n",
            "  inflating: blindtest/test/2428-83705-0007.flac  \n",
            "  inflating: blindtest/test/2428-83705-0008.flac  \n",
            "  inflating: blindtest/test/2428-83705-0009.flac  \n",
            "  inflating: blindtest/test/2428-83705-0010.flac  \n",
            "  inflating: blindtest/test/2428-83705-0011.flac  \n",
            "  inflating: blindtest/test/2428-83705-0012.flac  \n",
            "  inflating: blindtest/test/2428-83705-0013.flac  \n",
            "  inflating: blindtest/test/2428-83705-0014.flac  \n",
            "  inflating: blindtest/test/2428-83705-0015.flac  \n",
            "  inflating: blindtest/test/2428-83705-0016.flac  \n",
            "  inflating: blindtest/test/2428-83705-0017.flac  \n",
            "  inflating: blindtest/test/2428-83705-0018.flac  \n",
            "  inflating: blindtest/test/2428-83705-0019.flac  \n",
            "  inflating: blindtest/test/2428-83705-0020.flac  \n",
            "  inflating: blindtest/test/2428-83705-0021.flac  \n",
            "  inflating: blindtest/test/2428-83705-0022.flac  \n",
            "  inflating: blindtest/test/2428-83705-0023.flac  \n",
            "  inflating: blindtest/test/2428-83705-0024.flac  \n",
            "  inflating: blindtest/test/2428-83705-0025.flac  \n",
            "  inflating: blindtest/test/2428-83705-0026.flac  \n",
            "  inflating: blindtest/test/2428-83705-0027.flac  \n",
            "  inflating: blindtest/test/2428-83705-0028.flac  \n",
            "  inflating: blindtest/test/2428-83705-0029.flac  \n",
            "  inflating: blindtest/test/2428-83705-0030.flac  \n",
            "  inflating: blindtest/test/2428-83705-0031.flac  \n",
            "  inflating: blindtest/test/2428-83705-0032.flac  \n",
            "  inflating: blindtest/test/2428-83705-0033.flac  \n",
            "  inflating: blindtest/test/2428-83705-0034.flac  \n",
            "  inflating: blindtest/test/2428-83705-0035.flac  \n",
            "  inflating: blindtest/test/2428-83705-0036.flac  \n",
            "  inflating: blindtest/test/2428-83705-0037.flac  \n",
            "  inflating: blindtest/test/2428-83705-0038.flac  \n",
            "  inflating: blindtest/test/2428-83705-0039.flac  \n",
            "  inflating: blindtest/test/2428-83705-0040.flac  \n",
            "  inflating: blindtest/test/2428-83705-0041.flac  \n",
            "  inflating: blindtest/test/2428-83705-0042.flac  \n",
            "  inflating: blindtest/test/2428-83705-0043.flac  \n",
            "  inflating: blindtest/test/251-118436-0000.flac  \n",
            "  inflating: blindtest/test/251-118436-0001.flac  \n",
            "  inflating: blindtest/test/251-118436-0002.flac  \n",
            "  inflating: blindtest/test/251-118436-0003.flac  \n",
            "  inflating: blindtest/test/251-118436-0004.flac  \n",
            "  inflating: blindtest/test/251-118436-0005.flac  \n",
            "  inflating: blindtest/test/251-118436-0006.flac  \n",
            "  inflating: blindtest/test/251-118436-0007.flac  \n",
            "  inflating: blindtest/test/251-118436-0008.flac  \n",
            "  inflating: blindtest/test/251-118436-0009.flac  \n",
            "  inflating: blindtest/test/251-118436-0010.flac  \n",
            "  inflating: blindtest/test/251-118436-0011.flac  \n",
            "  inflating: blindtest/test/251-118436-0012.flac  \n",
            "  inflating: blindtest/test/251-118436-0013.flac  \n",
            "  inflating: blindtest/test/251-118436-0014.flac  \n",
            "  inflating: blindtest/test/251-118436-0015.flac  \n",
            "  inflating: blindtest/test/251-118436-0016.flac  \n",
            "  inflating: blindtest/test/251-118436-0017.flac  \n",
            "  inflating: blindtest/test/251-118436-0018.flac  \n",
            "  inflating: blindtest/test/251-118436-0019.flac  \n",
            "  inflating: blindtest/test/251-118436-0020.flac  \n",
            "  inflating: blindtest/test/251-118436-0021.flac  \n",
            "  inflating: blindtest/test/251-118436-0022.flac  \n",
            "  inflating: blindtest/test/251-118436-0023.flac  \n",
            "  inflating: blindtest/test/251-136532-0000.flac  \n",
            "  inflating: blindtest/test/251-136532-0001.flac  \n",
            "  inflating: blindtest/test/251-136532-0002.flac  \n",
            "  inflating: blindtest/test/251-136532-0003.flac  \n",
            "  inflating: blindtest/test/251-136532-0004.flac  \n",
            "  inflating: blindtest/test/251-136532-0005.flac  \n",
            "  inflating: blindtest/test/251-136532-0006.flac  \n",
            "  inflating: blindtest/test/251-136532-0007.flac  \n",
            "  inflating: blindtest/test/251-136532-0008.flac  \n",
            "  inflating: blindtest/test/251-136532-0009.flac  \n",
            "  inflating: blindtest/test/251-136532-0010.flac  \n",
            "  inflating: blindtest/test/251-136532-0011.flac  \n",
            "  inflating: blindtest/test/251-136532-0012.flac  \n",
            "  inflating: blindtest/test/251-136532-0013.flac  \n",
            "  inflating: blindtest/test/251-136532-0014.flac  \n",
            "  inflating: blindtest/test/251-136532-0015.flac  \n",
            "  inflating: blindtest/test/251-136532-0016.flac  \n",
            "  inflating: blindtest/test/251-136532-0017.flac  \n",
            "  inflating: blindtest/test/251-136532-0018.flac  \n",
            "  inflating: blindtest/test/251-136532-0019.flac  \n",
            "  inflating: blindtest/test/251-136532-0020.flac  \n",
            "  inflating: blindtest/test/251-136532-0021.flac  \n",
            "  inflating: blindtest/test/251-136532-0022.flac  \n",
            "  inflating: blindtest/test/251-136532-0023.flac  \n",
            "  inflating: blindtest/test/251-137823-0000.flac  \n",
            "  inflating: blindtest/test/251-137823-0001.flac  \n",
            "  inflating: blindtest/test/251-137823-0002.flac  \n",
            "  inflating: blindtest/test/251-137823-0003.flac  \n",
            "  inflating: blindtest/test/251-137823-0004.flac  \n",
            "  inflating: blindtest/test/251-137823-0005.flac  \n",
            "  inflating: blindtest/test/251-137823-0006.flac  \n",
            "  inflating: blindtest/test/251-137823-0007.flac  \n",
            "  inflating: blindtest/test/251-137823-0008.flac  \n",
            "  inflating: blindtest/test/251-137823-0009.flac  \n",
            "  inflating: blindtest/test/251-137823-0010.flac  \n",
            "  inflating: blindtest/test/251-137823-0011.flac  \n",
            "  inflating: blindtest/test/251-137823-0012.flac  \n",
            "  inflating: blindtest/test/251-137823-0013.flac  \n",
            "  inflating: blindtest/test/251-137823-0014.flac  \n",
            "  inflating: blindtest/test/251-137823-0015.flac  \n",
            "  inflating: blindtest/test/251-137823-0016.flac  \n",
            "  inflating: blindtest/test/251-137823-0017.flac  \n",
            "  inflating: blindtest/test/251-137823-0018.flac  \n",
            "  inflating: blindtest/test/251-137823-0019.flac  \n",
            "  inflating: blindtest/test/251-137823-0020.flac  \n",
            "  inflating: blindtest/test/251-137823-0021.flac  \n",
            "  inflating: blindtest/test/251-137823-0022.flac  \n",
            "  inflating: blindtest/test/251-137823-0023.flac  \n",
            "  inflating: blindtest/test/251-137823-0024.flac  \n",
            "  inflating: blindtest/test/251-137823-0025.flac  \n",
            "  inflating: blindtest/test/251-137823-0026.flac  \n",
            "  inflating: blindtest/test/2803-154320-0000.flac  \n",
            "  inflating: blindtest/test/2803-154320-0001.flac  \n",
            "  inflating: blindtest/test/2803-154320-0002.flac  \n",
            "  inflating: blindtest/test/2803-154320-0003.flac  \n",
            "  inflating: blindtest/test/2803-154320-0004.flac  \n",
            "  inflating: blindtest/test/2803-154320-0005.flac  \n",
            "  inflating: blindtest/test/2803-154320-0006.flac  \n",
            "  inflating: blindtest/test/2803-154320-0007.flac  \n",
            "  inflating: blindtest/test/2803-154320-0008.flac  \n",
            "  inflating: blindtest/test/2803-154320-0009.flac  \n",
            "  inflating: blindtest/test/2803-154320-0010.flac  \n",
            "  inflating: blindtest/test/2803-154320-0011.flac  \n",
            "  inflating: blindtest/test/2803-154320-0012.flac  \n",
            "  inflating: blindtest/test/2803-154320-0013.flac  \n",
            "  inflating: blindtest/test/2803-154320-0014.flac  \n",
            "  inflating: blindtest/test/2803-154328-0000.flac  \n",
            "  inflating: blindtest/test/2803-154328-0001.flac  \n",
            "  inflating: blindtest/test/2803-154328-0002.flac  \n",
            "  inflating: blindtest/test/2803-154328-0003.flac  \n",
            "  inflating: blindtest/test/2803-154328-0004.flac  \n",
            "  inflating: blindtest/test/2803-154328-0005.flac  \n",
            "  inflating: blindtest/test/2803-154328-0006.flac  \n",
            "  inflating: blindtest/test/2803-154328-0007.flac  \n",
            "  inflating: blindtest/test/2803-154328-0008.flac  \n",
            "  inflating: blindtest/test/2803-154328-0009.flac  \n",
            "  inflating: blindtest/test/2803-154328-0010.flac  \n",
            "  inflating: blindtest/test/2803-154328-0011.flac  \n",
            "  inflating: blindtest/test/2803-154328-0012.flac  \n",
            "  inflating: blindtest/test/2803-154328-0013.flac  \n",
            "  inflating: blindtest/test/2803-154328-0014.flac  \n",
            "  inflating: blindtest/test/2803-154328-0015.flac  \n",
            "  inflating: blindtest/test/2803-154328-0016.flac  \n",
            "  inflating: blindtest/test/2803-154328-0017.flac  \n",
            "  inflating: blindtest/test/2803-154328-0018.flac  \n",
            "  inflating: blindtest/test/2803-154328-0019.flac  \n",
            "  inflating: blindtest/test/2803-154328-0020.flac  \n",
            "  inflating: blindtest/test/2803-154328-0021.flac  \n",
            "  inflating: blindtest/test/2803-154328-0022.flac  \n",
            "  inflating: blindtest/test/2803-154328-0023.flac  \n",
            "  inflating: blindtest/test/2803-161169-0000.flac  \n",
            "  inflating: blindtest/test/2803-161169-0001.flac  \n",
            "  inflating: blindtest/test/2803-161169-0002.flac  \n",
            "  inflating: blindtest/test/2803-161169-0003.flac  \n",
            "  inflating: blindtest/test/2803-161169-0004.flac  \n",
            "  inflating: blindtest/test/2803-161169-0005.flac  \n",
            "  inflating: blindtest/test/2803-161169-0006.flac  \n",
            "  inflating: blindtest/test/2803-161169-0007.flac  \n",
            "  inflating: blindtest/test/2803-161169-0008.flac  \n",
            "  inflating: blindtest/test/2803-161169-0009.flac  \n",
            "  inflating: blindtest/test/2803-161169-0010.flac  \n",
            "  inflating: blindtest/test/2803-161169-0011.flac  \n",
            "  inflating: blindtest/test/2803-161169-0012.flac  \n",
            "  inflating: blindtest/test/2803-161169-0013.flac  \n",
            "  inflating: blindtest/test/2803-161169-0014.flac  \n",
            "  inflating: blindtest/test/2803-161169-0015.flac  \n",
            "  inflating: blindtest/test/2803-161169-0016.flac  \n",
            "  inflating: blindtest/test/2803-161169-0017.flac  \n",
            "  inflating: blindtest/test/3000-15664-0000.flac  \n",
            "  inflating: blindtest/test/3000-15664-0001.flac  \n",
            "  inflating: blindtest/test/3000-15664-0002.flac  \n",
            "  inflating: blindtest/test/3000-15664-0003.flac  \n",
            "  inflating: blindtest/test/3000-15664-0004.flac  \n",
            "  inflating: blindtest/test/3000-15664-0005.flac  \n",
            "  inflating: blindtest/test/3000-15664-0006.flac  \n",
            "  inflating: blindtest/test/3000-15664-0007.flac  \n",
            "  inflating: blindtest/test/3000-15664-0008.flac  \n",
            "  inflating: blindtest/test/3000-15664-0009.flac  \n",
            "  inflating: blindtest/test/3000-15664-0010.flac  \n",
            "  inflating: blindtest/test/3000-15664-0011.flac  \n",
            "  inflating: blindtest/test/3000-15664-0012.flac  \n",
            "  inflating: blindtest/test/3000-15664-0013.flac  \n",
            "  inflating: blindtest/test/3000-15664-0014.flac  \n",
            "  inflating: blindtest/test/3000-15664-0015.flac  \n",
            "  inflating: blindtest/test/3000-15664-0016.flac  \n",
            "  inflating: blindtest/test/3000-15664-0017.flac  \n",
            "  inflating: blindtest/test/3000-15664-0018.flac  \n",
            "  inflating: blindtest/test/3000-15664-0019.flac  \n",
            "  inflating: blindtest/test/3000-15664-0020.flac  \n",
            "  inflating: blindtest/test/3000-15664-0021.flac  \n",
            "  inflating: blindtest/test/3000-15664-0022.flac  \n",
            "  inflating: blindtest/test/3000-15664-0023.flac  \n",
            "  inflating: blindtest/test/3000-15664-0024.flac  \n",
            "  inflating: blindtest/test/3000-15664-0025.flac  \n",
            "  inflating: blindtest/test/3000-15664-0026.flac  \n",
            "  inflating: blindtest/test/3000-15664-0027.flac  \n",
            "  inflating: blindtest/test/3000-15664-0028.flac  \n",
            "  inflating: blindtest/test/3000-15664-0029.flac  \n",
            "  inflating: blindtest/test/3000-15664-0030.flac  \n",
            "  inflating: blindtest/test/3000-15664-0031.flac  \n",
            "  inflating: blindtest/test/3000-15664-0032.flac  \n",
            "  inflating: blindtest/test/3000-15664-0033.flac  \n",
            "  inflating: blindtest/test/3000-15664-0034.flac  \n",
            "  inflating: blindtest/test/3000-15664-0035.flac  \n",
            "  inflating: blindtest/test/3000-15664-0036.flac  \n",
            "  inflating: blindtest/test/3000-15664-0037.flac  \n",
            "  inflating: blindtest/test/3000-15664-0038.flac  \n",
            "  inflating: blindtest/test/3000-15664-0039.flac  \n",
            "  inflating: blindtest/test/3000-15664-0040.flac  \n",
            "  inflating: blindtest/test/3000-15664-0041.flac  \n",
            "  inflating: blindtest/test/3000-15664-0042.flac  \n",
            "  inflating: blindtest/test/3000-15664-0043.flac  \n",
            "  inflating: blindtest/test/3000-15664-0044.flac  \n",
            "  inflating: blindtest/test/3000-15664-0045.flac  \n",
            "  inflating: blindtest/test/3000-15664-0046.flac  \n",
            "  inflating: blindtest/test/3080-5032-0000.flac  \n",
            "  inflating: blindtest/test/3080-5032-0001.flac  \n",
            "  inflating: blindtest/test/3080-5032-0002.flac  \n",
            "  inflating: blindtest/test/3080-5032-0003.flac  \n",
            "  inflating: blindtest/test/3080-5032-0004.flac  \n",
            "  inflating: blindtest/test/3080-5032-0005.flac  \n",
            "  inflating: blindtest/test/3080-5032-0006.flac  \n",
            "  inflating: blindtest/test/3080-5032-0007.flac  \n",
            "  inflating: blindtest/test/3080-5032-0008.flac  \n",
            "  inflating: blindtest/test/3080-5032-0009.flac  \n",
            "  inflating: blindtest/test/3080-5032-0010.flac  \n",
            "  inflating: blindtest/test/3080-5032-0011.flac  \n",
            "  inflating: blindtest/test/3080-5032-0012.flac  \n",
            "  inflating: blindtest/test/3080-5032-0013.flac  \n",
            "  inflating: blindtest/test/3080-5032-0014.flac  \n",
            "  inflating: blindtest/test/3080-5032-0015.flac  \n",
            "  inflating: blindtest/test/3080-5032-0016.flac  \n",
            "  inflating: blindtest/test/3080-5032-0017.flac  \n",
            "  inflating: blindtest/test/3080-5032-0018.flac  \n",
            "  inflating: blindtest/test/3080-5032-0019.flac  \n",
            "  inflating: blindtest/test/3080-5032-0020.flac  \n",
            "  inflating: blindtest/test/3080-5032-0021.flac  \n",
            "  inflating: blindtest/test/3080-5032-0022.flac  \n",
            "  inflating: blindtest/test/3080-5032-0023.flac  \n",
            "  inflating: blindtest/test/3080-5032-0024.flac  \n",
            "  inflating: blindtest/test/3080-5032-0025.flac  \n",
            "  inflating: blindtest/test/3080-5032-0026.flac  \n",
            "  inflating: blindtest/test/3080-5040-0000.flac  \n",
            "  inflating: blindtest/test/3080-5040-0001.flac  \n",
            "  inflating: blindtest/test/3080-5040-0002.flac  \n",
            "  inflating: blindtest/test/3080-5040-0003.flac  \n",
            "  inflating: blindtest/test/3080-5040-0004.flac  \n",
            "  inflating: blindtest/test/3080-5040-0005.flac  \n",
            "  inflating: blindtest/test/3080-5040-0006.flac  \n",
            "  inflating: blindtest/test/3080-5040-0007.flac  \n",
            "  inflating: blindtest/test/3080-5040-0008.flac  \n",
            "  inflating: blindtest/test/3080-5040-0009.flac  \n",
            "  inflating: blindtest/test/3080-5040-0010.flac  \n",
            "  inflating: blindtest/test/3080-5040-0011.flac  \n",
            "  inflating: blindtest/test/3080-5040-0012.flac  \n",
            "  inflating: blindtest/test/3080-5040-0013.flac  \n",
            "  inflating: blindtest/test/3080-5040-0014.flac  \n",
            "  inflating: blindtest/test/3080-5040-0015.flac  \n",
            "  inflating: blindtest/test/3080-5040-0016.flac  \n",
            "  inflating: blindtest/test/3080-5040-0017.flac  \n",
            "  inflating: blindtest/test/3080-5040-0018.flac  \n",
            "  inflating: blindtest/test/3080-5040-0019.flac  \n",
            "  inflating: blindtest/test/3080-5040-0020.flac  \n",
            "  inflating: blindtest/test/3080-5040-0021.flac  \n",
            "  inflating: blindtest/test/3080-5040-0022.flac  \n",
            "  inflating: blindtest/test/3080-5040-0023.flac  \n",
            "  inflating: blindtest/test/3080-5040-0024.flac  \n",
            "  inflating: blindtest/test/3080-5040-0025.flac  \n",
            "  inflating: blindtest/test/3080-5040-0026.flac  \n",
            "  inflating: blindtest/test/3080-5040-0027.flac  \n",
            "  inflating: blindtest/test/3080-5040-0028.flac  \n",
            "  inflating: blindtest/test/3080-5040-0029.flac  \n",
            "  inflating: blindtest/test/3080-5040-0030.flac  \n",
            "  inflating: blindtest/test/3080-5040-0031.flac  \n",
            "  inflating: blindtest/test/3080-5040-0032.flac  \n",
            "  inflating: blindtest/test/3080-5040-0033.flac  \n",
            "  inflating: blindtest/test/3170-137482-0000.flac  \n",
            "  inflating: blindtest/test/3170-137482-0001.flac  \n",
            "  inflating: blindtest/test/3170-137482-0002.flac  \n",
            "  inflating: blindtest/test/3170-137482-0003.flac  \n",
            "  inflating: blindtest/test/3170-137482-0004.flac  \n",
            "  inflating: blindtest/test/3170-137482-0005.flac  \n",
            "  inflating: blindtest/test/3170-137482-0006.flac  \n",
            "  inflating: blindtest/test/3170-137482-0007.flac  \n",
            "  inflating: blindtest/test/3170-137482-0008.flac  \n",
            "  inflating: blindtest/test/3170-137482-0009.flac  \n",
            "  inflating: blindtest/test/3170-137482-0010.flac  \n",
            "  inflating: blindtest/test/3170-137482-0011.flac  \n",
            "  inflating: blindtest/test/3170-137482-0012.flac  \n",
            "  inflating: blindtest/test/3170-137482-0013.flac  \n",
            "  inflating: blindtest/test/3170-137482-0014.flac  \n",
            "  inflating: blindtest/test/3170-137482-0015.flac  \n",
            "  inflating: blindtest/test/3170-137482-0016.flac  \n",
            "  inflating: blindtest/test/3170-137482-0017.flac  \n",
            "  inflating: blindtest/test/3170-137482-0018.flac  \n",
            "  inflating: blindtest/test/3170-137482-0019.flac  \n",
            "  inflating: blindtest/test/3170-137482-0020.flac  \n",
            "  inflating: blindtest/test/3170-137482-0021.flac  \n",
            "  inflating: blindtest/test/3170-137482-0022.flac  \n",
            "  inflating: blindtest/test/3170-137482-0023.flac  \n",
            "  inflating: blindtest/test/3170-137482-0024.flac  \n",
            "  inflating: blindtest/test/3170-137482-0025.flac  \n",
            "  inflating: blindtest/test/3170-137482-0026.flac  \n",
            "  inflating: blindtest/test/3170-137482-0027.flac  \n",
            "  inflating: blindtest/test/3170-137482-0028.flac  \n",
            "  inflating: blindtest/test/3170-137482-0029.flac  \n",
            "  inflating: blindtest/test/3170-137482-0030.flac  \n",
            "  inflating: blindtest/test/3170-137482-0031.flac  \n",
            "  inflating: blindtest/test/3170-137482-0032.flac  \n",
            "  inflating: blindtest/test/3170-137482-0033.flac  \n",
            "  inflating: blindtest/test/3170-137482-0034.flac  \n",
            "  inflating: blindtest/test/3170-137482-0035.flac  \n",
            "  inflating: blindtest/test/3170-137482-0036.flac  \n",
            "  inflating: blindtest/test/3170-137482-0037.flac  \n",
            "  inflating: blindtest/test/3170-137482-0038.flac  \n",
            "  inflating: blindtest/test/3170-137482-0039.flac  \n",
            "  inflating: blindtest/test/3170-137482-0040.flac  \n",
            "  inflating: blindtest/test/3170-137482-0041.flac  \n",
            "  inflating: blindtest/test/3170-137482-0042.flac  \n",
            "  inflating: blindtest/test/3170-137482-0043.flac  \n",
            "  inflating: blindtest/test/3170-137482-0044.flac  \n",
            "  inflating: blindtest/test/3170-137482-0045.flac  \n",
            "  inflating: blindtest/test/3170-137482-0046.flac  \n",
            "  inflating: blindtest/test/3170-137482-0047.flac  \n",
            "  inflating: blindtest/test/3170-137482-0048.flac  \n",
            "  inflating: blindtest/test/3576-138058-0000.flac  \n",
            "  inflating: blindtest/test/3576-138058-0001.flac  \n",
            "  inflating: blindtest/test/3576-138058-0002.flac  \n",
            "  inflating: blindtest/test/3576-138058-0003.flac  \n",
            "  inflating: blindtest/test/3576-138058-0004.flac  \n",
            "  inflating: blindtest/test/3576-138058-0005.flac  \n",
            "  inflating: blindtest/test/3576-138058-0006.flac  \n",
            "  inflating: blindtest/test/3576-138058-0007.flac  \n",
            "  inflating: blindtest/test/3576-138058-0008.flac  \n",
            "  inflating: blindtest/test/3576-138058-0009.flac  \n",
            "  inflating: blindtest/test/3576-138058-0010.flac  \n",
            "  inflating: blindtest/test/3576-138058-0011.flac  \n",
            "  inflating: blindtest/test/3576-138058-0012.flac  \n",
            "  inflating: blindtest/test/3576-138058-0013.flac  \n",
            "  inflating: blindtest/test/3576-138058-0014.flac  \n",
            "  inflating: blindtest/test/3576-138058-0015.flac  \n",
            "  inflating: blindtest/test/3576-138058-0016.flac  \n",
            "  inflating: blindtest/test/3576-138058-0017.flac  \n",
            "  inflating: blindtest/test/3576-138058-0018.flac  \n",
            "  inflating: blindtest/test/3576-138058-0019.flac  \n",
            "  inflating: blindtest/test/3576-138058-0020.flac  \n",
            "  inflating: blindtest/test/3576-138058-0021.flac  \n",
            "  inflating: blindtest/test/3576-138058-0022.flac  \n",
            "  inflating: blindtest/test/3576-138058-0023.flac  \n",
            "  inflating: blindtest/test/3576-138058-0024.flac  \n",
            "  inflating: blindtest/test/3576-138058-0025.flac  \n",
            "  inflating: blindtest/test/3576-138058-0026.flac  \n",
            "  inflating: blindtest/test/3576-138058-0027.flac  \n",
            "  inflating: blindtest/test/3576-138058-0028.flac  \n",
            "  inflating: blindtest/test/3576-138058-0029.flac  \n",
            "  inflating: blindtest/test/3576-138058-0030.flac  \n",
            "  inflating: blindtest/test/3576-138058-0031.flac  \n",
            "  inflating: blindtest/test/3576-138058-0032.flac  \n",
            "  inflating: blindtest/test/3576-138058-0033.flac  \n",
            "  inflating: blindtest/test/3576-138058-0034.flac  \n",
            "  inflating: blindtest/test/3576-138058-0035.flac  \n",
            "  inflating: blindtest/test/3576-138058-0036.flac  \n",
            "  inflating: blindtest/test/3576-138058-0037.flac  \n",
            "  inflating: blindtest/test/3576-138058-0038.flac  \n",
            "  inflating: blindtest/test/3576-138058-0039.flac  \n",
            "  inflating: blindtest/test/3576-138058-0040.flac  \n",
            "  inflating: blindtest/test/422-122949-0000.flac  \n",
            "  inflating: blindtest/test/422-122949-0001.flac  \n",
            "  inflating: blindtest/test/422-122949-0002.flac  \n",
            "  inflating: blindtest/test/422-122949-0003.flac  \n",
            "  inflating: blindtest/test/422-122949-0004.flac  \n",
            "  inflating: blindtest/test/422-122949-0005.flac  \n",
            "  inflating: blindtest/test/422-122949-0006.flac  \n",
            "  inflating: blindtest/test/422-122949-0007.flac  \n",
            "  inflating: blindtest/test/422-122949-0008.flac  \n",
            "  inflating: blindtest/test/422-122949-0009.flac  \n",
            "  inflating: blindtest/test/422-122949-0010.flac  \n",
            "  inflating: blindtest/test/422-122949-0011.flac  \n",
            "  inflating: blindtest/test/422-122949-0012.flac  \n",
            "  inflating: blindtest/test/422-122949-0013.flac  \n",
            "  inflating: blindtest/test/422-122949-0014.flac  \n",
            "  inflating: blindtest/test/422-122949-0015.flac  \n",
            "  inflating: blindtest/test/422-122949-0016.flac  \n",
            "  inflating: blindtest/test/422-122949-0017.flac  \n",
            "  inflating: blindtest/test/422-122949-0018.flac  \n",
            "  inflating: blindtest/test/422-122949-0019.flac  \n",
            "  inflating: blindtest/test/422-122949-0020.flac  \n",
            "  inflating: blindtest/test/422-122949-0021.flac  \n",
            "  inflating: blindtest/test/422-122949-0022.flac  \n",
            "  inflating: blindtest/test/422-122949-0023.flac  \n",
            "  inflating: blindtest/test/422-122949-0024.flac  \n",
            "  inflating: blindtest/test/422-122949-0025.flac  \n",
            "  inflating: blindtest/test/422-122949-0026.flac  \n",
            "  inflating: blindtest/test/422-122949-0027.flac  \n",
            "  inflating: blindtest/test/422-122949-0028.flac  \n",
            "  inflating: blindtest/test/422-122949-0029.flac  \n",
            "  inflating: blindtest/test/422-122949-0030.flac  \n",
            "  inflating: blindtest/test/422-122949-0031.flac  \n",
            "  inflating: blindtest/test/422-122949-0032.flac  \n",
            "  inflating: blindtest/test/422-122949-0033.flac  \n",
            "  inflating: blindtest/test/422-122949-0034.flac  \n",
            "  inflating: blindtest/test/422-122949-0035.flac  \n",
            "  inflating: blindtest/test/5536-43358-0000.flac  \n",
            "  inflating: blindtest/test/5536-43358-0001.flac  \n",
            "  inflating: blindtest/test/5536-43358-0002.flac  \n",
            "  inflating: blindtest/test/5536-43358-0003.flac  \n",
            "  inflating: blindtest/test/5536-43358-0004.flac  \n",
            "  inflating: blindtest/test/5536-43358-0005.flac  \n",
            "  inflating: blindtest/test/5536-43358-0006.flac  \n",
            "  inflating: blindtest/test/5536-43358-0007.flac  \n",
            "  inflating: blindtest/test/5536-43358-0008.flac  \n",
            "  inflating: blindtest/test/5536-43358-0009.flac  \n",
            "  inflating: blindtest/test/5536-43358-0010.flac  \n",
            "  inflating: blindtest/test/5536-43358-0011.flac  \n",
            "  inflating: blindtest/test/5536-43358-0012.flac  \n",
            "  inflating: blindtest/test/5536-43358-0013.flac  \n",
            "  inflating: blindtest/test/5536-43358-0014.flac  \n",
            "  inflating: blindtest/test/5536-43358-0015.flac  \n",
            "  inflating: blindtest/test/5536-43358-0016.flac  \n",
            "  inflating: blindtest/test/5536-43358-0017.flac  \n",
            "  inflating: blindtest/test/5536-43358-0018.flac  \n",
            "  inflating: blindtest/test/5536-43358-0019.flac  \n",
            "  inflating: blindtest/test/5536-43359-0000.flac  \n",
            "  inflating: blindtest/test/5536-43359-0001.flac  \n",
            "  inflating: blindtest/test/5536-43359-0002.flac  \n",
            "  inflating: blindtest/test/5536-43359-0003.flac  \n",
            "  inflating: blindtest/test/5536-43359-0004.flac  \n",
            "  inflating: blindtest/test/5536-43359-0005.flac  \n",
            "  inflating: blindtest/test/5536-43359-0006.flac  \n",
            "  inflating: blindtest/test/5536-43359-0007.flac  \n",
            "  inflating: blindtest/test/5536-43359-0008.flac  \n",
            "  inflating: blindtest/test/5536-43359-0009.flac  \n",
            "  inflating: blindtest/test/5536-43359-0010.flac  \n",
            "  inflating: blindtest/test/5536-43359-0011.flac  \n",
            "  inflating: blindtest/test/5536-43359-0012.flac  \n",
            "  inflating: blindtest/test/5536-43359-0013.flac  \n",
            "  inflating: blindtest/test/5536-43359-0014.flac  \n",
            "  inflating: blindtest/test/5536-43359-0015.flac  \n",
            "  inflating: blindtest/test/5536-43359-0016.flac  \n",
            "  inflating: blindtest/test/5536-43359-0017.flac  \n",
            "  inflating: blindtest/test/5536-43359-0018.flac  \n",
            "  inflating: blindtest/test/5536-43363-0000.flac  \n",
            "  inflating: blindtest/test/5536-43363-0001.flac  \n",
            "  inflating: blindtest/test/5536-43363-0002.flac  \n",
            "  inflating: blindtest/test/5536-43363-0003.flac  \n",
            "  inflating: blindtest/test/5536-43363-0004.flac  \n",
            "  inflating: blindtest/test/5536-43363-0005.flac  \n",
            "  inflating: blindtest/test/5536-43363-0006.flac  \n",
            "  inflating: blindtest/test/5536-43363-0007.flac  \n",
            "  inflating: blindtest/test/5536-43363-0008.flac  \n",
            "  inflating: blindtest/test/5536-43363-0009.flac  \n",
            "  inflating: blindtest/test/5536-43363-0010.flac  \n",
            "  inflating: blindtest/test/5536-43363-0011.flac  \n",
            "  inflating: blindtest/test/5536-43363-0012.flac  \n",
            "  inflating: blindtest/test/5536-43363-0013.flac  \n",
            "  inflating: blindtest/test/5536-43363-0014.flac  \n",
            "  inflating: blindtest/test/5536-43363-0015.flac  \n",
            "  inflating: blindtest/test/5536-43363-0016.flac  \n",
            "  inflating: blindtest/test/5536-43363-0017.flac  \n",
            "  inflating: blindtest/test/5536-43363-0018.flac  \n",
            "  inflating: blindtest/test/5536-43363-0019.flac  \n",
            "  inflating: blindtest/test/5694-64025-0000.flac  \n",
            "  inflating: blindtest/test/5694-64025-0001.flac  \n",
            "  inflating: blindtest/test/5694-64025-0002.flac  \n",
            "  inflating: blindtest/test/5694-64025-0003.flac  \n",
            "  inflating: blindtest/test/5694-64025-0004.flac  \n",
            "  inflating: blindtest/test/5694-64025-0005.flac  \n",
            "  inflating: blindtest/test/5694-64025-0006.flac  \n",
            "  inflating: blindtest/test/5694-64025-0007.flac  \n",
            "  inflating: blindtest/test/5694-64025-0008.flac  \n",
            "  inflating: blindtest/test/5694-64025-0009.flac  \n",
            "  inflating: blindtest/test/5694-64025-0010.flac  \n",
            "  inflating: blindtest/test/5694-64025-0011.flac  \n",
            "  inflating: blindtest/test/5694-64025-0012.flac  \n",
            "  inflating: blindtest/test/5694-64025-0013.flac  \n",
            "  inflating: blindtest/test/5694-64025-0014.flac  \n",
            "  inflating: blindtest/test/5694-64025-0015.flac  \n",
            "  inflating: blindtest/test/5694-64025-0016.flac  \n",
            "  inflating: blindtest/test/5694-64025-0017.flac  \n",
            "  inflating: blindtest/test/5694-64025-0018.flac  \n",
            "  inflating: blindtest/test/5694-64025-0019.flac  \n",
            "  inflating: blindtest/test/5694-64025-0020.flac  \n",
            "  inflating: blindtest/test/5694-64025-0021.flac  \n",
            "  inflating: blindtest/test/5694-64025-0022.flac  \n",
            "  inflating: blindtest/test/5694-64025-0023.flac  \n",
            "  inflating: blindtest/test/5694-64029-0000.flac  \n",
            "  inflating: blindtest/test/5694-64029-0001.flac  \n",
            "  inflating: blindtest/test/5694-64029-0002.flac  \n",
            "  inflating: blindtest/test/5694-64029-0003.flac  \n",
            "  inflating: blindtest/test/5694-64029-0004.flac  \n",
            "  inflating: blindtest/test/5694-64029-0005.flac  \n",
            "  inflating: blindtest/test/5694-64029-0006.flac  \n",
            "  inflating: blindtest/test/5694-64029-0007.flac  \n",
            "  inflating: blindtest/test/5694-64029-0008.flac  \n",
            "  inflating: blindtest/test/5694-64029-0009.flac  \n",
            "  inflating: blindtest/test/5694-64029-0010.flac  \n",
            "  inflating: blindtest/test/5694-64029-0011.flac  \n",
            "  inflating: blindtest/test/5694-64029-0012.flac  \n",
            "  inflating: blindtest/test/5694-64029-0013.flac  \n",
            "  inflating: blindtest/test/5694-64029-0014.flac  \n",
            "  inflating: blindtest/test/5694-64029-0015.flac  \n",
            "  inflating: blindtest/test/5694-64029-0016.flac  \n",
            "  inflating: blindtest/test/5694-64029-0017.flac  \n",
            "  inflating: blindtest/test/5694-64029-0018.flac  \n",
            "  inflating: blindtest/test/5694-64029-0019.flac  \n",
            "  inflating: blindtest/test/5694-64029-0020.flac  \n",
            "  inflating: blindtest/test/5694-64029-0021.flac  \n",
            "  inflating: blindtest/test/5694-64029-0022.flac  \n",
            "  inflating: blindtest/test/5694-64029-0023.flac  \n",
            "  inflating: blindtest/test/5694-64029-0024.flac  \n",
            "  inflating: blindtest/test/5694-64029-0025.flac  \n",
            "  inflating: blindtest/test/5694-64029-0026.flac  \n",
            "  inflating: blindtest/test/5694-64029-0027.flac  \n",
            "  inflating: blindtest/test/5694-64029-0028.flac  \n",
            "  inflating: blindtest/test/5694-64029-0029.flac  \n",
            "  inflating: blindtest/test/5694-64029-0030.flac  \n",
            "  inflating: blindtest/test/5694-64029-0031.flac  \n",
            "  inflating: blindtest/test/5694-64029-0032.flac  \n",
            "  inflating: blindtest/test/5694-64038-0000.flac  \n",
            "  inflating: blindtest/test/5694-64038-0001.flac  \n",
            "  inflating: blindtest/test/5694-64038-0002.flac  \n",
            "  inflating: blindtest/test/5694-64038-0003.flac  \n",
            "  inflating: blindtest/test/5694-64038-0004.flac  \n",
            "  inflating: blindtest/test/5694-64038-0005.flac  \n",
            "  inflating: blindtest/test/5694-64038-0006.flac  \n",
            "  inflating: blindtest/test/5694-64038-0007.flac  \n",
            "  inflating: blindtest/test/5694-64038-0008.flac  \n",
            "  inflating: blindtest/test/5694-64038-0009.flac  \n",
            "  inflating: blindtest/test/5694-64038-0010.flac  \n",
            "  inflating: blindtest/test/5694-64038-0011.flac  \n",
            "  inflating: blindtest/test/5694-64038-0012.flac  \n",
            "  inflating: blindtest/test/5694-64038-0013.flac  \n",
            "  inflating: blindtest/test/5694-64038-0014.flac  \n",
            "  inflating: blindtest/test/5694-64038-0015.flac  \n",
            "  inflating: blindtest/test/5694-64038-0016.flac  \n",
            "  inflating: blindtest/test/5694-64038-0017.flac  \n",
            "  inflating: blindtest/test/5694-64038-0018.flac  \n",
            "  inflating: blindtest/test/5694-64038-0019.flac  \n",
            "  inflating: blindtest/test/5694-64038-0020.flac  \n",
            "  inflating: blindtest/test/5694-64038-0021.flac  \n",
            "  inflating: blindtest/test/5694-64038-0022.flac  \n",
            "  inflating: blindtest/test/5694-64038-0023.flac  \n",
            "  inflating: blindtest/test/5694-64038-0024.flac  \n",
            "  inflating: blindtest/test/5694-64038-0025.flac  \n",
            "  inflating: blindtest/test/5895-34615-0000.flac  \n",
            "  inflating: blindtest/test/5895-34615-0001.flac  \n",
            "  inflating: blindtest/test/5895-34615-0002.flac  \n",
            "  inflating: blindtest/test/5895-34615-0003.flac  \n",
            "  inflating: blindtest/test/5895-34615-0004.flac  \n",
            "  inflating: blindtest/test/5895-34615-0005.flac  \n",
            "  inflating: blindtest/test/5895-34615-0006.flac  \n",
            "  inflating: blindtest/test/5895-34615-0007.flac  \n",
            "  inflating: blindtest/test/5895-34615-0008.flac  \n",
            "  inflating: blindtest/test/5895-34615-0009.flac  \n",
            "  inflating: blindtest/test/5895-34615-0010.flac  \n",
            "  inflating: blindtest/test/5895-34615-0011.flac  \n",
            "  inflating: blindtest/test/5895-34615-0012.flac  \n",
            "  inflating: blindtest/test/5895-34615-0013.flac  \n",
            "  inflating: blindtest/test/5895-34615-0014.flac  \n",
            "  inflating: blindtest/test/5895-34615-0015.flac  \n",
            "  inflating: blindtest/test/5895-34615-0016.flac  \n",
            "  inflating: blindtest/test/5895-34615-0017.flac  \n",
            "  inflating: blindtest/test/5895-34615-0018.flac  \n",
            "  inflating: blindtest/test/5895-34615-0019.flac  \n",
            "  inflating: blindtest/test/5895-34615-0020.flac  \n",
            "  inflating: blindtest/test/5895-34615-0021.flac  \n",
            "  inflating: blindtest/test/5895-34622-0000.flac  \n",
            "  inflating: blindtest/test/5895-34622-0001.flac  \n",
            "  inflating: blindtest/test/5895-34622-0002.flac  \n",
            "  inflating: blindtest/test/5895-34622-0003.flac  \n",
            "  inflating: blindtest/test/5895-34622-0004.flac  \n",
            "  inflating: blindtest/test/5895-34622-0005.flac  \n",
            "  inflating: blindtest/test/5895-34622-0006.flac  \n",
            "  inflating: blindtest/test/5895-34622-0007.flac  \n",
            "  inflating: blindtest/test/5895-34622-0008.flac  \n",
            "  inflating: blindtest/test/5895-34622-0009.flac  \n",
            "  inflating: blindtest/test/5895-34622-0010.flac  \n",
            "  inflating: blindtest/test/5895-34622-0011.flac  \n",
            "  inflating: blindtest/test/5895-34622-0012.flac  \n",
            "  inflating: blindtest/test/5895-34622-0013.flac  \n",
            "  inflating: blindtest/test/5895-34622-0014.flac  \n",
            "  inflating: blindtest/test/5895-34622-0015.flac  \n",
            "  inflating: blindtest/test/5895-34622-0016.flac  \n",
            "  inflating: blindtest/test/5895-34622-0017.flac  \n",
            "  inflating: blindtest/test/5895-34622-0018.flac  \n",
            "  inflating: blindtest/test/5895-34622-0019.flac  \n",
            "  inflating: blindtest/test/5895-34622-0020.flac  \n",
            "  inflating: blindtest/test/5895-34622-0021.flac  \n",
            "  inflating: blindtest/test/5895-34622-0022.flac  \n",
            "  inflating: blindtest/test/5895-34622-0023.flac  \n",
            "  inflating: blindtest/test/5895-34629-0000.flac  \n",
            "  inflating: blindtest/test/5895-34629-0001.flac  \n",
            "  inflating: blindtest/test/5895-34629-0002.flac  \n",
            "  inflating: blindtest/test/5895-34629-0003.flac  \n",
            "  inflating: blindtest/test/5895-34629-0004.flac  \n",
            "  inflating: blindtest/test/5895-34629-0005.flac  \n",
            "  inflating: blindtest/test/5895-34629-0006.flac  \n",
            "  inflating: blindtest/test/5895-34629-0007.flac  \n",
            "  inflating: blindtest/test/5895-34629-0008.flac  \n",
            "  inflating: blindtest/test/5895-34629-0009.flac  \n",
            "  inflating: blindtest/test/5895-34629-0010.flac  \n",
            "  inflating: blindtest/test/5895-34629-0011.flac  \n",
            "  inflating: blindtest/test/5895-34629-0012.flac  \n",
            "  inflating: blindtest/test/5895-34629-0013.flac  \n",
            "  inflating: blindtest/test/5895-34629-0014.flac  \n",
            "  inflating: blindtest/test/5895-34629-0015.flac  \n",
            "  inflating: blindtest/test/5895-34629-0016.flac  \n",
            "  inflating: blindtest/test/5895-34629-0017.flac  \n",
            "  inflating: blindtest/test/5895-34629-0018.flac  \n",
            "  inflating: blindtest/test/5895-34629-0019.flac  \n",
            "  inflating: blindtest/test/5895-34629-0020.flac  \n",
            "  inflating: blindtest/test/5895-34629-0021.flac  \n",
            "  inflating: blindtest/test/5895-34629-0022.flac  \n",
            "  inflating: blindtest/test/5895-34629-0023.flac  \n",
            "  inflating: blindtest/test/5895-34629-0024.flac  \n",
            "  inflating: blindtest/test/5895-34629-0025.flac  \n",
            "  inflating: blindtest/test/5895-34629-0026.flac  \n",
            "  inflating: blindtest/test/5895-34629-0027.flac  \n",
            "  inflating: blindtest/test/5895-34629-0028.flac  \n",
            "  inflating: blindtest/test/5895-34629-0029.flac  \n",
            "  inflating: blindtest/test/5895-34629-0030.flac  \n",
            "  inflating: blindtest/test/5895-34629-0031.flac  \n",
            "  inflating: blindtest/test/5895-34629-0032.flac  \n",
            "  inflating: blindtest/test/5895-34629-0033.flac  \n",
            "  inflating: blindtest/test/6123-59150-0000.flac  \n",
            "  inflating: blindtest/test/6123-59150-0001.flac  \n",
            "  inflating: blindtest/test/6123-59150-0002.flac  \n",
            "  inflating: blindtest/test/6123-59150-0003.flac  \n",
            "  inflating: blindtest/test/6123-59150-0004.flac  \n",
            "  inflating: blindtest/test/6123-59150-0005.flac  \n",
            "  inflating: blindtest/test/6123-59150-0006.flac  \n",
            "  inflating: blindtest/test/6123-59150-0007.flac  \n",
            "  inflating: blindtest/test/6123-59150-0008.flac  \n",
            "  inflating: blindtest/test/6123-59150-0009.flac  \n",
            "  inflating: blindtest/test/6123-59150-0010.flac  \n",
            "  inflating: blindtest/test/6123-59150-0011.flac  \n",
            "  inflating: blindtest/test/6123-59150-0012.flac  \n",
            "  inflating: blindtest/test/6123-59150-0013.flac  \n",
            "  inflating: blindtest/test/6123-59150-0014.flac  \n",
            "  inflating: blindtest/test/6123-59150-0015.flac  \n",
            "  inflating: blindtest/test/6123-59150-0016.flac  \n",
            "  inflating: blindtest/test/6123-59150-0017.flac  \n",
            "  inflating: blindtest/test/6123-59150-0018.flac  \n",
            "  inflating: blindtest/test/6123-59150-0019.flac  \n",
            "  inflating: blindtest/test/6123-59150-0020.flac  \n",
            "  inflating: blindtest/test/6123-59150-0021.flac  \n",
            "  inflating: blindtest/test/6123-59150-0022.flac  \n",
            "  inflating: blindtest/test/6123-59150-0023.flac  \n",
            "  inflating: blindtest/test/6123-59150-0024.flac  \n",
            "  inflating: blindtest/test/6123-59150-0025.flac  \n",
            "  inflating: blindtest/test/6123-59150-0026.flac  \n",
            "  inflating: blindtest/test/6123-59150-0027.flac  \n",
            "  inflating: blindtest/test/6123-59150-0028.flac  \n",
            "  inflating: blindtest/test/6123-59150-0029.flac  \n",
            "  inflating: blindtest/test/6123-59150-0030.flac  \n",
            "  inflating: blindtest/test/6123-59150-0031.flac  \n",
            "  inflating: blindtest/test/6123-59150-0032.flac  \n",
            "  inflating: blindtest/test/6123-59150-0033.flac  \n",
            "  inflating: blindtest/test/6123-59150-0034.flac  \n",
            "  inflating: blindtest/test/6123-59150-0035.flac  \n",
            "  inflating: blindtest/test/6123-59150-0036.flac  \n",
            "  inflating: blindtest/test/6123-59150-0037.flac  \n",
            "  inflating: blindtest/test/6123-59150-0038.flac  \n",
            "  inflating: blindtest/test/6123-59150-0039.flac  \n",
            "  inflating: blindtest/test/6123-59150-0040.flac  \n",
            "  inflating: blindtest/test/6123-59150-0041.flac  \n",
            "  inflating: blindtest/test/6123-59150-0042.flac  \n",
            "  inflating: blindtest/test/6123-59150-0043.flac  \n",
            "  inflating: blindtest/test/6123-59150-0044.flac  \n",
            "  inflating: blindtest/test/6123-59150-0045.flac  \n",
            "  inflating: blindtest/test/6123-59150-0046.flac  \n",
            "  inflating: blindtest/test/6123-59186-0000.flac  \n",
            "  inflating: blindtest/test/6123-59186-0001.flac  \n",
            "  inflating: blindtest/test/6123-59186-0002.flac  \n",
            "  inflating: blindtest/test/6123-59186-0003.flac  \n",
            "  inflating: blindtest/test/6123-59186-0004.flac  \n",
            "  inflating: blindtest/test/6123-59186-0005.flac  \n",
            "  inflating: blindtest/test/6123-59186-0006.flac  \n",
            "  inflating: blindtest/test/6123-59186-0007.flac  \n",
            "  inflating: blindtest/test/6123-59186-0008.flac  \n",
            "  inflating: blindtest/test/6123-59186-0009.flac  \n",
            "  inflating: blindtest/test/6123-59186-0010.flac  \n",
            "  inflating: blindtest/test/6123-59186-0011.flac  \n",
            "  inflating: blindtest/test/6123-59186-0012.flac  \n",
            "  inflating: blindtest/test/6123-59186-0013.flac  \n",
            "  inflating: blindtest/test/6123-59186-0014.flac  \n",
            "  inflating: blindtest/test/6123-59186-0015.flac  \n",
            "  inflating: blindtest/test/6123-59186-0016.flac  \n",
            "  inflating: blindtest/test/6123-59186-0017.flac  \n",
            "  inflating: blindtest/test/6123-59186-0018.flac  \n",
            "  inflating: blindtest/test/6123-59186-0019.flac  \n",
            "  inflating: blindtest/test/6123-59186-0020.flac  \n",
            "  inflating: blindtest/test/6123-59186-0021.flac  \n",
            "  inflating: blindtest/test/6123-59186-0022.flac  \n",
            "  inflating: blindtest/test/6123-59186-0023.flac  \n",
            "  inflating: blindtest/test/6123-59186-0024.flac  \n",
            "  inflating: blindtest/test/6123-59186-0025.flac  \n",
            "  inflating: blindtest/test/6123-59186-0026.flac  \n",
            "  inflating: blindtest/test/6123-59186-0027.flac  \n",
            "  inflating: blindtest/test/6123-59186-0028.flac  \n",
            "  inflating: blindtest/test/6123-59186-0029.flac  \n",
            "  inflating: blindtest/test/6123-59186-0030.flac  \n",
            "  inflating: blindtest/test/6123-59186-0031.flac  \n",
            "  inflating: blindtest/test/6123-59186-0032.flac  \n",
            "  inflating: blindtest/test/6123-59186-0033.flac  \n",
            "  inflating: blindtest/test/6123-59186-0034.flac  \n",
            "  inflating: blindtest/test/6123-59186-0035.flac  \n",
            "  inflating: blindtest/test/6123-59186-0036.flac  \n",
            "  inflating: blindtest/test/6123-59186-0037.flac  \n",
            "  inflating: blindtest/test/6123-59186-0038.flac  \n",
            "  inflating: blindtest/test/6123-59186-0039.flac  \n",
            "  inflating: blindtest/test/6123-59186-0040.flac  \n",
            "  inflating: blindtest/test/6241-61943-0000.flac  \n",
            "  inflating: blindtest/test/6241-61943-0001.flac  \n",
            "  inflating: blindtest/test/6241-61943-0002.flac  \n",
            "  inflating: blindtest/test/6241-61943-0003.flac  \n",
            "  inflating: blindtest/test/6241-61943-0004.flac  \n",
            "  inflating: blindtest/test/6241-61943-0005.flac  \n",
            "  inflating: blindtest/test/6241-61943-0006.flac  \n",
            "  inflating: blindtest/test/6241-61943-0007.flac  \n",
            "  inflating: blindtest/test/6241-61943-0008.flac  \n",
            "  inflating: blindtest/test/6241-61943-0009.flac  \n",
            "  inflating: blindtest/test/6241-61943-0010.flac  \n",
            "  inflating: blindtest/test/6241-61943-0011.flac  \n",
            "  inflating: blindtest/test/6241-61943-0012.flac  \n",
            "  inflating: blindtest/test/6241-61943-0013.flac  \n",
            "  inflating: blindtest/test/6241-61943-0014.flac  \n",
            "  inflating: blindtest/test/6241-61943-0015.flac  \n",
            "  inflating: blindtest/test/6241-61943-0016.flac  \n",
            "  inflating: blindtest/test/6241-61943-0017.flac  \n",
            "  inflating: blindtest/test/6241-61943-0018.flac  \n",
            "  inflating: blindtest/test/6241-61943-0019.flac  \n",
            "  inflating: blindtest/test/6241-61943-0020.flac  \n",
            "  inflating: blindtest/test/6241-61943-0021.flac  \n",
            "  inflating: blindtest/test/6241-61943-0022.flac  \n",
            "  inflating: blindtest/test/6241-61943-0023.flac  \n",
            "  inflating: blindtest/test/6241-61943-0024.flac  \n",
            "  inflating: blindtest/test/6241-61943-0025.flac  \n",
            "  inflating: blindtest/test/6241-61943-0026.flac  \n",
            "  inflating: blindtest/test/6241-61943-0027.flac  \n",
            "  inflating: blindtest/test/6241-61946-0000.flac  \n",
            "  inflating: blindtest/test/6241-61946-0001.flac  \n",
            "  inflating: blindtest/test/6241-61946-0002.flac  \n",
            "  inflating: blindtest/test/6241-61946-0003.flac  \n",
            "  inflating: blindtest/test/6241-61946-0004.flac  \n",
            "  inflating: blindtest/test/6241-61946-0005.flac  \n",
            "  inflating: blindtest/test/6241-61946-0006.flac  \n",
            "  inflating: blindtest/test/6241-61946-0007.flac  \n",
            "  inflating: blindtest/test/6241-61946-0008.flac  \n",
            "  inflating: blindtest/test/6241-61946-0009.flac  \n",
            "  inflating: blindtest/test/6241-61946-0010.flac  \n",
            "  inflating: blindtest/test/6241-61946-0011.flac  \n",
            "  inflating: blindtest/test/6241-61946-0012.flac  \n",
            "  inflating: blindtest/test/6241-61946-0013.flac  \n",
            "  inflating: blindtest/test/6241-61946-0014.flac  \n",
            "  inflating: blindtest/test/6241-61946-0015.flac  \n",
            "  inflating: blindtest/test/6241-61946-0016.flac  \n",
            "  inflating: blindtest/test/6241-61946-0017.flac  \n",
            "  inflating: blindtest/test/6241-61946-0018.flac  \n",
            "  inflating: blindtest/test/6241-61946-0019.flac  \n",
            "  inflating: blindtest/test/6241-61946-0020.flac  \n",
            "  inflating: blindtest/test/6241-61946-0021.flac  \n",
            "  inflating: blindtest/test/6241-61946-0022.flac  \n",
            "  inflating: blindtest/test/6241-61946-0023.flac  \n",
            "  inflating: blindtest/test/6241-66616-0000.flac  \n",
            "  inflating: blindtest/test/6241-66616-0001.flac  \n",
            "  inflating: blindtest/test/6241-66616-0002.flac  \n",
            "  inflating: blindtest/test/6241-66616-0003.flac  \n",
            "  inflating: blindtest/test/6241-66616-0004.flac  \n",
            "  inflating: blindtest/test/6241-66616-0005.flac  \n",
            "  inflating: blindtest/test/6241-66616-0006.flac  \n",
            "  inflating: blindtest/test/6241-66616-0007.flac  \n",
            "  inflating: blindtest/test/6241-66616-0008.flac  \n",
            "  inflating: blindtest/test/6241-66616-0009.flac  \n",
            "  inflating: blindtest/test/6241-66616-0010.flac  \n",
            "  inflating: blindtest/test/6241-66616-0011.flac  \n",
            "  inflating: blindtest/test/6241-66616-0012.flac  \n",
            "  inflating: blindtest/test/6241-66616-0013.flac  \n",
            "  inflating: blindtest/test/6241-66616-0014.flac  \n",
            "  inflating: blindtest/test/6241-66616-0015.flac  \n",
            "  inflating: blindtest/test/6241-66616-0016.flac  \n",
            "  inflating: blindtest/test/6241-66616-0017.flac  \n",
            "  inflating: blindtest/test/6241-66616-0018.flac  \n",
            "  inflating: blindtest/test/6241-66616-0019.flac  \n",
            "  inflating: blindtest/test/6241-66616-0020.flac  \n",
            "  inflating: blindtest/test/6241-66616-0021.flac  \n",
            "  inflating: blindtest/test/6241-66616-0022.flac  \n",
            "  inflating: blindtest/test/6241-66616-0023.flac  \n",
            "  inflating: blindtest/test/6241-66616-0024.flac  \n",
            "  inflating: blindtest/test/6241-66616-0025.flac  \n",
            "  inflating: blindtest/test/6295-244435-0000.flac  \n",
            "  inflating: blindtest/test/6295-244435-0001.flac  \n",
            "  inflating: blindtest/test/6295-244435-0002.flac  \n",
            "  inflating: blindtest/test/6295-244435-0003.flac  \n",
            "  inflating: blindtest/test/6295-244435-0004.flac  \n",
            "  inflating: blindtest/test/6295-244435-0005.flac  \n",
            "  inflating: blindtest/test/6295-244435-0006.flac  \n",
            "  inflating: blindtest/test/6295-244435-0007.flac  \n",
            "  inflating: blindtest/test/6295-244435-0008.flac  \n",
            "  inflating: blindtest/test/6295-244435-0009.flac  \n",
            "  inflating: blindtest/test/6295-244435-0010.flac  \n",
            "  inflating: blindtest/test/6295-244435-0011.flac  \n",
            "  inflating: blindtest/test/6295-244435-0012.flac  \n",
            "  inflating: blindtest/test/6295-244435-0013.flac  \n",
            "  inflating: blindtest/test/6295-244435-0014.flac  \n",
            "  inflating: blindtest/test/6295-244435-0015.flac  \n",
            "  inflating: blindtest/test/6295-244435-0016.flac  \n",
            "  inflating: blindtest/test/6295-244435-0017.flac  \n",
            "  inflating: blindtest/test/6295-244435-0018.flac  \n",
            "  inflating: blindtest/test/6295-244435-0019.flac  \n",
            "  inflating: blindtest/test/6295-244435-0020.flac  \n",
            "  inflating: blindtest/test/6295-244435-0021.flac  \n",
            "  inflating: blindtest/test/6295-244435-0022.flac  \n",
            "  inflating: blindtest/test/6295-244435-0023.flac  \n",
            "  inflating: blindtest/test/6295-244435-0024.flac  \n",
            "  inflating: blindtest/test/6295-244435-0025.flac  \n",
            "  inflating: blindtest/test/6295-244435-0026.flac  \n",
            "  inflating: blindtest/test/6295-244435-0027.flac  \n",
            "  inflating: blindtest/test/6295-244435-0028.flac  \n",
            "  inflating: blindtest/test/6295-244435-0029.flac  \n",
            "  inflating: blindtest/test/6295-244435-0030.flac  \n",
            "  inflating: blindtest/test/6295-244435-0031.flac  \n",
            "  inflating: blindtest/test/6295-244435-0032.flac  \n",
            "  inflating: blindtest/test/6295-244435-0033.flac  \n",
            "  inflating: blindtest/test/6295-244435-0034.flac  \n",
            "  inflating: blindtest/test/6295-244435-0035.flac  \n",
            "  inflating: blindtest/test/6295-244435-0036.flac  \n",
            "  inflating: blindtest/test/6295-244435-0037.flac  \n",
            "  inflating: blindtest/test/6295-244435-0038.flac  \n",
            "  inflating: blindtest/test/6295-244435-0039.flac  \n",
            "  inflating: blindtest/test/6295-244435-0040.flac  \n",
            "  inflating: blindtest/test/6295-64301-0000.flac  \n",
            "  inflating: blindtest/test/6295-64301-0001.flac  \n",
            "  inflating: blindtest/test/6295-64301-0002.flac  \n",
            "  inflating: blindtest/test/6295-64301-0003.flac  \n",
            "  inflating: blindtest/test/6295-64301-0004.flac  \n",
            "  inflating: blindtest/test/6295-64301-0005.flac  \n",
            "  inflating: blindtest/test/6295-64301-0006.flac  \n",
            "  inflating: blindtest/test/6295-64301-0007.flac  \n",
            "  inflating: blindtest/test/6295-64301-0008.flac  \n",
            "  inflating: blindtest/test/6295-64301-0009.flac  \n",
            "  inflating: blindtest/test/6295-64301-0010.flac  \n",
            "  inflating: blindtest/test/6295-64301-0011.flac  \n",
            "  inflating: blindtest/test/6295-64301-0012.flac  \n",
            "  inflating: blindtest/test/6295-64301-0013.flac  \n",
            "  inflating: blindtest/test/6295-64301-0014.flac  \n",
            "  inflating: blindtest/test/6295-64301-0015.flac  \n",
            "  inflating: blindtest/test/6295-64301-0016.flac  \n",
            "  inflating: blindtest/test/6295-64301-0017.flac  \n",
            "  inflating: blindtest/test/6295-64301-0018.flac  \n",
            "  inflating: blindtest/test/6295-64301-0019.flac  \n",
            "  inflating: blindtest/test/6295-64301-0020.flac  \n",
            "  inflating: blindtest/test/6295-64301-0021.flac  \n",
            "  inflating: blindtest/test/6295-64301-0022.flac  \n",
            "  inflating: blindtest/test/6295-64301-0023.flac  \n",
            "  inflating: blindtest/test/6295-64301-0024.flac  \n",
            "  inflating: blindtest/test/6295-64301-0025.flac  \n",
            "  inflating: blindtest/test/6295-64301-0026.flac  \n",
            "  inflating: blindtest/test/6295-64301-0027.flac  \n",
            "  inflating: blindtest/test/6295-64301-0028.flac  \n",
            "  inflating: blindtest/test/6295-64301-0029.flac  \n",
            "  inflating: blindtest/test/6295-64301-0030.flac  \n",
            "  inflating: blindtest/test/6295-64301-0031.flac  \n",
            "  inflating: blindtest/test/6295-64301-0032.flac  \n",
            "  inflating: blindtest/test/6319-275224-0000.flac  \n",
            "  inflating: blindtest/test/6319-275224-0001.flac  \n",
            "  inflating: blindtest/test/6319-275224-0002.flac  \n",
            "  inflating: blindtest/test/6319-275224-0003.flac  \n",
            "  inflating: blindtest/test/6319-275224-0004.flac  \n",
            "  inflating: blindtest/test/6319-275224-0005.flac  \n",
            "  inflating: blindtest/test/6319-275224-0006.flac  \n",
            "  inflating: blindtest/test/6319-275224-0007.flac  \n",
            "  inflating: blindtest/test/6319-275224-0008.flac  \n",
            "  inflating: blindtest/test/6319-275224-0009.flac  \n",
            "  inflating: blindtest/test/6319-275224-0010.flac  \n",
            "  inflating: blindtest/test/6319-275224-0011.flac  \n",
            "  inflating: blindtest/test/6319-275224-0012.flac  \n",
            "  inflating: blindtest/test/6319-275224-0013.flac  \n",
            "  inflating: blindtest/test/6319-275224-0014.flac  \n",
            "  inflating: blindtest/test/6319-275224-0015.flac  \n",
            "  inflating: blindtest/test/6319-275224-0016.flac  \n",
            "  inflating: blindtest/test/6319-275224-0017.flac  \n",
            "  inflating: blindtest/test/6319-275224-0018.flac  \n",
            "  inflating: blindtest/test/6319-275224-0019.flac  \n",
            "  inflating: blindtest/test/6319-275224-0020.flac  \n",
            "  inflating: blindtest/test/6319-57405-0000.flac  \n",
            "  inflating: blindtest/test/6319-57405-0001.flac  \n",
            "  inflating: blindtest/test/6319-57405-0002.flac  \n",
            "  inflating: blindtest/test/6319-57405-0003.flac  \n",
            "  inflating: blindtest/test/6319-57405-0004.flac  \n",
            "  inflating: blindtest/test/6319-57405-0005.flac  \n",
            "  inflating: blindtest/test/6319-57405-0006.flac  \n",
            "  inflating: blindtest/test/6319-57405-0007.flac  \n",
            "  inflating: blindtest/test/6319-57405-0008.flac  \n",
            "  inflating: blindtest/test/6319-57405-0009.flac  \n",
            "  inflating: blindtest/test/6319-57405-0010.flac  \n",
            "  inflating: blindtest/test/6319-57405-0011.flac  \n",
            "  inflating: blindtest/test/6319-57405-0012.flac  \n",
            "  inflating: blindtest/test/6319-64726-0000.flac  \n",
            "  inflating: blindtest/test/6319-64726-0001.flac  \n",
            "  inflating: blindtest/test/6319-64726-0002.flac  \n",
            "  inflating: blindtest/test/6319-64726-0003.flac  \n",
            "  inflating: blindtest/test/6319-64726-0004.flac  \n",
            "  inflating: blindtest/test/6319-64726-0005.flac  \n",
            "  inflating: blindtest/test/6319-64726-0006.flac  \n",
            "  inflating: blindtest/test/6319-64726-0007.flac  \n",
            "  inflating: blindtest/test/6319-64726-0008.flac  \n",
            "  inflating: blindtest/test/6319-64726-0009.flac  \n",
            "  inflating: blindtest/test/6319-64726-0010.flac  \n",
            "  inflating: blindtest/test/6319-64726-0011.flac  \n",
            "  inflating: blindtest/test/6319-64726-0012.flac  \n",
            "  inflating: blindtest/test/6319-64726-0013.flac  \n",
            "  inflating: blindtest/test/6319-64726-0014.flac  \n",
            "  inflating: blindtest/test/6319-64726-0015.flac  \n",
            "  inflating: blindtest/test/6319-64726-0016.flac  \n",
            "  inflating: blindtest/test/6319-64726-0017.flac  \n",
            "  inflating: blindtest/test/6319-64726-0018.flac  \n",
            "  inflating: blindtest/test/6319-64726-0019.flac  \n",
            "  inflating: blindtest/test/6319-64726-0020.flac  \n",
            "  inflating: blindtest/test/6345-64257-0000.flac  \n",
            "  inflating: blindtest/test/6345-64257-0001.flac  \n",
            "  inflating: blindtest/test/6345-64257-0002.flac  \n",
            "  inflating: blindtest/test/6345-64257-0003.flac  \n",
            "  inflating: blindtest/test/6345-64257-0004.flac  \n",
            "  inflating: blindtest/test/6345-64257-0005.flac  \n",
            "  inflating: blindtest/test/6345-64257-0006.flac  \n",
            "  inflating: blindtest/test/6345-64257-0007.flac  \n",
            "  inflating: blindtest/test/6345-64257-0008.flac  \n",
            "  inflating: blindtest/test/6345-64257-0009.flac  \n",
            "  inflating: blindtest/test/6345-64257-0010.flac  \n",
            "  inflating: blindtest/test/6345-64257-0011.flac  \n",
            "  inflating: blindtest/test/6345-64257-0012.flac  \n",
            "  inflating: blindtest/test/6345-64257-0013.flac  \n",
            "  inflating: blindtest/test/6345-64257-0014.flac  \n",
            "  inflating: blindtest/test/6345-64257-0015.flac  \n",
            "  inflating: blindtest/test/6345-64257-0016.flac  \n",
            "  inflating: blindtest/test/6345-64257-0017.flac  \n",
            "  inflating: blindtest/test/6345-64257-0018.flac  \n",
            "  inflating: blindtest/test/6345-64257-0019.flac  \n",
            "  inflating: blindtest/test/6345-64257-0020.flac  \n",
            "  inflating: blindtest/test/6345-93302-0000.flac  \n",
            "  inflating: blindtest/test/6345-93302-0001.flac  \n",
            "  inflating: blindtest/test/6345-93302-0002.flac  \n",
            "  inflating: blindtest/test/6345-93302-0003.flac  \n",
            "  inflating: blindtest/test/6345-93302-0004.flac  \n",
            "  inflating: blindtest/test/6345-93302-0005.flac  \n",
            "  inflating: blindtest/test/6345-93302-0006.flac  \n",
            "  inflating: blindtest/test/6345-93302-0007.flac  \n",
            "  inflating: blindtest/test/6345-93302-0008.flac  \n",
            "  inflating: blindtest/test/6345-93302-0009.flac  \n",
            "  inflating: blindtest/test/6345-93302-0010.flac  \n",
            "  inflating: blindtest/test/6345-93302-0011.flac  \n",
            "  inflating: blindtest/test/6345-93302-0012.flac  \n",
            "  inflating: blindtest/test/6345-93302-0013.flac  \n",
            "  inflating: blindtest/test/6345-93302-0014.flac  \n",
            "  inflating: blindtest/test/6345-93302-0015.flac  \n",
            "  inflating: blindtest/test/6345-93302-0016.flac  \n",
            "  inflating: blindtest/test/6345-93302-0017.flac  \n",
            "  inflating: blindtest/test/6345-93302-0018.flac  \n",
            "  inflating: blindtest/test/6345-93302-0019.flac  \n",
            "  inflating: blindtest/test/6345-93302-0020.flac  \n",
            "  inflating: blindtest/test/6345-93302-0021.flac  \n",
            "  inflating: blindtest/test/6345-93302-0022.flac  \n",
            "  inflating: blindtest/test/6345-93302-0023.flac  \n",
            "  inflating: blindtest/test/6345-93302-0024.flac  \n",
            "  inflating: blindtest/test/6345-93302-0025.flac  \n",
            "  inflating: blindtest/test/6345-93302-0026.flac  \n",
            "  inflating: blindtest/test/6345-93302-0027.flac  \n",
            "  inflating: blindtest/test/6345-93302-0028.flac  \n",
            "  inflating: blindtest/test/6345-93302-0029.flac  \n",
            "  inflating: blindtest/test/6345-93306-0000.flac  \n",
            "  inflating: blindtest/test/6345-93306-0001.flac  \n",
            "  inflating: blindtest/test/6345-93306-0002.flac  \n",
            "  inflating: blindtest/test/6345-93306-0003.flac  \n",
            "  inflating: blindtest/test/6345-93306-0004.flac  \n",
            "  inflating: blindtest/test/6345-93306-0005.flac  \n",
            "  inflating: blindtest/test/6345-93306-0006.flac  \n",
            "  inflating: blindtest/test/6345-93306-0007.flac  \n",
            "  inflating: blindtest/test/6345-93306-0008.flac  \n",
            "  inflating: blindtest/test/6345-93306-0009.flac  \n",
            "  inflating: blindtest/test/6345-93306-0010.flac  \n",
            "  inflating: blindtest/test/6345-93306-0011.flac  \n",
            "  inflating: blindtest/test/6345-93306-0012.flac  \n",
            "  inflating: blindtest/test/6345-93306-0013.flac  \n",
            "  inflating: blindtest/test/6345-93306-0014.flac  \n",
            "  inflating: blindtest/test/6345-93306-0015.flac  \n",
            "  inflating: blindtest/test/6345-93306-0016.flac  \n",
            "  inflating: blindtest/test/6345-93306-0017.flac  \n",
            "  inflating: blindtest/test/6345-93306-0018.flac  \n",
            "  inflating: blindtest/test/6345-93306-0019.flac  \n",
            "  inflating: blindtest/test/6345-93306-0020.flac  \n",
            "  inflating: blindtest/test/6345-93306-0021.flac  \n",
            "  inflating: blindtest/test/6345-93306-0022.flac  \n",
            "  inflating: blindtest/test/6345-93306-0023.flac  \n",
            "  inflating: blindtest/test/6345-93306-0024.flac  \n",
            "  inflating: blindtest/test/6345-93306-0025.flac  \n",
            "  inflating: blindtest/test/6455-66379-0000.flac  \n",
            "  inflating: blindtest/test/6455-66379-0001.flac  \n",
            "  inflating: blindtest/test/6455-66379-0002.flac  \n",
            "  inflating: blindtest/test/6455-66379-0003.flac  \n",
            "  inflating: blindtest/test/6455-66379-0004.flac  \n",
            "  inflating: blindtest/test/6455-66379-0005.flac  \n",
            "  inflating: blindtest/test/6455-66379-0006.flac  \n",
            "  inflating: blindtest/test/6455-66379-0007.flac  \n",
            "  inflating: blindtest/test/6455-66379-0008.flac  \n",
            "  inflating: blindtest/test/6455-66379-0009.flac  \n",
            "  inflating: blindtest/test/6455-66379-0010.flac  \n",
            "  inflating: blindtest/test/6455-66379-0011.flac  \n",
            "  inflating: blindtest/test/6455-66379-0012.flac  \n",
            "  inflating: blindtest/test/6455-66379-0013.flac  \n",
            "  inflating: blindtest/test/6455-66379-0014.flac  \n",
            "  inflating: blindtest/test/6455-66379-0015.flac  \n",
            "  inflating: blindtest/test/6455-66379-0016.flac  \n",
            "  inflating: blindtest/test/6455-66379-0017.flac  \n",
            "  inflating: blindtest/test/6455-66379-0018.flac  \n",
            "  inflating: blindtest/test/6455-66379-0019.flac  \n",
            "  inflating: blindtest/test/6455-67803-0000.flac  \n",
            "  inflating: blindtest/test/6455-67803-0001.flac  \n",
            "  inflating: blindtest/test/6455-67803-0002.flac  \n",
            "  inflating: blindtest/test/6455-67803-0003.flac  \n",
            "  inflating: blindtest/test/6455-67803-0004.flac  \n",
            "  inflating: blindtest/test/6455-67803-0005.flac  \n",
            "  inflating: blindtest/test/6455-67803-0006.flac  \n",
            "  inflating: blindtest/test/6455-67803-0007.flac  \n",
            "  inflating: blindtest/test/6455-67803-0008.flac  \n",
            "  inflating: blindtest/test/6455-67803-0009.flac  \n",
            "  inflating: blindtest/test/6455-67803-0010.flac  \n",
            "  inflating: blindtest/test/6455-67803-0011.flac  \n",
            "  inflating: blindtest/test/6455-67803-0012.flac  \n",
            "  inflating: blindtest/test/6455-67803-0013.flac  \n",
            "  inflating: blindtest/test/6455-67803-0014.flac  \n",
            "  inflating: blindtest/test/6455-67803-0015.flac  \n",
            "  inflating: blindtest/test/6455-67803-0016.flac  \n",
            "  inflating: blindtest/test/6455-67803-0017.flac  \n",
            "  inflating: blindtest/test/6455-67803-0018.flac  \n",
            "  inflating: blindtest/test/6455-67803-0019.flac  \n",
            "  inflating: blindtest/test/6455-67803-0020.flac  \n",
            "  inflating: blindtest/test/6455-67803-0021.flac  \n",
            "  inflating: blindtest/test/6455-67803-0022.flac  \n",
            "  inflating: blindtest/test/6455-67803-0023.flac  \n",
            "  inflating: blindtest/test/6455-67803-0024.flac  \n",
            "  inflating: blindtest/test/6455-67803-0025.flac  \n",
            "  inflating: blindtest/test/6455-67803-0026.flac  \n",
            "  inflating: blindtest/test/6455-67803-0027.flac  \n",
            "  inflating: blindtest/test/6455-67803-0028.flac  \n",
            "  inflating: blindtest/test/6455-67803-0029.flac  \n",
            "  inflating: blindtest/test/6455-67803-0030.flac  \n",
            "  inflating: blindtest/test/6455-67803-0031.flac  \n",
            "  inflating: blindtest/test/6455-67803-0032.flac  \n",
            "  inflating: blindtest/test/6455-67803-0033.flac  \n",
            "  inflating: blindtest/test/6455-67803-0034.flac  \n",
            "  inflating: blindtest/test/6455-67803-0035.flac  \n",
            "  inflating: blindtest/test/6455-67803-0036.flac  \n",
            "  inflating: blindtest/test/6455-67804-0000.flac  \n",
            "  inflating: blindtest/test/6455-67804-0001.flac  \n",
            "  inflating: blindtest/test/6455-67804-0002.flac  \n",
            "  inflating: blindtest/test/6455-67804-0003.flac  \n",
            "  inflating: blindtest/test/6455-67804-0004.flac  \n",
            "  inflating: blindtest/test/6455-67804-0005.flac  \n",
            "  inflating: blindtest/test/6455-67804-0006.flac  \n",
            "  inflating: blindtest/test/6455-67804-0007.flac  \n",
            "  inflating: blindtest/test/6455-67804-0008.flac  \n",
            "  inflating: blindtest/test/6455-67804-0009.flac  \n",
            "  inflating: blindtest/test/6455-67804-0010.flac  \n",
            "  inflating: blindtest/test/6455-67804-0011.flac  \n",
            "  inflating: blindtest/test/6455-67804-0012.flac  \n",
            "  inflating: blindtest/test/6455-67804-0013.flac  \n",
            "  inflating: blindtest/test/6455-67804-0014.flac  \n",
            "  inflating: blindtest/test/6455-67804-0015.flac  \n",
            "  inflating: blindtest/test/6455-67804-0016.flac  \n",
            "  inflating: blindtest/test/6455-67804-0017.flac  \n",
            "  inflating: blindtest/test/6455-67804-0018.flac  \n",
            "  inflating: blindtest/test/6455-67804-0019.flac  \n",
            "  inflating: blindtest/test/6455-67804-0020.flac  \n",
            "  inflating: blindtest/test/6455-67804-0021.flac  \n",
            "  inflating: blindtest/test/6455-67804-0022.flac  \n",
            "  inflating: blindtest/test/6455-67804-0023.flac  \n",
            "  inflating: blindtest/test/6455-67804-0024.flac  \n",
            "  inflating: blindtest/test/6455-67804-0025.flac  \n",
            "  inflating: blindtest/test/6455-67804-0026.flac  \n",
            "  inflating: blindtest/test/6455-67804-0027.flac  \n",
            "  inflating: blindtest/test/6455-67804-0028.flac  \n",
            "  inflating: blindtest/test/6455-67804-0029.flac  \n",
            "  inflating: blindtest/test/6455-67804-0030.flac  \n",
            "  inflating: blindtest/test/6455-67804-0031.flac  \n",
            "  inflating: blindtest/test/6455-67804-0032.flac  \n",
            "  inflating: blindtest/test/6455-67804-0033.flac  \n",
            "  inflating: blindtest/test/6455-67804-0034.flac  \n",
            "  inflating: blindtest/test/6455-67804-0035.flac  \n",
            "  inflating: blindtest/test/6455-67804-0036.flac  \n",
            "  inflating: blindtest/test/6455-67804-0037.flac  \n",
            "  inflating: blindtest/test/6455-67804-0038.flac  \n",
            "  inflating: blindtest/test/6455-67804-0039.flac  \n",
            "  inflating: blindtest/test/6455-67804-0040.flac  \n",
            "  inflating: blindtest/test/6599-38590-0000.flac  \n",
            "  inflating: blindtest/test/6599-38590-0001.flac  \n",
            "  inflating: blindtest/test/6599-38590-0002.flac  \n",
            "  inflating: blindtest/test/6599-38590-0003.flac  \n",
            "  inflating: blindtest/test/6599-38590-0004.flac  \n",
            "  inflating: blindtest/test/6599-38590-0005.flac  \n",
            "  inflating: blindtest/test/6599-38590-0006.flac  \n",
            "  inflating: blindtest/test/6599-38590-0007.flac  \n",
            "  inflating: blindtest/test/6599-38590-0008.flac  \n",
            "  inflating: blindtest/test/6599-38590-0009.flac  \n",
            "  inflating: blindtest/test/6599-38591-0000.flac  \n",
            "  inflating: blindtest/test/6599-38591-0001.flac  \n",
            "  inflating: blindtest/test/6599-38591-0002.flac  \n",
            "  inflating: blindtest/test/6599-38591-0003.flac  \n",
            "  inflating: blindtest/test/6599-38591-0004.flac  \n",
            "  inflating: blindtest/test/6599-38591-0005.flac  \n",
            "  inflating: blindtest/test/6599-38591-0006.flac  \n",
            "  inflating: blindtest/test/6599-38591-0007.flac  \n",
            "  inflating: blindtest/test/6599-38591-0008.flac  \n",
            "  inflating: blindtest/test/6599-38591-0009.flac  \n",
            "  inflating: blindtest/test/6599-38591-0010.flac  \n",
            "  inflating: blindtest/test/6599-38591-0011.flac  \n",
            "  inflating: blindtest/test/6599-38591-0012.flac  \n",
            "  inflating: blindtest/test/6599-38591-0013.flac  \n",
            "  inflating: blindtest/test/6938-70848-0000.flac  \n",
            "  inflating: blindtest/test/6938-70848-0001.flac  \n",
            "  inflating: blindtest/test/6938-70848-0002.flac  \n",
            "  inflating: blindtest/test/6938-70848-0003.flac  \n",
            "  inflating: blindtest/test/6938-70848-0004.flac  \n",
            "  inflating: blindtest/test/6938-70848-0005.flac  \n",
            "  inflating: blindtest/test/6938-70848-0006.flac  \n",
            "  inflating: blindtest/test/6938-70848-0007.flac  \n",
            "  inflating: blindtest/test/6938-70848-0008.flac  \n",
            "  inflating: blindtest/test/6938-70848-0009.flac  \n",
            "  inflating: blindtest/test/6938-70848-0010.flac  \n",
            "  inflating: blindtest/test/6938-70848-0011.flac  \n",
            "  inflating: blindtest/test/6938-70848-0012.flac  \n",
            "  inflating: blindtest/test/6938-70848-0013.flac  \n",
            "  inflating: blindtest/test/6938-70848-0014.flac  \n",
            "  inflating: blindtest/test/6938-70848-0015.flac  \n",
            "  inflating: blindtest/test/6938-70848-0016.flac  \n",
            "  inflating: blindtest/test/6938-70848-0017.flac  \n",
            "  inflating: blindtest/test/6938-70848-0018.flac  \n",
            "  inflating: blindtest/test/6938-70848-0019.flac  \n",
            "  inflating: blindtest/test/6938-70848-0020.flac  \n",
            "  inflating: blindtest/test/6938-70848-0021.flac  \n",
            "  inflating: blindtest/test/6938-70848-0022.flac  \n",
            "  inflating: blindtest/test/6938-70848-0023.flac  \n",
            "  inflating: blindtest/test/6938-70848-0024.flac  \n",
            "  inflating: blindtest/test/6938-70848-0025.flac  \n",
            "  inflating: blindtest/test/6938-70848-0026.flac  \n",
            "  inflating: blindtest/test/6938-70848-0027.flac  \n",
            "  inflating: blindtest/test/6938-70848-0028.flac  \n",
            "  inflating: blindtest/test/6938-70848-0029.flac  \n",
            "  inflating: blindtest/test/6938-70848-0030.flac  \n",
            "  inflating: blindtest/test/7018-75788-0000.flac  \n",
            "  inflating: blindtest/test/7018-75788-0001.flac  \n",
            "  inflating: blindtest/test/7018-75788-0002.flac  \n",
            "  inflating: blindtest/test/7018-75788-0003.flac  \n",
            "  inflating: blindtest/test/7018-75788-0004.flac  \n",
            "  inflating: blindtest/test/7018-75788-0005.flac  \n",
            "  inflating: blindtest/test/7018-75788-0006.flac  \n",
            "  inflating: blindtest/test/7018-75788-0007.flac  \n",
            "  inflating: blindtest/test/7018-75788-0008.flac  \n",
            "  inflating: blindtest/test/7018-75788-0009.flac  \n",
            "  inflating: blindtest/test/7018-75788-0010.flac  \n",
            "  inflating: blindtest/test/7018-75788-0011.flac  \n",
            "  inflating: blindtest/test/7018-75788-0012.flac  \n",
            "  inflating: blindtest/test/7018-75788-0013.flac  \n",
            "  inflating: blindtest/test/7018-75788-0014.flac  \n",
            "  inflating: blindtest/test/7018-75788-0015.flac  \n",
            "  inflating: blindtest/test/7018-75788-0016.flac  \n",
            "  inflating: blindtest/test/7018-75788-0017.flac  \n",
            "  inflating: blindtest/test/7018-75788-0018.flac  \n",
            "  inflating: blindtest/test/7018-75788-0019.flac  \n",
            "  inflating: blindtest/test/7018-75789-0000.flac  \n",
            "  inflating: blindtest/test/7018-75789-0001.flac  \n",
            "  inflating: blindtest/test/7018-75789-0002.flac  \n",
            "  inflating: blindtest/test/7018-75789-0003.flac  \n",
            "  inflating: blindtest/test/7018-75789-0004.flac  \n",
            "  inflating: blindtest/test/7018-75789-0005.flac  \n",
            "  inflating: blindtest/test/7018-75789-0006.flac  \n",
            "  inflating: blindtest/test/7018-75789-0007.flac  \n",
            "  inflating: blindtest/test/7018-75789-0008.flac  \n",
            "  inflating: blindtest/test/7018-75789-0009.flac  \n",
            "  inflating: blindtest/test/7018-75789-0010.flac  \n",
            "  inflating: blindtest/test/7018-75789-0011.flac  \n",
            "  inflating: blindtest/test/7018-75789-0012.flac  \n",
            "  inflating: blindtest/test/7018-75789-0013.flac  \n",
            "  inflating: blindtest/test/7018-75789-0014.flac  \n",
            "  inflating: blindtest/test/7018-75789-0015.flac  \n",
            "  inflating: blindtest/test/7018-75789-0016.flac  \n",
            "  inflating: blindtest/test/7018-75789-0017.flac  \n",
            "  inflating: blindtest/test/7018-75789-0018.flac  \n",
            "  inflating: blindtest/test/7018-75789-0019.flac  \n",
            "  inflating: blindtest/test/7018-75789-0020.flac  \n",
            "  inflating: blindtest/test/7018-75789-0021.flac  \n",
            "  inflating: blindtest/test/7018-75789-0022.flac  \n",
            "  inflating: blindtest/test/7018-75789-0023.flac  \n",
            "  inflating: blindtest/test/7018-75789-0024.flac  \n",
            "  inflating: blindtest/test/7018-75789-0025.flac  \n",
            "  inflating: blindtest/test/7018-75789-0026.flac  \n",
            "  inflating: blindtest/test/7018-75789-0027.flac  \n",
            "  inflating: blindtest/test/7018-75789-0028.flac  \n",
            "  inflating: blindtest/test/7018-75789-0029.flac  \n",
            "  inflating: blindtest/test/7018-75789-0030.flac  \n",
            "  inflating: blindtest/test/7018-75789-0031.flac  \n",
            "  inflating: blindtest/test/7105-2330-0000.flac  \n",
            "  inflating: blindtest/test/7105-2330-0001.flac  \n",
            "  inflating: blindtest/test/7105-2330-0002.flac  \n",
            "  inflating: blindtest/test/7105-2330-0003.flac  \n",
            "  inflating: blindtest/test/7105-2330-0004.flac  \n",
            "  inflating: blindtest/test/7105-2330-0005.flac  \n",
            "  inflating: blindtest/test/7105-2330-0006.flac  \n",
            "  inflating: blindtest/test/7105-2330-0007.flac  \n",
            "  inflating: blindtest/test/7105-2330-0008.flac  \n",
            "  inflating: blindtest/test/7105-2330-0009.flac  \n",
            "  inflating: blindtest/test/7105-2330-0010.flac  \n",
            "  inflating: blindtest/test/7105-2330-0011.flac  \n",
            "  inflating: blindtest/test/7105-2330-0012.flac  \n",
            "  inflating: blindtest/test/7105-2330-0013.flac  \n",
            "  inflating: blindtest/test/7105-2330-0014.flac  \n",
            "  inflating: blindtest/test/7105-2330-0015.flac  \n",
            "  inflating: blindtest/test/7105-2330-0016.flac  \n",
            "  inflating: blindtest/test/7105-2330-0017.flac  \n",
            "  inflating: blindtest/test/7105-2330-0018.flac  \n",
            "  inflating: blindtest/test/7105-2330-0019.flac  \n",
            "  inflating: blindtest/test/7105-2330-0020.flac  \n",
            "  inflating: blindtest/test/7105-2330-0021.flac  \n",
            "  inflating: blindtest/test/7105-2330-0022.flac  \n",
            "  inflating: blindtest/test/7105-2330-0023.flac  \n",
            "  inflating: blindtest/test/7105-2330-0024.flac  \n",
            "  inflating: blindtest/test/7105-2330-0025.flac  \n",
            "  inflating: blindtest/test/7105-2330-0026.flac  \n",
            "  inflating: blindtest/test/7105-2330-0027.flac  \n",
            "  inflating: blindtest/test/7105-2330-0028.flac  \n",
            "  inflating: blindtest/test/7105-2330-0029.flac  \n",
            "  inflating: blindtest/test/7105-2330-0030.flac  \n",
            "  inflating: blindtest/test/7105-2330-0031.flac  \n",
            "  inflating: blindtest/test/7105-2330-0032.flac  \n",
            "  inflating: blindtest/test/7105-2330-0033.flac  \n",
            "  inflating: blindtest/test/7105-2330-0034.flac  \n",
            "  inflating: blindtest/test/7105-2330-0035.flac  \n",
            "  inflating: blindtest/test/7105-2330-0036.flac  \n",
            "  inflating: blindtest/test/7105-2330-0037.flac  \n",
            "  inflating: blindtest/test/7105-2330-0038.flac  \n",
            "  inflating: blindtest/test/7105-2330-0039.flac  \n",
            "  inflating: blindtest/test/7105-2330-0040.flac  \n",
            "  inflating: blindtest/test/7105-2330-0041.flac  \n",
            "  inflating: blindtest/test/7105-2340-0000.flac  \n",
            "  inflating: blindtest/test/7105-2340-0001.flac  \n",
            "  inflating: blindtest/test/7105-2340-0002.flac  \n",
            "  inflating: blindtest/test/7105-2340-0003.flac  \n",
            "  inflating: blindtest/test/7105-2340-0004.flac  \n",
            "  inflating: blindtest/test/7105-2340-0005.flac  \n",
            "  inflating: blindtest/test/7105-2340-0006.flac  \n",
            "  inflating: blindtest/test/7105-2340-0007.flac  \n",
            "  inflating: blindtest/test/7105-2340-0008.flac  \n",
            "  inflating: blindtest/test/7105-2340-0009.flac  \n",
            "  inflating: blindtest/test/7105-2340-0010.flac  \n",
            "  inflating: blindtest/test/7105-2340-0011.flac  \n",
            "  inflating: blindtest/test/7105-2340-0012.flac  \n",
            "  inflating: blindtest/test/7105-2340-0013.flac  \n",
            "  inflating: blindtest/test/7105-2340-0014.flac  \n",
            "  inflating: blindtest/test/7105-2340-0015.flac  \n",
            "  inflating: blindtest/test/7105-2340-0016.flac  \n",
            "  inflating: blindtest/test/7105-2340-0017.flac  \n",
            "  inflating: blindtest/test/7105-2340-0018.flac  \n",
            "  inflating: blindtest/test/7105-2340-0019.flac  \n",
            "  inflating: blindtest/test/7105-2340-0020.flac  \n",
            "  inflating: blindtest/test/7105-2340-0021.flac  \n",
            "  inflating: blindtest/test/7105-2340-0022.flac  \n",
            "  inflating: blindtest/test/7105-2340-0023.flac  \n",
            "  inflating: blindtest/test/7105-2340-0024.flac  \n",
            "  inflating: blindtest/test/7105-2340-0025.flac  \n",
            "  inflating: blindtest/test/7105-2340-0026.flac  \n",
            "  inflating: blindtest/test/7105-2340-0027.flac  \n",
            "  inflating: blindtest/test/7105-2340-0028.flac  \n",
            "  inflating: blindtest/test/7105-2340-0029.flac  \n",
            "  inflating: blindtest/test/7105-2340-0030.flac  \n",
            "  inflating: blindtest/test/7105-2340-0031.flac  \n",
            "  inflating: blindtest/test/7105-2340-0032.flac  \n",
            "  inflating: blindtest/test/7105-2340-0033.flac  \n",
            "  inflating: blindtest/test/7105-2340-0034.flac  \n",
            "  inflating: blindtest/test/7105-2340-0035.flac  \n",
            "  inflating: blindtest/test/7105-2340-0036.flac  \n",
            "  inflating: blindtest/test/7105-2340-0037.flac  \n",
            "  inflating: blindtest/test/7601-101619-0000.flac  \n",
            "  inflating: blindtest/test/7601-101619-0001.flac  \n",
            "  inflating: blindtest/test/7601-101619-0002.flac  \n",
            "  inflating: blindtest/test/7601-101619-0003.flac  \n",
            "  inflating: blindtest/test/7601-101619-0004.flac  \n",
            "  inflating: blindtest/test/7601-101619-0005.flac  \n",
            "  inflating: blindtest/test/7601-101622-0000.flac  \n",
            "  inflating: blindtest/test/7601-101622-0001.flac  \n",
            "  inflating: blindtest/test/7601-101622-0002.flac  \n",
            "  inflating: blindtest/test/7601-101622-0003.flac  \n",
            "  inflating: blindtest/test/7601-101622-0004.flac  \n",
            "  inflating: blindtest/test/7601-101622-0005.flac  \n",
            "  inflating: blindtest/test/7601-101622-0006.flac  \n",
            "  inflating: blindtest/test/7601-101622-0007.flac  \n",
            "  inflating: blindtest/test/7601-175351-0000.flac  \n",
            "  inflating: blindtest/test/7601-175351-0001.flac  \n",
            "  inflating: blindtest/test/7601-175351-0002.flac  \n",
            "  inflating: blindtest/test/7601-175351-0003.flac  \n",
            "  inflating: blindtest/test/7601-175351-0004.flac  \n",
            "  inflating: blindtest/test/7601-175351-0005.flac  \n",
            "  inflating: blindtest/test/7601-175351-0006.flac  \n",
            "  inflating: blindtest/test/7601-175351-0007.flac  \n",
            "  inflating: blindtest/test/7601-175351-0008.flac  \n",
            "  inflating: blindtest/test/7601-175351-0009.flac  \n",
            "  inflating: blindtest/test/7601-175351-0010.flac  \n",
            "  inflating: blindtest/test/7601-175351-0011.flac  \n",
            "  inflating: blindtest/test/7601-175351-0012.flac  \n",
            "  inflating: blindtest/test/7601-175351-0013.flac  \n",
            "  inflating: blindtest/test/7601-175351-0014.flac  \n",
            "  inflating: blindtest/test/7601-175351-0015.flac  \n",
            "  inflating: blindtest/test/7601-175351-0016.flac  \n",
            "  inflating: blindtest/test/7601-175351-0017.flac  \n",
            "  inflating: blindtest/test/7601-175351-0018.flac  \n",
            "  inflating: blindtest/test/7601-175351-0019.flac  \n",
            "  inflating: blindtest/test/7601-175351-0020.flac  \n",
            "  inflating: blindtest/test/7601-175351-0021.flac  \n",
            "  inflating: blindtest/test/7601-175351-0022.flac  \n",
            "  inflating: blindtest/test/7601-175351-0023.flac  \n",
            "  inflating: blindtest/test/7601-175351-0024.flac  \n",
            "  inflating: blindtest/test/7601-175351-0025.flac  \n",
            "  inflating: blindtest/test/7601-175351-0026.flac  \n",
            "  inflating: blindtest/test/7601-175351-0027.flac  \n",
            "  inflating: blindtest/test/7601-291468-0000.flac  \n",
            "  inflating: blindtest/test/7601-291468-0001.flac  \n",
            "  inflating: blindtest/test/7601-291468-0002.flac  \n",
            "  inflating: blindtest/test/7601-291468-0003.flac  \n",
            "  inflating: blindtest/test/7601-291468-0004.flac  \n",
            "  inflating: blindtest/test/7601-291468-0005.flac  \n",
            "  inflating: blindtest/test/7601-291468-0006.flac  \n",
            "  inflating: blindtest/test/7601-291468-0007.flac  \n",
            "  inflating: blindtest/test/7641-96252-0000.flac  \n",
            "  inflating: blindtest/test/7641-96252-0001.flac  \n",
            "  inflating: blindtest/test/7641-96252-0002.flac  \n",
            "  inflating: blindtest/test/7641-96252-0003.flac  \n",
            "  inflating: blindtest/test/7641-96252-0004.flac  \n",
            "  inflating: blindtest/test/7641-96252-0005.flac  \n",
            "  inflating: blindtest/test/7641-96252-0006.flac  \n",
            "  inflating: blindtest/test/7641-96252-0007.flac  \n",
            "  inflating: blindtest/test/7641-96252-0008.flac  \n",
            "  inflating: blindtest/test/7641-96252-0009.flac  \n",
            "  inflating: blindtest/test/7641-96252-0010.flac  \n",
            "  inflating: blindtest/test/7641-96252-0011.flac  \n",
            "  inflating: blindtest/test/7641-96252-0012.flac  \n",
            "  inflating: blindtest/test/7641-96252-0013.flac  \n",
            "  inflating: blindtest/test/7641-96252-0014.flac  \n",
            "  inflating: blindtest/test/7641-96252-0015.flac  \n",
            "  inflating: blindtest/test/7641-96252-0016.flac  \n",
            "  inflating: blindtest/test/7641-96252-0017.flac  \n",
            "  inflating: blindtest/test/7641-96252-0018.flac  \n",
            "  inflating: blindtest/test/7641-96252-0019.flac  \n",
            "  inflating: blindtest/test/7641-96252-0020.flac  \n",
            "  inflating: blindtest/test/7641-96252-0021.flac  \n",
            "  inflating: blindtest/test/7641-96252-0022.flac  \n",
            "  inflating: blindtest/test/7641-96670-0000.flac  \n",
            "  inflating: blindtest/test/7641-96670-0001.flac  \n",
            "  inflating: blindtest/test/7641-96670-0002.flac  \n",
            "  inflating: blindtest/test/7641-96670-0003.flac  \n",
            "  inflating: blindtest/test/7641-96670-0004.flac  \n",
            "  inflating: blindtest/test/7641-96670-0005.flac  \n",
            "  inflating: blindtest/test/7641-96670-0006.flac  \n",
            "  inflating: blindtest/test/7641-96670-0007.flac  \n",
            "  inflating: blindtest/test/7641-96670-0008.flac  \n",
            "  inflating: blindtest/test/7641-96670-0009.flac  \n",
            "  inflating: blindtest/test/7641-96670-0010.flac  \n",
            "  inflating: blindtest/test/7641-96670-0011.flac  \n",
            "  inflating: blindtest/test/7641-96670-0012.flac  \n",
            "  inflating: blindtest/test/7641-96670-0013.flac  \n",
            "  inflating: blindtest/test/7641-96670-0014.flac  \n",
            "  inflating: blindtest/test/7641-96670-0015.flac  \n",
            "  inflating: blindtest/test/7641-96670-0016.flac  \n",
            "  inflating: blindtest/test/7641-96670-0017.flac  \n",
            "  inflating: blindtest/test/7641-96670-0018.flac  \n",
            "  inflating: blindtest/test/7641-96670-0019.flac  \n",
            "  inflating: blindtest/test/7641-96670-0020.flac  \n",
            "  inflating: blindtest/test/7641-96670-0021.flac  \n",
            "  inflating: blindtest/test/7641-96670-0022.flac  \n",
            "  inflating: blindtest/test/7641-96670-0023.flac  \n",
            "  inflating: blindtest/test/7641-96670-0024.flac  \n",
            "  inflating: blindtest/test/7641-96670-0025.flac  \n",
            "  inflating: blindtest/test/7641-96670-0026.flac  \n",
            "  inflating: blindtest/test/7641-96670-0027.flac  \n",
            "  inflating: blindtest/test/7641-96684-0000.flac  \n",
            "  inflating: blindtest/test/7641-96684-0001.flac  \n",
            "  inflating: blindtest/test/7641-96684-0002.flac  \n",
            "  inflating: blindtest/test/7641-96684-0003.flac  \n",
            "  inflating: blindtest/test/7641-96684-0004.flac  \n",
            "  inflating: blindtest/test/7641-96684-0005.flac  \n",
            "  inflating: blindtest/test/7641-96684-0006.flac  \n",
            "  inflating: blindtest/test/7641-96684-0007.flac  \n",
            "  inflating: blindtest/test/7641-96684-0008.flac  \n",
            "  inflating: blindtest/test/7641-96684-0009.flac  \n",
            "  inflating: blindtest/test/7641-96684-0010.flac  \n",
            "  inflating: blindtest/test/7641-96684-0011.flac  \n",
            "  inflating: blindtest/test/7641-96684-0012.flac  \n",
            "  inflating: blindtest/test/7641-96684-0013.flac  \n",
            "  inflating: blindtest/test/7641-96684-0014.flac  \n",
            "  inflating: blindtest/test/7641-96684-0015.flac  \n",
            "  inflating: blindtest/test/7641-96684-0016.flac  \n",
            "  inflating: blindtest/test/7641-96684-0017.flac  \n",
            "  inflating: blindtest/test/7641-96684-0018.flac  \n",
            "  inflating: blindtest/test/7641-96684-0019.flac  \n",
            "  inflating: blindtest/test/7641-96684-0020.flac  \n",
            "  inflating: blindtest/test/7641-96684-0021.flac  \n",
            "  inflating: blindtest/test/7641-96684-0022.flac  \n",
            "  inflating: blindtest/test/7641-96684-0023.flac  \n",
            "  inflating: blindtest/test/7641-96684-0024.flac  \n",
            "  inflating: blindtest/test/7641-96684-0025.flac  \n",
            "  inflating: blindtest/test/7641-96684-0026.flac  \n",
            "  inflating: blindtest/test/7641-96684-0027.flac  \n",
            "  inflating: blindtest/test/7641-96684-0028.flac  \n",
            "  inflating: blindtest/test/7641-96684-0029.flac  \n",
            "  inflating: blindtest/test/7641-96684-0030.flac  \n",
            "  inflating: blindtest/test/7641-96684-0031.flac  \n",
            "  inflating: blindtest/test/7641-96684-0032.flac  \n",
            "  inflating: blindtest/test/7641-96684-0033.flac  \n",
            "  inflating: blindtest/test/7641-96684-0034.flac  \n",
            "  inflating: blindtest/test/7641-96684-0035.flac  \n",
            "  inflating: blindtest/test/7641-96684-0036.flac  \n",
            "  inflating: blindtest/test/7641-96684-0037.flac  \n",
            "  inflating: blindtest/test/7641-96684-0038.flac  \n",
            "  inflating: blindtest/test/777-126732-0000.flac  \n",
            "  inflating: blindtest/test/777-126732-0001.flac  \n",
            "  inflating: blindtest/test/777-126732-0002.flac  \n",
            "  inflating: blindtest/test/777-126732-0003.flac  \n",
            "  inflating: blindtest/test/777-126732-0004.flac  \n",
            "  inflating: blindtest/test/777-126732-0005.flac  \n",
            "  inflating: blindtest/test/777-126732-0006.flac  \n",
            "  inflating: blindtest/test/777-126732-0007.flac  \n",
            "  inflating: blindtest/test/777-126732-0008.flac  \n",
            "  inflating: blindtest/test/777-126732-0009.flac  \n",
            "  inflating: blindtest/test/777-126732-0010.flac  \n",
            "  inflating: blindtest/test/777-126732-0011.flac  \n",
            "  inflating: blindtest/test/777-126732-0012.flac  \n",
            "  inflating: blindtest/test/777-126732-0013.flac  \n",
            "  inflating: blindtest/test/777-126732-0014.flac  \n",
            "  inflating: blindtest/test/777-126732-0015.flac  \n",
            "  inflating: blindtest/test/777-126732-0016.flac  \n",
            "  inflating: blindtest/test/777-126732-0017.flac  \n",
            "  inflating: blindtest/test/777-126732-0018.flac  \n",
            "  inflating: blindtest/test/777-126732-0019.flac  \n",
            "  inflating: blindtest/test/777-126732-0020.flac  \n",
            "  inflating: blindtest/test/777-126732-0021.flac  \n",
            "  inflating: blindtest/test/777-126732-0022.flac  \n",
            "  inflating: blindtest/test/777-126732-0023.flac  \n",
            "  inflating: blindtest/test/777-126732-0024.flac  \n",
            "  inflating: blindtest/test/777-126732-0025.flac  \n",
            "  inflating: blindtest/test/777-126732-0026.flac  \n",
            "  inflating: blindtest/test/777-126732-0027.flac  \n",
            "  inflating: blindtest/test/777-126732-0028.flac  \n",
            "  inflating: blindtest/test/777-126732-0029.flac  \n",
            "  inflating: blindtest/test/777-126732-0030.flac  \n",
            "  inflating: blindtest/test/777-126732-0031.flac  \n",
            "  inflating: blindtest/test/777-126732-0032.flac  \n",
            "  inflating: blindtest/test/777-126732-0033.flac  \n",
            "  inflating: blindtest/test/777-126732-0034.flac  \n",
            "  inflating: blindtest/test/777-126732-0035.flac  \n",
            "  inflating: blindtest/test/777-126732-0036.flac  \n",
            "  inflating: blindtest/test/777-126732-0037.flac  \n",
            "  inflating: blindtest/test/777-126732-0038.flac  \n",
            "  inflating: blindtest/test/777-126732-0039.flac  \n",
            "  inflating: blindtest/test/777-126732-0040.flac  \n",
            "  inflating: blindtest/test/777-126732-0041.flac  \n",
            "  inflating: blindtest/test/777-126732-0042.flac  \n",
            "  inflating: blindtest/test/777-126732-0043.flac  \n",
            "  inflating: blindtest/test/777-126732-0044.flac  \n",
            "  inflating: blindtest/test/777-126732-0045.flac  \n",
            "  inflating: blindtest/test/777-126732-0046.flac  \n",
            "  inflating: blindtest/test/777-126732-0047.flac  \n",
            "  inflating: blindtest/test/777-126732-0048.flac  \n",
            "  inflating: blindtest/test/777-126732-0049.flac  \n",
            "  inflating: blindtest/test/777-126732-0050.flac  \n",
            "  inflating: blindtest/test/777-126732-0051.flac  \n",
            "  inflating: blindtest/test/777-126732-0052.flac  \n",
            "  inflating: blindtest/test/777-126732-0053.flac  \n",
            "  inflating: blindtest/test/777-126732-0054.flac  \n",
            "  inflating: blindtest/test/777-126732-0055.flac  \n",
            "  inflating: blindtest/test/777-126732-0056.flac  \n",
            "  inflating: blindtest/test/777-126732-0057.flac  \n",
            "  inflating: blindtest/test/777-126732-0058.flac  \n",
            "  inflating: blindtest/test/777-126732-0059.flac  \n",
            "  inflating: blindtest/test/777-126732-0060.flac  \n",
            "  inflating: blindtest/test/777-126732-0061.flac  \n",
            "  inflating: blindtest/test/777-126732-0062.flac  \n",
            "  inflating: blindtest/test/777-126732-0063.flac  \n",
            "  inflating: blindtest/test/777-126732-0064.flac  \n",
            "  inflating: blindtest/test/777-126732-0065.flac  \n",
            "  inflating: blindtest/test/777-126732-0066.flac  \n",
            "  inflating: blindtest/test/777-126732-0067.flac  \n",
            "  inflating: blindtest/test/777-126732-0068.flac  \n",
            "  inflating: blindtest/test/777-126732-0069.flac  \n",
            "  inflating: blindtest/test/777-126732-0070.flac  \n",
            "  inflating: blindtest/test/777-126732-0071.flac  \n",
            "  inflating: blindtest/test/777-126732-0072.flac  \n",
            "  inflating: blindtest/test/777-126732-0073.flac  \n",
            "  inflating: blindtest/test/777-126732-0074.flac  \n",
            "  inflating: blindtest/test/777-126732-0075.flac  \n",
            "  inflating: blindtest/test/777-126732-0076.flac  \n",
            "  inflating: blindtest/test/777-126732-0077.flac  \n",
            "  inflating: blindtest/test/777-126732-0078.flac  \n",
            "  inflating: blindtest/test/777-126732-0079.flac  \n",
            "  inflating: blindtest/test/777-126732-0080.flac  \n",
            "  inflating: blindtest/test/777-126732-0081.flac  \n",
            "  inflating: blindtest/test/8131-117016-0000.flac  \n",
            "  inflating: blindtest/test/8131-117016-0001.flac  \n",
            "  inflating: blindtest/test/8131-117016-0002.flac  \n",
            "  inflating: blindtest/test/8131-117016-0003.flac  \n",
            "  inflating: blindtest/test/8131-117016-0004.flac  \n",
            "  inflating: blindtest/test/8131-117016-0005.flac  \n",
            "  inflating: blindtest/test/8131-117016-0006.flac  \n",
            "  inflating: blindtest/test/8131-117016-0007.flac  \n",
            "  inflating: blindtest/test/8131-117016-0008.flac  \n",
            "  inflating: blindtest/test/8131-117016-0009.flac  \n",
            "  inflating: blindtest/test/8131-117016-0010.flac  \n",
            "  inflating: blindtest/test/8131-117016-0011.flac  \n",
            "  inflating: blindtest/test/8131-117016-0012.flac  \n",
            "  inflating: blindtest/test/8131-117016-0013.flac  \n",
            "  inflating: blindtest/test/8131-117016-0014.flac  \n",
            "  inflating: blindtest/test/8131-117016-0015.flac  \n",
            "  inflating: blindtest/test/8131-117016-0016.flac  \n",
            "  inflating: blindtest/test/8131-117016-0017.flac  \n",
            "  inflating: blindtest/test/8131-117016-0018.flac  \n",
            "  inflating: blindtest/test/8131-117016-0019.flac  \n",
            "  inflating: blindtest/test/8131-117016-0020.flac  \n",
            "  inflating: blindtest/test/8131-117016-0021.flac  \n",
            "  inflating: blindtest/test/8131-117016-0022.flac  \n",
            "  inflating: blindtest/test/8131-117016-0023.flac  \n",
            "  inflating: blindtest/test/8131-117016-0024.flac  \n",
            "  inflating: blindtest/test/8131-117016-0025.flac  \n",
            "  inflating: blindtest/test/8131-117016-0026.flac  \n",
            "  inflating: blindtest/test/8131-117016-0027.flac  \n",
            "  inflating: blindtest/test/8131-117016-0028.flac  \n",
            "  inflating: blindtest/test/8131-117016-0029.flac  \n",
            "  inflating: blindtest/test/8131-117016-0030.flac  \n",
            "  inflating: blindtest/test/8131-117016-0031.flac  \n",
            "  inflating: blindtest/test/8131-117016-0032.flac  \n",
            "  inflating: blindtest/test/8131-117016-0033.flac  \n",
            "  inflating: blindtest/test/8131-117016-0034.flac  \n",
            "  inflating: blindtest/test/8131-117016-0035.flac  \n",
            "  inflating: blindtest/test/8131-117016-0036.flac  \n",
            "  inflating: blindtest/test/8131-117016-0037.flac  \n",
            "  inflating: blindtest/test/8131-117016-0038.flac  \n",
            "  inflating: blindtest/test/8131-117016-0039.flac  \n",
            "  inflating: blindtest/test/8131-117016-0040.flac  \n",
            "  inflating: blindtest/test/8131-117016-0041.flac  \n",
            "  inflating: blindtest/test/8131-117016-0042.flac  \n",
            "  inflating: blindtest/test/8131-117016-0043.flac  \n",
            "  inflating: blindtest/test/8131-117016-0044.flac  \n",
            "  inflating: blindtest/test/8131-117016-0045.flac  \n",
            "  inflating: blindtest/test/8131-117016-0046.flac  \n",
            "  inflating: blindtest/test/8131-117016-0047.flac  \n",
            "  inflating: blindtest/test/8131-117016-0048.flac  \n",
            "  inflating: blindtest/test/8131-117016-0049.flac  \n",
            "  inflating: blindtest/test/8131-117016-0050.flac  \n",
            "  inflating: blindtest/test/8131-117016-0051.flac  \n",
            "  inflating: blindtest/test/8131-117016-0052.flac  \n",
            "  inflating: blindtest/test/8131-117016-0053.flac  \n",
            "  inflating: blindtest/test/8131-117016-0054.flac  \n",
            "  inflating: blindtest/test/8131-117016-0055.flac  \n",
            "  inflating: blindtest/test/8131-117016-0056.flac  \n",
            "  inflating: blindtest/test/8131-117016-0057.flac  \n",
            "  inflating: blindtest/test/8131-117016-0058.flac  \n",
            "  inflating: blindtest/test/8131-117016-0059.flac  \n",
            "  inflating: blindtest/test/8131-117016-0060.flac  \n",
            "  inflating: blindtest/test/8131-117016-0061.flac  \n",
            "  inflating: blindtest/test/8131-117017-0000.flac  \n",
            "  inflating: blindtest/test/8131-117017-0001.flac  \n",
            "  inflating: blindtest/test/8131-117017-0002.flac  \n",
            "  inflating: blindtest/test/8131-117017-0003.flac  \n",
            "  inflating: blindtest/test/8131-117017-0004.flac  \n",
            "  inflating: blindtest/test/8131-117017-0005.flac  \n",
            "  inflating: blindtest/test/8131-117017-0006.flac  \n",
            "  inflating: blindtest/test/8131-117017-0007.flac  \n",
            "  inflating: blindtest/test/8131-117017-0008.flac  \n",
            "  inflating: blindtest/test/8131-117017-0009.flac  \n",
            "  inflating: blindtest/test/8131-117017-0010.flac  \n",
            "  inflating: blindtest/test/8131-117017-0011.flac  \n",
            "  inflating: blindtest/test/8131-117017-0012.flac  \n",
            "  inflating: blindtest/test/8131-117017-0013.flac  \n",
            "  inflating: blindtest/test/8131-117017-0014.flac  \n",
            "  inflating: blindtest/test/8131-117017-0015.flac  \n",
            "  inflating: blindtest/test/8131-117017-0016.flac  \n",
            "  inflating: blindtest/test/8131-117017-0017.flac  \n",
            "  inflating: blindtest/test/8131-117017-0018.flac  \n",
            "  inflating: blindtest/test/8131-117017-0019.flac  \n",
            "  inflating: blindtest/test/8131-117017-0020.flac  \n",
            "  inflating: blindtest/test/8131-117017-0021.flac  \n",
            "  inflating: blindtest/test/8131-117017-0022.flac  \n",
            "  inflating: blindtest/test/8131-117017-0023.flac  \n",
            "  inflating: blindtest/test/8131-117017-0024.flac  \n",
            "  inflating: blindtest/test/8131-117017-0025.flac  \n",
            "  inflating: blindtest/test/8131-117017-0026.flac  \n",
            "  inflating: blindtest/test/8131-117017-0027.flac  \n",
            "  inflating: blindtest/test/8131-117017-0028.flac  \n",
            "  inflating: blindtest/test/8131-117017-0029.flac  \n",
            "  inflating: blindtest/test/8131-117017-0030.flac  \n",
            "  inflating: blindtest/test/8131-117017-0031.flac  \n",
            "  inflating: blindtest/test/8131-117017-0032.flac  \n",
            "  inflating: blindtest/test/8131-117029-0000.flac  \n",
            "  inflating: blindtest/test/8131-117029-0001.flac  \n",
            "  inflating: blindtest/test/8131-117029-0002.flac  \n",
            "  inflating: blindtest/test/8131-117029-0003.flac  \n",
            "  inflating: blindtest/test/8131-117029-0004.flac  \n",
            "  inflating: blindtest/test/8131-117029-0005.flac  \n",
            "  inflating: blindtest/test/8131-117029-0006.flac  \n",
            "  inflating: blindtest/test/8131-117029-0007.flac  \n",
            "  inflating: blindtest/test/8131-117029-0008.flac  \n",
            "  inflating: blindtest/test/8131-117029-0009.flac  \n",
            "  inflating: blindtest/test/8131-117029-0010.flac  \n",
            "  inflating: blindtest/test/8131-117029-0011.flac  \n",
            "  inflating: blindtest/test/8131-117029-0012.flac  \n",
            "  inflating: blindtest/test/8131-117029-0013.flac  \n",
            "  inflating: blindtest/test/8131-117029-0014.flac  \n",
            "  inflating: blindtest/test/8131-117029-0015.flac  \n",
            "  inflating: blindtest/test/8131-117029-0016.flac  \n",
            "  inflating: blindtest/test/8131-117029-0017.flac  \n",
            "  inflating: blindtest/test/8131-117029-0018.flac  \n",
            "  inflating: blindtest/test/8131-117029-0019.flac  \n",
            "  inflating: blindtest/test/8131-117029-0020.flac  \n",
            "  inflating: blindtest/test/8131-117029-0021.flac  \n",
            "  inflating: blindtest/test/8131-117029-0022.flac  \n",
            "  inflating: blindtest/test/8254-115543-0000.flac  \n",
            "  inflating: blindtest/test/8254-115543-0001.flac  \n",
            "  inflating: blindtest/test/8254-115543-0002.flac  \n",
            "  inflating: blindtest/test/8254-115543-0003.flac  \n",
            "  inflating: blindtest/test/8254-115543-0004.flac  \n",
            "  inflating: blindtest/test/8254-115543-0005.flac  \n",
            "  inflating: blindtest/test/8254-115543-0006.flac  \n",
            "  inflating: blindtest/test/8254-115543-0007.flac  \n",
            "  inflating: blindtest/test/8254-115543-0008.flac  \n",
            "  inflating: blindtest/test/8254-115543-0009.flac  \n",
            "  inflating: blindtest/test/8254-115543-0010.flac  \n",
            "  inflating: blindtest/test/8254-115543-0011.flac  \n",
            "  inflating: blindtest/test/8254-115543-0012.flac  \n",
            "  inflating: blindtest/test/8254-115543-0013.flac  \n",
            "  inflating: blindtest/test/8254-115543-0014.flac  \n",
            "  inflating: blindtest/test/8254-115543-0015.flac  \n",
            "  inflating: blindtest/test/8254-115543-0016.flac  \n",
            "  inflating: blindtest/test/8254-115543-0017.flac  \n",
            "  inflating: blindtest/test/8254-115543-0018.flac  \n",
            "  inflating: blindtest/test/8254-115543-0019.flac  \n",
            "  inflating: blindtest/test/8254-115543-0020.flac  \n",
            "  inflating: blindtest/test/8254-115543-0021.flac  \n",
            "  inflating: blindtest/test/8254-115543-0022.flac  \n",
            "  inflating: blindtest/test/8254-115543-0023.flac  \n",
            "  inflating: blindtest/test/8254-115543-0024.flac  \n",
            "  inflating: blindtest/test/8254-115543-0025.flac  \n",
            "  inflating: blindtest/test/8254-115543-0026.flac  \n",
            "  inflating: blindtest/test/8254-115543-0027.flac  \n",
            "  inflating: blindtest/test/8254-115543-0028.flac  \n",
            "  inflating: blindtest/test/8254-115543-0029.flac  \n",
            "  inflating: blindtest/test/8254-115543-0030.flac  \n",
            "  inflating: blindtest/test/8254-115543-0031.flac  \n",
            "  inflating: blindtest/test/8254-115543-0032.flac  \n",
            "  inflating: blindtest/test/8254-115543-0033.flac  \n",
            "  inflating: blindtest/test/8254-115543-0034.flac  \n",
            "  inflating: blindtest/test/8254-115543-0035.flac  \n",
            "  inflating: blindtest/test/8254-115543-0036.flac  \n",
            "  inflating: blindtest/test/8254-115543-0037.flac  \n",
            "  inflating: blindtest/test/8254-115543-0038.flac  \n",
            "  inflating: blindtest/test/8254-115543-0039.flac  \n",
            "  inflating: blindtest/test/8254-115543-0040.flac  \n",
            "  inflating: blindtest/test/8254-115543-0041.flac  \n",
            "  inflating: blindtest/test/8254-115543-0042.flac  \n",
            "  inflating: blindtest/test/8254-115543-0043.flac  \n",
            "  inflating: blindtest/test/8254-115543-0044.flac  \n",
            "  inflating: blindtest/test/8254-115543-0045.flac  \n",
            "  inflating: blindtest/test/8254-84205-0000.flac  \n",
            "  inflating: blindtest/test/8254-84205-0001.flac  \n",
            "  inflating: blindtest/test/8254-84205-0002.flac  \n",
            "  inflating: blindtest/test/8254-84205-0003.flac  \n",
            "  inflating: blindtest/test/8254-84205-0004.flac  \n",
            "  inflating: blindtest/test/8254-84205-0005.flac  \n",
            "  inflating: blindtest/test/8254-84205-0006.flac  \n",
            "  inflating: blindtest/test/8254-84205-0007.flac  \n",
            "  inflating: blindtest/test/8254-84205-0008.flac  \n",
            "  inflating: blindtest/test/8254-84205-0009.flac  \n",
            "  inflating: blindtest/test/8254-84205-0010.flac  \n",
            "  inflating: blindtest/test/8254-84205-0011.flac  \n",
            "  inflating: blindtest/test/8254-84205-0012.flac  \n",
            "  inflating: blindtest/test/8254-84205-0013.flac  \n",
            "  inflating: blindtest/test/8254-84205-0014.flac  \n",
            "  inflating: blindtest/test/8254-84205-0015.flac  \n",
            "  inflating: blindtest/test/8254-84205-0016.flac  \n",
            "  inflating: blindtest/test/8254-84205-0017.flac  \n",
            "  inflating: blindtest/test/8254-84205-0018.flac  \n",
            "  inflating: blindtest/test/8254-84205-0019.flac  \n",
            "  inflating: blindtest/test/8254-84205-0020.flac  \n",
            "  inflating: blindtest/test/8254-84205-0021.flac  \n",
            "  inflating: blindtest/test/8254-84205-0022.flac  \n",
            "  inflating: blindtest/test/8254-84205-0023.flac  \n",
            "  inflating: blindtest/test/8254-84205-0024.flac  \n",
            "  inflating: blindtest/test/8254-84205-0025.flac  \n",
            "  inflating: blindtest/test/8254-84205-0026.flac  \n",
            "  inflating: blindtest/test/8254-84205-0027.flac  \n",
            "  inflating: blindtest/test/8254-84205-0028.flac  \n",
            "  inflating: blindtest/test/8254-84205-0029.flac  \n",
            "  inflating: blindtest/test/8254-84205-0030.flac  \n",
            "  inflating: blindtest/test/8254-84205-0031.flac  \n",
            "  inflating: blindtest/test/8254-84205-0032.flac  \n",
            "  inflating: blindtest/test/8254-84205-0033.flac  \n",
            "  inflating: blindtest/test/8254-84205-0034.flac  \n",
            "  inflating: blindtest/test/8254-84205-0035.flac  \n",
            "  inflating: blindtest/test/8254-84205-0036.flac  \n",
            "  inflating: blindtest/test/8254-84205-0037.flac  \n",
            "  inflating: blindtest/test/8254-84205-0038.flac  \n",
            "  inflating: blindtest/test/8254-84205-0039.flac  \n",
            "  inflating: blindtest/test/8254-84205-0040.flac  \n",
            "  inflating: blindtest/test/8254-84205-0041.flac  \n",
            "  inflating: blindtest/test/8254-84205-0042.flac  \n",
            "  inflating: blindtest/test/8254-84205-0043.flac  \n",
            "  inflating: blindtest/test/8254-84205-0044.flac  \n",
            "  inflating: blindtest/test/8254-84205-0045.flac  \n",
            "  inflating: blindtest/test/8254-84205-0046.flac  \n",
            "  inflating: blindtest/test/8254-84205-0047.flac  \n",
            "  inflating: blindtest/test/8254-84205-0048.flac  \n",
            "  inflating: blindtest/test/8254-84205-0049.flac  \n",
            "  inflating: blindtest/test/8254-84205-0050.flac  \n",
            "  inflating: blindtest/test/8254-84205-0051.flac  \n",
            "  inflating: blindtest/test/8254-84205-0052.flac  \n",
            "  inflating: blindtest/test/8254-84205-0053.flac  \n",
            "  inflating: blindtest/test/8254-84205-0054.flac  \n",
            "  inflating: blindtest/test/8254-84205-0055.flac  \n",
            "  inflating: blindtest/test/8254-84205-0056.flac  \n",
            "  inflating: blindtest/test/8254-84205-0057.flac  \n",
            "  inflating: blindtest/test/8254-84205-0058.flac  \n",
            "  inflating: blindtest/test/8254-84205-0059.flac  \n",
            "  inflating: blindtest/test/8254-84205-0060.flac  \n",
            "  inflating: blindtest/test/8254-84205-0061.flac  \n",
            "  inflating: blindtest/test/8254-84205-0062.flac  \n",
            "  inflating: blindtest/test/8254-84205-0063.flac  \n",
            "  inflating: blindtest/test/8254-84205-0064.flac  \n",
            "  inflating: blindtest/test/8254-84205-0065.flac  \n",
            "  inflating: blindtest/test/8254-84205-0066.flac  \n",
            "  inflating: blindtest/test/8254-84205-0067.flac  \n",
            "  inflating: blindtest/test/8254-84205-0068.flac  \n",
            "  inflating: blindtest/test/8254-84205-0069.flac  \n",
            "  inflating: blindtest/test/8254-84205-0070.flac  \n",
            "  inflating: blindtest/test/8254-84205-0071.flac  \n",
            "  inflating: blindtest/test/8254-84205-0072.flac  \n",
            "  inflating: blindtest/test/8254-84205-0073.flac  \n",
            "  inflating: blindtest/test/8254-84205-0074.flac  \n",
            "  inflating: blindtest/test/8254-84205-0075.flac  \n",
            "  inflating: blindtest/test/8254-84205-0076.flac  \n",
            "  inflating: blindtest/test/8297-275154-0000.flac  \n",
            "  inflating: blindtest/test/8297-275154-0001.flac  \n",
            "  inflating: blindtest/test/8297-275154-0002.flac  \n",
            "  inflating: blindtest/test/8297-275154-0003.flac  \n",
            "  inflating: blindtest/test/8297-275154-0004.flac  \n",
            "  inflating: blindtest/test/8297-275154-0005.flac  \n",
            "  inflating: blindtest/test/8297-275154-0006.flac  \n",
            "  inflating: blindtest/test/8297-275154-0007.flac  \n",
            "  inflating: blindtest/test/8297-275154-0008.flac  \n",
            "  inflating: blindtest/test/8297-275154-0009.flac  \n",
            "  inflating: blindtest/test/8297-275154-0010.flac  \n",
            "  inflating: blindtest/test/8297-275154-0011.flac  \n",
            "  inflating: blindtest/test/8297-275154-0012.flac  \n",
            "  inflating: blindtest/test/8297-275154-0013.flac  \n",
            "  inflating: blindtest/test/8297-275154-0014.flac  \n",
            "  inflating: blindtest/test/8297-275154-0015.flac  \n",
            "  inflating: blindtest/test/8297-275154-0016.flac  \n",
            "  inflating: blindtest/test/8297-275154-0017.flac  \n",
            "  inflating: blindtest/test/8297-275154-0018.flac  \n",
            "  inflating: blindtest/test/8297-275154-0019.flac  \n",
            "  inflating: blindtest/test/8297-275154-0020.flac  \n",
            "  inflating: blindtest/test/8297-275154-0021.flac  \n",
            "  inflating: blindtest/test/8297-275154-0022.flac  \n",
            "  inflating: blindtest/test/8297-275154-0023.flac  \n",
            "  inflating: blindtest/test/8297-275154-0024.flac  \n",
            "  inflating: blindtest/test/8297-275154-0025.flac  \n",
            "  inflating: blindtest/test/8297-275154-0026.flac  \n",
            "  inflating: blindtest/test/8297-275154-0027.flac  \n",
            "  inflating: blindtest/test/8297-275155-0000.flac  \n",
            "  inflating: blindtest/test/8297-275155-0001.flac  \n",
            "  inflating: blindtest/test/8297-275155-0002.flac  \n",
            "  inflating: blindtest/test/8297-275155-0003.flac  \n",
            "  inflating: blindtest/test/8297-275155-0004.flac  \n",
            "  inflating: blindtest/test/8297-275155-0005.flac  \n",
            "  inflating: blindtest/test/8297-275155-0006.flac  \n",
            "  inflating: blindtest/test/8297-275155-0007.flac  \n",
            "  inflating: blindtest/test/8297-275155-0008.flac  \n",
            "  inflating: blindtest/test/8297-275155-0009.flac  \n",
            "  inflating: blindtest/test/8297-275155-0010.flac  \n",
            "  inflating: blindtest/test/8297-275155-0011.flac  \n",
            "  inflating: blindtest/test/8297-275155-0012.flac  \n",
            "  inflating: blindtest/test/8297-275155-0013.flac  \n",
            "  inflating: blindtest/test/8297-275155-0014.flac  \n",
            "  inflating: blindtest/test/8297-275155-0015.flac  \n",
            "  inflating: blindtest/test/8297-275155-0016.flac  \n",
            "  inflating: blindtest/test/8297-275155-0017.flac  \n",
            "  inflating: blindtest/test/8297-275155-0018.flac  \n",
            "  inflating: blindtest/test/8297-275155-0019.flac  \n",
            "  inflating: blindtest/test/8297-275155-0020.flac  \n",
            "  inflating: blindtest/test/8297-275155-0021.flac  \n",
            "  inflating: blindtest/test/8297-275155-0022.flac  \n",
            "  inflating: blindtest/test/8297-275155-0023.flac  \n",
            "  inflating: blindtest/test/8297-275155-0024.flac  \n",
            "  inflating: blindtest/test/8297-275155-0025.flac  \n",
            "  inflating: blindtest/test/8297-275155-0026.flac  \n",
            "  inflating: blindtest/test/8297-275155-0027.flac  \n",
            "  inflating: blindtest/test/8297-275155-0028.flac  \n",
            "  inflating: blindtest/test/8297-275155-0029.flac  \n",
            "  inflating: blindtest/test/8297-275155-0030.flac  \n",
            "  inflating: blindtest/test/8297-275155-0031.flac  \n",
            "  inflating: blindtest/test/8297-275155-0032.flac  \n",
            "  inflating: blindtest/test/8297-275156-0000.flac  \n",
            "  inflating: blindtest/test/8297-275156-0001.flac  \n",
            "  inflating: blindtest/test/8297-275156-0002.flac  \n",
            "  inflating: blindtest/test/8297-275156-0003.flac  \n",
            "  inflating: blindtest/test/8297-275156-0004.flac  \n",
            "  inflating: blindtest/test/8297-275156-0005.flac  \n",
            "  inflating: blindtest/test/8297-275156-0006.flac  \n",
            "  inflating: blindtest/test/8297-275156-0007.flac  \n",
            "  inflating: blindtest/test/8297-275156-0008.flac  \n",
            "  inflating: blindtest/test/8297-275156-0009.flac  \n",
            "  inflating: blindtest/test/8297-275156-0010.flac  \n",
            "  inflating: blindtest/test/8297-275156-0011.flac  \n",
            "  inflating: blindtest/test/8297-275156-0012.flac  \n",
            "  inflating: blindtest/test/8297-275156-0013.flac  \n",
            "  inflating: blindtest/test/84-121123-0000.flac  \n",
            "  inflating: blindtest/test/84-121123-0001.flac  \n",
            "  inflating: blindtest/test/84-121123-0002.flac  \n",
            "  inflating: blindtest/test/84-121123-0003.flac  \n",
            "  inflating: blindtest/test/84-121123-0004.flac  \n",
            "  inflating: blindtest/test/84-121123-0005.flac  \n",
            "  inflating: blindtest/test/84-121123-0006.flac  \n",
            "  inflating: blindtest/test/84-121123-0007.flac  \n",
            "  inflating: blindtest/test/84-121123-0008.flac  \n",
            "  inflating: blindtest/test/84-121123-0009.flac  \n",
            "  inflating: blindtest/test/84-121123-0010.flac  \n",
            "  inflating: blindtest/test/84-121123-0011.flac  \n",
            "  inflating: blindtest/test/84-121123-0012.flac  \n",
            "  inflating: blindtest/test/84-121123-0013.flac  \n",
            "  inflating: blindtest/test/84-121123-0014.flac  \n",
            "  inflating: blindtest/test/84-121123-0015.flac  \n",
            "  inflating: blindtest/test/84-121123-0016.flac  \n",
            "  inflating: blindtest/test/84-121123-0017.flac  \n",
            "  inflating: blindtest/test/84-121123-0018.flac  \n",
            "  inflating: blindtest/test/84-121123-0019.flac  \n",
            "  inflating: blindtest/test/84-121123-0020.flac  \n",
            "  inflating: blindtest/test/84-121123-0021.flac  \n",
            "  inflating: blindtest/test/84-121123-0022.flac  \n",
            "  inflating: blindtest/test/84-121123-0023.flac  \n",
            "  inflating: blindtest/test/84-121123-0024.flac  \n",
            "  inflating: blindtest/test/84-121123-0025.flac  \n",
            "  inflating: blindtest/test/84-121123-0026.flac  \n",
            "  inflating: blindtest/test/84-121123-0027.flac  \n",
            "  inflating: blindtest/test/84-121123-0028.flac  \n",
            "  inflating: blindtest/test/84-121550-0000.flac  \n",
            "  inflating: blindtest/test/84-121550-0001.flac  \n",
            "  inflating: blindtest/test/84-121550-0002.flac  \n",
            "  inflating: blindtest/test/84-121550-0003.flac  \n",
            "  inflating: blindtest/test/84-121550-0004.flac  \n",
            "  inflating: blindtest/test/84-121550-0005.flac  \n",
            "  inflating: blindtest/test/84-121550-0006.flac  \n",
            "  inflating: blindtest/test/84-121550-0007.flac  \n",
            "  inflating: blindtest/test/84-121550-0008.flac  \n",
            "  inflating: blindtest/test/84-121550-0009.flac  \n",
            "  inflating: blindtest/test/84-121550-0010.flac  \n",
            "  inflating: blindtest/test/84-121550-0011.flac  \n",
            "  inflating: blindtest/test/84-121550-0012.flac  \n",
            "  inflating: blindtest/test/84-121550-0013.flac  \n",
            "  inflating: blindtest/test/84-121550-0014.flac  \n",
            "  inflating: blindtest/test/84-121550-0015.flac  \n",
            "  inflating: blindtest/test/84-121550-0016.flac  \n",
            "  inflating: blindtest/test/84-121550-0017.flac  \n",
            "  inflating: blindtest/test/84-121550-0018.flac  \n",
            "  inflating: blindtest/test/84-121550-0019.flac  \n",
            "  inflating: blindtest/test/84-121550-0020.flac  \n",
            "  inflating: blindtest/test/84-121550-0021.flac  \n",
            "  inflating: blindtest/test/84-121550-0022.flac  \n",
            "  inflating: blindtest/test/84-121550-0023.flac  \n",
            "  inflating: blindtest/test/84-121550-0024.flac  \n",
            "  inflating: blindtest/test/84-121550-0025.flac  \n",
            "  inflating: blindtest/test/84-121550-0026.flac  \n",
            "  inflating: blindtest/test/84-121550-0027.flac  \n",
            "  inflating: blindtest/test/84-121550-0028.flac  \n",
            "  inflating: blindtest/test/84-121550-0029.flac  \n",
            "  inflating: blindtest/test/84-121550-0030.flac  \n",
            "  inflating: blindtest/test/84-121550-0031.flac  \n",
            "  inflating: blindtest/test/84-121550-0032.flac  \n",
            "  inflating: blindtest/test/84-121550-0033.flac  \n",
            "  inflating: blindtest/test/84-121550-0034.flac  \n",
            "  inflating: blindtest/test/84-121550-0035.flac  \n",
            "  inflating: blindtest/test/8842-302196-0000.flac  \n",
            "  inflating: blindtest/test/8842-302196-0001.flac  \n",
            "  inflating: blindtest/test/8842-302196-0002.flac  \n",
            "  inflating: blindtest/test/8842-302196-0003.flac  \n",
            "  inflating: blindtest/test/8842-302196-0004.flac  \n",
            "  inflating: blindtest/test/8842-302196-0005.flac  \n",
            "  inflating: blindtest/test/8842-302196-0006.flac  \n",
            "  inflating: blindtest/test/8842-302196-0007.flac  \n",
            "  inflating: blindtest/test/8842-302196-0008.flac  \n",
            "  inflating: blindtest/test/8842-302196-0009.flac  \n",
            "  inflating: blindtest/test/8842-302196-0010.flac  \n",
            "  inflating: blindtest/test/8842-302196-0011.flac  \n",
            "  inflating: blindtest/test/8842-302196-0012.flac  \n",
            "  inflating: blindtest/test/8842-302201-0000.flac  \n",
            "  inflating: blindtest/test/8842-302201-0001.flac  \n",
            "  inflating: blindtest/test/8842-302201-0002.flac  \n",
            "  inflating: blindtest/test/8842-302201-0003.flac  \n",
            "  inflating: blindtest/test/8842-302201-0004.flac  \n",
            "  inflating: blindtest/test/8842-302201-0005.flac  \n",
            "  inflating: blindtest/test/8842-302201-0006.flac  \n",
            "  inflating: blindtest/test/8842-302201-0007.flac  \n",
            "  inflating: blindtest/test/8842-302201-0008.flac  \n",
            "  inflating: blindtest/test/8842-302201-0009.flac  \n",
            "  inflating: blindtest/test/8842-302201-0010.flac  \n",
            "  inflating: blindtest/test/8842-302201-0011.flac  \n",
            "  inflating: blindtest/test/8842-302201-0012.flac  \n",
            "  inflating: blindtest/test/8842-302201-0013.flac  \n",
            "  inflating: blindtest/test/8842-302201-0014.flac  \n",
            "  inflating: blindtest/test/8842-302201-0015.flac  \n",
            "  inflating: blindtest/test/8842-302203-0000.flac  \n",
            "  inflating: blindtest/test/8842-302203-0001.flac  \n",
            "  inflating: blindtest/test/8842-302203-0002.flac  \n",
            "  inflating: blindtest/test/8842-302203-0003.flac  \n",
            "  inflating: blindtest/test/8842-302203-0004.flac  \n",
            "  inflating: blindtest/test/8842-302203-0005.flac  \n",
            "  inflating: blindtest/test/8842-302203-0006.flac  \n",
            "  inflating: blindtest/test/8842-302203-0007.flac  \n",
            "  inflating: blindtest/test/8842-302203-0008.flac  \n",
            "  inflating: blindtest/test/8842-302203-0009.flac  \n",
            "  inflating: blindtest/test/8842-302203-0010.flac  \n",
            "  inflating: blindtest/test/8842-302203-0011.flac  \n",
            "  inflating: blindtest/test/8842-304647-0000.flac  \n",
            "  inflating: blindtest/test/8842-304647-0001.flac  \n",
            "  inflating: blindtest/test/8842-304647-0002.flac  \n",
            "  inflating: blindtest/test/8842-304647-0003.flac  \n",
            "  inflating: blindtest/test/8842-304647-0004.flac  \n",
            "  inflating: blindtest/test/8842-304647-0005.flac  \n",
            "  inflating: blindtest/test/8842-304647-0006.flac  \n",
            "  inflating: blindtest/test/8842-304647-0007.flac  \n",
            "  inflating: blindtest/test/8842-304647-0008.flac  \n",
            "  inflating: blindtest/test/8842-304647-0009.flac  \n",
            "  inflating: blindtest/test/8842-304647-0010.flac  \n",
            "  inflating: blindtest/test/8842-304647-0011.flac  \n",
            "  inflating: blindtest/test/8842-304647-0012.flac  \n",
            "  inflating: blindtest/test/8842-304647-0013.flac  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('/content/blindtest/test/'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlWlk6z_S26r",
        "outputId": "5d9165fa-18ad-454e-fdb8-b461af685e13"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2807"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/blindtest/test/')[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YxS-U_7OTGzV",
        "outputId": "69e9b2c0-f1be-4573-e1a7-fa6611e0efb2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'6241-66616-0005.flac'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[0]['sig'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gzyaTNSWzP5",
        "outputId": "79587d6f-4237-44c3-e908-b8e44ee3596f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([87280])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbrain.dataio.dataio import read_audio\n",
        "blindtest_dir = '/content/blindtest/test/'\n",
        "for audio_file_path in os.listdir('/content/blindtest/test/'):\n",
        "  xs_speech = read_audio(blindtest_dir+audio_file_path)\n",
        "  #xs_speech = xs_speech.unsqueeze(0)\n",
        "  #print(xs_speech.shape)\n",
        "  asr_brain.compute_forward(xs_speech,stage=sb.Stage.TEST)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "Ed36-51YUKBQ",
        "outputId": "a9676ecb-0daa-44f8-f104-a0cf57b802a9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'sig'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-2749ead97d75>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#xs_speech = xs_speech.unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#print(xs_speech.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0masr_brain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs_speech\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-aab81247429b>\u001b[0m in \u001b[0;36mcompute_forward\u001b[0;34m(self, batch, stage)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;34m\"\"\"Performs a forward pass through the encoder\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mwavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtokens_bos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_bos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sig'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "# Specify the path to the zip file\n",
        "zip_file_path = '/content/drive/MyDrive/blindtest.zip'\n",
        "\n",
        "# Specify the directory where you want to extract the contents\n",
        "extracted_dir_path = '/content/'\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "os.makedirs(extracted_dir_path, exist_ok=True)\n",
        "\n",
        "# Open and extract the zip file\n",
        "with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir_path)\n",
        "\n",
        "# List the contents of the extracted directory\n",
        "extracted_files = os.listdir(extracted_dir_path)\n",
        "print(\"Extracted files:\", extracted_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5voH89TrSA0c",
        "outputId": "19e7af76-ca01-45ff-f6da-f77ccbbb2475"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['.config', 'drive', 'task2B_model_Checkpoints.zip', 'speechbrain', 'blindtest', 'sample_data']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bKXIYTJSOnt_",
        "rtdr1VnyCQTJ"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}